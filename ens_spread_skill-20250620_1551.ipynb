{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2X-b5-NzDBVB",
    "outputId": "830cc9d8-d560-4244-a0f1-743a16e6735e"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from metpy.constants import g\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xarray as xr\n",
    "\n",
    "# prompt: Find gefs data , use this information https://nomads.ncep.noaa.gov/pub/data/nccf/com/gens/prod/gefs.20250530/.. download the data before trying to access it\n",
    "\n",
    "# We need to construct the URL for the desired data.\n",
    "# The pattern seems to be: https://nomads.ncep.noaa.gov/pub/data/nccf/com/gens/prod/gefs.YYYYMMDD/\n",
    "# We also need to specify the ensemble member and the forecast hour.\n",
    "# The specific file names are also structured, e.g., gefs.t00z.pgrb2a.0p50.f006.grib2\n",
    "# where t00z is the cycle time (00Z), pgrb2a is the grid type, 0p50 is the resolution.\n",
    "\n",
    "now = pd.Timestamp.now()\n",
    "init_start = pd.to_datetime(\"20250608\")\n",
    "init_end = pd.to_datetime(\"20250614\")\n",
    "forecast_length = 120\n",
    "\n",
    "init_time_range = pd.date_range(\n",
    "    init_start.floor(\"24h\"), init_end.floor(\"24h\"), freq=\"24h\"\n",
    ")  # round down to nearest multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for init_time in init_time_range:\n",
    "    print(init_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortName, isobaricInhPa = \"t\", 850\n",
    "\n",
    "\n",
    "def build_file_url(init_time, mem, forecast_hour):\n",
    "    return (\n",
    "        f\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gens/prod/gefs.{init_time:%Y%m%d}/\"\n",
    "        f\"{init_time:%H}/atmos/pgrb2ap5/\"\n",
    "        f\"{mem}.t{init_time:%H}z.pgrb2a.0p50.f{forecast_hour:03d}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def local_path_from_url(gefsdir, file_url_str):\n",
    "    url_path = Path(file_url_str)\n",
    "    file_path = gefsdir.joinpath(*url_path.parts[-5:])\n",
    "    os.makedirs(file_path.parent, exist_ok=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def open_grib_dataset(path):\n",
    "    ds = xr.open_dataset(\n",
    "        path,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"errors\": \"ignore\"},\n",
    "        filter_by_keys={\n",
    "            \"typeOfLevel\": \"isobaricInhPa\",\n",
    "            \"level\": isobaricInhPa,\n",
    "            \"shortName\": \"gh\" if shortName == \"z\" else shortName,\n",
    "        },\n",
    "        decode_timedelta=True,\n",
    "        chunks={},  # chunking can help reduce memory usage\n",
    "    )\n",
    "    if shortName == 'z':\n",
    "        ds = ds.rename(gh='z')\n",
    "    return ds\n",
    "\n",
    "\n",
    "def download_file(url, local_file_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully downloaded: {local_file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        print(f\"Could not download {url}\")\n",
    "        print(\"Please verify the date of the file exists on the server.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pangu_ifiles(pangu_dir, init_time, mem, forecast_length):\n",
    "    file_path = pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"ens{mem}\" / \"pangu_forecast_data\"\n",
    "    ifiles = [file_path / f\"pangu_ens{mem}_pred_{i:03d}.nc\" for i in range(24, forecast_length+1, 24)]\n",
    "    return ifiles\n",
    "\n",
    "\n",
    "def add_ensemble_number(ds):\n",
    "    \"\"\"\n",
    "    A preprocessing function to be used with xr.open_mfdataset.\n",
    "    It extracts the ensemble member number from the source filename\n",
    "    and adds it as a 'number' coordinate.\n",
    "    \"\"\"\n",
    "    # Get the basename of the file (e.g., \"pangu_ens0_pred_162.nc\")\n",
    "    try:\n",
    "        filename = os.path.basename(ds.encoding[\"source\"])\n",
    "    except (KeyError, TypeError):\n",
    "        # Fallback if source encoding is not available\n",
    "        return ds\n",
    "\n",
    "    # Use a regular expression to find the number following 'ens'\n",
    "    match = re.search(r'ens(\\d+)', filename)\n",
    "\n",
    "    if match:\n",
    "        # Extract the number, convert to integer\n",
    "        ensemble_number = int(match.group(1))\n",
    "\n",
    "        # Add 'number' as a new dimension and assign the extracted number as its coordinate\n",
    "        return ds.expand_dims(number=[ensemble_number])\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pangu_dir = Path(\"/glade/derecho/scratch/sobash/pangu_realtime\")\n",
    "# Create a nested list where list[init_time][member] = [files_for_all_forecast_hours]\n",
    "nested_files = []\n",
    "for init_time in init_time_range:\n",
    "    # List for all members for this init_time\n",
    "    time_specific_files = []\n",
    "    for mem in range(51):\n",
    "        # List of all forecast hour files for this specific member\n",
    "        member_files = pangu_ifiles(pangu_dir, init_time, mem, forecast_length)\n",
    "        time_specific_files.append(member_files)\n",
    "\n",
    "    # Only add the list of time-specific files if it's not empty\n",
    "    if time_specific_files:\n",
    "        nested_files.append(time_specific_files)\n",
    "\n",
    "# The channel label we want to select\n",
    "channel_label = f\"{shortName}{isobaricInhPa}\"\n",
    "\n",
    "ds_pangu = (\n",
    "    xr.open_mfdataset(\n",
    "        nested_files,\n",
    "        combine=\"nested\",\n",
    "        concat_dim=[\"init_time\", \"number\", \"prediction_timedelta\"],\n",
    "        chunks=\"auto\",  # Use 'auto' for better performance with dask\n",
    "    )\n",
    "    # Rename dimensions and coordinates at the start\n",
    "    .rename(\n",
    "        {\n",
    "            \"init_time\": \"initialization_time\",\n",
    "            \"prediction_timedelta\": \"step\",\n",
    "            \"lat\": \"latitude\",\n",
    "            \"lon\": \"longitude\",\n",
    "            \"__xarray_dataarray_variable__\": shortName,  # Rename the main variable\n",
    "        }\n",
    "    )\n",
    "    # Assign the integer coordinate for the 'number' dimension\n",
    "    .assign_coords(number=range(51))\n",
    "    # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "    .pipe(\n",
    "        lambda ds: ds.assign_coords(\n",
    "            forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "        ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "    )\n",
    "    # Calculate the valid_time coordinate\n",
    "    .assign(valid_time=lambda ds: ds.initialization_time + ds.step)\n",
    "    # Select the desired channel by its label (more readable)\n",
    "    .sel(channel=channel_label)\n",
    "    # Add the pressure level as a non-dimension coordinate\n",
    "    .assign_coords(isobaricInhPa=isobaricInhPa)\n",
    ")\n",
    "\n",
    "ds_pangu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gefs_members = ['gec00'] + [f'gep{i:02d}' for i in range(1, 31)]\n",
    "# Use pathlib for robust path management. Default to './gefs_data' if TMPDIR isn't set.\n",
    "gefsdir = Path(os.getenv(\"TMPDIR\", \"./gefs_data\")) / \"gefs\"\n",
    "gefsdir = Path(\"/glade/derecho/scratch/ahijevyc/tmp/gefs\")\n",
    "\n",
    "existing_files = set(p.resolve() for p in gefsdir.glob(\"*/??/atmos/pgrb2ap5/*pgrb2a.0p50.f???\"))\n",
    "\n",
    "# Base URL for the public NOAA GEFS S3 bucket\n",
    "base_url = \"https://noaa-gefs-pds.s3.amazonaws.com\"\n",
    "# ===================================================================\n",
    "# PHASE 1: DEFINE FILE REQUIREMENTS\n",
    "# ===================================================================\n",
    "print(\"--- Phase 1: Defining all required file paths ---\")\n",
    "required_files = []\n",
    "\n",
    "# Extend end by 120 hours (5 days) so we have \"truth\" to compare forecasts to.\n",
    "last_valid_time = init_time_range[-1] + pd.Timedelta(hours=120)\n",
    "extended_time_range = pd.date_range(start=init_time_range[0], end=last_valid_time, freq=\"24h\")\n",
    "for init_time in extended_time_range:\n",
    "    date_str = init_time.strftime('%Y%m%d')\n",
    "    cycle = init_time.strftime('%H')\n",
    "    for member in gefs_members:\n",
    "        for fhr in range(0, forecast_length+1, 24):\n",
    "            valid_time = init_time + pd.Timedelta(hours=fhr)\n",
    "            # We don't need non-zero-hour forecasts beyond last_valid_time.\n",
    "            if fhr > 0 and valid_time > last_valid_time:\n",
    "                #print(f\"don't need forecast {init_time} {fhr} {valid_time} past {last_valid_time}\")\n",
    "                continue\n",
    "            fhr_str = f\"{fhr:03d}\"\n",
    "            \n",
    "            # --- USING YOUR CORRECT FILE AND PATH STRUCTURE ---\n",
    "            s3_filename = f\"{member}.t{cycle}z.pgrb2a.0p50.f{fhr_str}\"\n",
    "            file_path_on_s3 = f\"gefs.{date_str}/{cycle}/atmos/pgrb2ap5/{s3_filename}\"\n",
    "            url = f\"{base_url}/{file_path_on_s3}\"\n",
    "\n",
    "            # Create a descriptive and unique local path\n",
    "            local_path = local_path_from_url(gefsdir, url)\n",
    "            required_files.append({'url': url, 'local_path': local_path})\n",
    "\n",
    "print(f\"‚úÖ Defined {len(required_files)} total files required for analysis.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"‚úÖ Data saved in: {gefsdir}\")\n",
    "print(\"-\" * 50)\n",
    "datasets = []\n",
    "\n",
    "for required_file in required_files:\n",
    "    url = required_file[\"url\"]\n",
    "    local_file_path = required_file[\"local_path\"]\n",
    "    if local_file_path in existing_files:\n",
    "        print(f\"   -> üü¢ File already exists. Skipping.\")\n",
    "    else:\n",
    "        print(local_file_path)\n",
    "        # Ensure the destination directory exists before downloading\n",
    "        Path(local_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   -> ‚¨áÔ∏è  Attempting to download: {url}\")\n",
    "        download_file(url, local_file_path)\n",
    "    ds_gfs = open_grib_dataset(local_file_path)\n",
    "    datasets.append(ds_gfs)\n",
    "\n",
    "ds_gfs = xr.combine_nested(datasets, concat_dim=[\"time\"])\n",
    "\n",
    "init_times = ds_gfs[\"valid_time\"] - ds_gfs[\"step\"]\n",
    "ds_gfs = ds_gfs.assign_coords(initialization_time=init_times)\n",
    "ds_gfs = ds_gfs.groupby([\"initialization_time\", \"step\", \"number\"]).first()\n",
    "\n",
    "step_as_hours = ds_gfs[\"step\"].data / pd.to_timedelta(\"1h\")\n",
    "ds_gfs = ds_gfs.assign_coords(forecast_hour=(\"step\", step_as_hours))\n",
    "ds_gfs = ds_gfs.swap_dims(step=\"forecast_hour\")\n",
    "ds_gfs[\"valid_time\"] = ds_gfs.initialization_time + ds_gfs.step\n",
    "\n",
    "ds_gfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the geographic selection to each dataset and re-assign it to the original variable\n",
    "ds_gfs = ds_gfs.sel(latitude=slice(60, 20), longitude=slice(220, 300)).load()\n",
    "ds_pangu = ds_pangu.sel(latitude=slice(60, 20), longitude=slice(220, 300)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gfs.valid_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = ds_gfs.sel(forecast_hour=0, drop=True).mean(dim=\"number\")  # drop forecast_hour coordinate\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# These will store the handles and labels for shared legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for ax, ds_model, title in zip(axes, [ds_gfs, ds_pangu], [\"GEFS\", \"Pangu-Weather\"]):\n",
    "    # --- Data processing steps (explained above) ---\n",
    "    ensemble_mean = ds_model.mean(dim=[\"number\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Data Reshaping (Stacking) ---\n",
    "    # To prepare for plotting against 'valid_time', we first need to reshape the\n",
    "    # data. `.stack()` combines the two dimensions into a single \"MultiIndex\"\n",
    "    # dimension named 'point'. The data is now effectively a 1D series where\n",
    "    # each data point is indexed by a (initialization_time, forecast_hour) pair.\n",
    "    stacked_ds = ensemble_mean.stack(point=(\"initialization_time\", \"forecast_hour\"))\n",
    "\n",
    "    # --- Swapping the Index ---\n",
    "    # This is a critical step. We replace the 'point' index with a new MultiIndex\n",
    "    # composed of 'initialization_time' and 'valid_time'. This aligns each\n",
    "    # data point with its specific start time and valid time, preparing it for the final pivot.\n",
    "    tidy_ds = stacked_ds.set_index(point=[\"initialization_time\", \"valid_time\"])\n",
    "\n",
    "    # --- Final Pivot (Unstacking) ---\n",
    "    # `.unstack()` performs the final pivot. It converts the data into the ideal 2D\n",
    "    # \"tidy\" format for plotting:\n",
    "    #   - Rows are indexed by 'valid_time' (our desired x-axis).\n",
    "    #   - Columns are indexed by 'initialization_time' (our desired series/hues).\n",
    "    plot_ready = tidy_ds[shortName].unstack(\"point\")\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plot_ready.plot.line(\n",
    "        x=\"valid_time\",\n",
    "        hue=\"initialization_time\",\n",
    "        ax=ax,\n",
    "        add_legend=False,\n",
    "    )\n",
    "    truth[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "        ax=ax, x=\"initialization_time\", marker=\"o\", color=\"k\"\n",
    "    )\n",
    "\n",
    "    # --- Axis formatting ---\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.set_xlabel(\"Valid Time\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # --- Capture handles and create labels ONCE from the first plot ---\n",
    "    if not handles:  # An empty list is False, so this runs only on the first iteration\n",
    "        handles = ax.get_lines()\n",
    "        # Create nicely formatted labels from the coordinate values\n",
    "        labels = [\n",
    "            pd.to_datetime(t).strftime(\"%Y-%m-%d %H:%M\")\n",
    "            for t in plot_ready.initialization_time.values\n",
    "        ]\n",
    "\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    handles, labels, title=\"Initialization Time\", loc=\"center right\", bbox_to_anchor=(1.0, 0.5)\n",
    ")\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "fig.tight_layout(rect=[0, 0, 0.88, 1])  # Adjusted rect to give legend more space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ready.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 5), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(axes, [ds_gfs, ds_pangu], [\"GEFS\", \"Pangu-Weather\"]):\n",
    "    # Average over spatial dimensions ONLY\n",
    "    # This keeps the 'number' dimension for the ensemble members.\n",
    "    ds_processed = ds_model.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "    # Loop over each initialization time\n",
    "    for init_time in ds_processed.initialization_time:\n",
    "        # Select all members for this single forecast run\n",
    "        run_with_members = ds_processed.sel(initialization_time=init_time)\n",
    "\n",
    "        # Plot the first member to establish the color and label for the legend\n",
    "        first_member_data = run_with_members.isel(number=0)\n",
    "        line = ax.plot(\n",
    "            first_member_data.valid_time,\n",
    "            first_member_data[shortName],\n",
    "            alpha=0.5,\n",
    "            label=pd.to_datetime(init_time.values).strftime(\"%Y-%m-%d %H:%M\"),\n",
    "        )\n",
    "        # Get the color matplotlib automatically assigned to the first line\n",
    "        run_color = line[0].get_color()\n",
    "\n",
    "        # Loop over the REST of the members and plot them with the same color\n",
    "        for member_index in range(1, len(run_with_members.number)):\n",
    "            member_data = run_with_members.isel(number=member_index)\n",
    "            ax.plot(\n",
    "                member_data.valid_time,\n",
    "                member_data[shortName],\n",
    "                alpha=0.5,\n",
    "                color=run_color,  # Reuse the color from the first member\n",
    "            )\n",
    "    truth[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "        ax=ax, x=\"initialization_time\", marker=\"o\", color=\"k\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.set_xlabel(\"Valid Time\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend(title=\"Initialization Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "HET4wJfH2khw",
    "outputId": "1cf2e7ac-477a-46f8-a0ec-6f79ba3f196e"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 5), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(axes, [ds_gfs, ds_pangu], [\"GEFS\", \"Pangu-Weather\"]):\n",
    "    # average over initialization_time, lat and lon. std over ensemble (number).\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName].std(dim=\"number\", ddof=1).mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    # error = ensemble mean - truth\n",
    "    error = ds_model.mean(dim=\"number\") - truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "    rmse.plot.line(ax=ax, x=\"forecast_hour\", marker=\"o\", label=\"rmse\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"forecast_hour\", marker=\"o\", label=\"spread\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NPL 2025a",
   "language": "python",
   "name": "npl-2025a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
