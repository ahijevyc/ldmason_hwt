{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2X-b5-NzDBVB",
    "outputId": "830cc9d8-d560-4244-a0f1-743a16e6735e"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from hwt import firstRun\n",
    "from metpy.constants import g\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Place where Dave cached datasets for quicker use\n",
    "CACHEDIR = Path(\"/glade/derecho/scratch/ahijevyc/ldmason_hwt\")\n",
    "\n",
    "now = pd.Timestamp.now()\n",
    "year = 2024\n",
    "init_start = firstRun(year)\n",
    "init_end = pd.to_datetime(f\"{year}0531\")\n",
    "\n",
    "init_times = pd.date_range(\n",
    "    init_start.floor(\"24h\"), init_end.floor(\"24h\"), freq=\"24h\"\n",
    ")  # round down to nearest multiple\n",
    "\n",
    "forecast_length = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel = True\n",
    "if parallel:\n",
    "    try:\n",
    "        client.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cluster.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    # Create a local cluster with workers (CPUs)\n",
    "    cluster = LocalCluster(threads_per_worker=1)\n",
    "    client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortName, isobaricInhPa, units = \"z\", 500, \"meter\"\n",
    "\n",
    "\n",
    "def build_file_url(init_time, mem, forecast_hour):\n",
    "    return (\n",
    "        f\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gens/prod/gefs.{init_time:%Y%m%d}/\"\n",
    "        f\"{init_time:%H}/atmos/pgrb2ap5/\"\n",
    "        f\"{mem}.t{init_time:%H}z.pgrb2a.0p50.f{forecast_hour:03d}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def local_path_from_url(gefsdir, file_url_str):\n",
    "    url_path = Path(file_url_str)\n",
    "    file_path = gefsdir.joinpath(*url_path.parts[-5:])\n",
    "    os.makedirs(file_path.parent, exist_ok=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def open_grib_dataset(path):\n",
    "    ds = xr.open_dataset(\n",
    "        path,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"errors\": \"ignore\"},\n",
    "        filter_by_keys={\n",
    "            \"typeOfLevel\": \"isobaricInhPa\",\n",
    "            \"level\": isobaricInhPa,\n",
    "            \"shortName\": \"gh\" if shortName == \"z\" else shortName,\n",
    "        },\n",
    "        decode_timedelta=True,\n",
    "        chunks={},  # chunking can help reduce memory usage\n",
    "    )\n",
    "    if shortName == \"z\":\n",
    "        ds = ds.rename(gh=\"z\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def download_file(url, local_file_path, delay_seconds=1):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully downloaded: {local_file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        print(f\"Could not download {url}\")\n",
    "\n",
    "\n",
    "def ai_ifiles(pangu_dir, model, init_time, mem, forecast_length):\n",
    "    # return list of files at different lead times.\n",
    "    if init_time > pd.to_datetime(\"20250101\"):\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"ens{mem}\" / f\"{model}_forecast_data\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_ens{mem}_pred_{i:03d}.nc\"\n",
    "            for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "    else:\n",
    "        control_or_perturbation = \"p\" if mem > 0 else \"c\"\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"{control_or_perturbation}{mem:02d}\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_gefs_pred_{i:03d}.nc\" for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "\n",
    "    return ifiles\n",
    "\n",
    "\n",
    "def add_ensemble_number(ds):\n",
    "    \"\"\"\n",
    "    A preprocessing function to be used with xr.open_mfdataset.\n",
    "    It extracts the ensemble member number from the source filename\n",
    "    and adds it as a 'number' coordinate.\n",
    "    \"\"\"\n",
    "    # Get the basename of the file (e.g., \"pangu_ens0_pred_162.nc\")\n",
    "    try:\n",
    "        filename = os.path.basename(ds.encoding[\"source\"])\n",
    "    except (KeyError, TypeError):\n",
    "        # Fallback if source encoding is not available\n",
    "        return ds\n",
    "\n",
    "    # Use a regular expression to find the number following 'ens'\n",
    "    match = re.search(r\"ens(\\d+)\", filename)\n",
    "\n",
    "    if match:\n",
    "        # Extract the number, convert to integer\n",
    "        ensemble_number = int(match.group(1))\n",
    "\n",
    "        # Add 'number' as a new dimension and assign the extracted number as its coordinate\n",
    "        return ds.expand_dims(number=[ensemble_number])\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_files(ai_dir, init_times, forecast_length, nmem=51):\n",
    "    # derive model from ai_dir\n",
    "    model = ai_dir.name.rstrip(\"_realtime\")\n",
    "    # Create a nested list where list[init_time][member] = [files_for_all_forecast_hours]\n",
    "    nested_files = []\n",
    "    for init_time in init_times:\n",
    "        # List for all members for this init_time\n",
    "        time_specific_files = []\n",
    "        for mem in range(nmem):\n",
    "            # List of all forecast hour files for this specific member\n",
    "            lead_times = ai_ifiles(ai_dir, model, init_time, mem, forecast_length)\n",
    "            if all([os.path.exists(f) for f in lead_times]):\n",
    "                time_specific_files.append(lead_times)\n",
    "            else:\n",
    "                print(f\"not all lead times present for {model} {mem} {init_time}\")\n",
    "\n",
    "        if len(time_specific_files) != nmem:\n",
    "            print(f\"only {len(time_specific_files)}/{nmem} {model} {init_time} files\")\n",
    "            continue\n",
    "        nested_files.append(time_specific_files)\n",
    "\n",
    "    return nested_files\n",
    "\n",
    "\n",
    "def merge_nested_files(nested_files, shortName, isobaricInhPa, units):\n",
    "    # The channel label we want to select\n",
    "    channel_label = f\"{shortName}{isobaricInhPa}\"\n",
    "\n",
    "    nmem = len(nested_files[0])\n",
    "    ds = (\n",
    "        xr.open_mfdataset(\n",
    "            nested_files,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"init_time\", \"number\", \"prediction_timedelta\"],\n",
    "            chunks=\"auto\",  # Use 'auto' for better performance with dask\n",
    "        )\n",
    "        # Rename dimensions and coordinates at the start\n",
    "        .rename(\n",
    "            {\n",
    "                \"init_time\": \"initialization_time\",\n",
    "                \"prediction_timedelta\": \"step\",\n",
    "                \"lat\": \"latitude\",\n",
    "                \"lon\": \"longitude\",\n",
    "                \"__xarray_dataarray_variable__\": shortName,  # Rename the main variable\n",
    "            }\n",
    "        )\n",
    "        # Assign the integer coordinate for the 'number' dimension\n",
    "        .assign_coords(number=range(nmem))\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        .pipe(\n",
    "            lambda ds: ds.assign_coords(\n",
    "                forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "            ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        )\n",
    "        # Calculate the valid_time coordinate\n",
    "        .assign(valid_time=lambda ds: ds.initialization_time + ds.step)\n",
    "        # Select the desired channel by its label (more readable)\n",
    "        .sel(channel=channel_label)\n",
    "        # Add the pressure level as a non-dimension coordinate\n",
    "        .assign_coords(isobaricInhPa=isobaricInhPa)\n",
    "    )\n",
    "\n",
    "    if shortName == \"z\":\n",
    "        if \"units\" in ds[\"z\"].attrs:\n",
    "            # Let metpy take care of the units\n",
    "            ds = ds.metpy.quantify()\n",
    "            print(\"divide by g (as Quantity with units)\")\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        else:\n",
    "            # no units, like in fengwu\n",
    "            print(\"divide by g (no units)\")\n",
    "            ds[\"z\"] /= g.m\n",
    "            ds[\"z\"].attrs.update({\"units\": units})\n",
    "    assert (\n",
    "        ds[shortName].attrs[\"units\"] == units\n",
    "    ), f\"expected units {units}. got {ds[shortName].attrs['units']}\"\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if init_start > pd.to_datetime(\"20250101\"):\n",
    "    # Output from real-time runs in Ryan's directory\n",
    "    pangu_dir = Path(\"/glade/derecho/scratch/sobash/pangu_realtime\")\n",
    "    nested_files = get_nested_files(pangu_dir, init_times, forecast_length)\n",
    "    ds_pangu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "\n",
    "    fengwu_dir = Path(\"/glade/derecho/scratch/sobash/fengwu_realtime\")\n",
    "    nested_files = get_nested_files(fengwu_dir, init_times, forecast_length)\n",
    "    ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "else:\n",
    "    ofile = (\n",
    "        CACHEDIR / f\"pangu.{init_start:%Y%m%d%H}-{init_end:%Y%m%d%H}.{forecast_length:03d}h.zarr\"\n",
    "    )\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_pangu = xr.open_zarr(ofile)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        # Output from older dates (HWT2023/4)\n",
    "        # Nested list\n",
    "        #                         init0          , ... ,           initn\n",
    "        # model_runs = [[gec0, gep1, ... , gep30], ... , [gec0, gep1, ... , gep30]]\n",
    "        model_runs = []\n",
    "        for init_time in init_times:\n",
    "            idir = Path(\n",
    "                \"/glade/derecho/scratch/ahijevyc/ai-models/output/panguweather\"\n",
    "            ) / init_time.strftime(\"%Y%m%d%H\")\n",
    "            ifiles = sorted(list(idir.glob(\"g??[0-9][0-9].grib\")))\n",
    "            if len(ifiles) == 31:\n",
    "                model_runs.append(ifiles)\n",
    "            else:\n",
    "                logging.warning(f\"{init_time} has {len(ifiles)}/31 pangu files in {idir}\")\n",
    "        ds = xr.open_mfdataset(\n",
    "            model_runs,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"errors\": \"ignore\"},\n",
    "            filter_by_keys={\n",
    "                \"typeOfLevel\": \"isobaricInhPa\",\n",
    "                \"level\": isobaricInhPa,\n",
    "                \"shortName\": shortName,  # Don't worry about z being called gh. It's called z.\n",
    "            },\n",
    "            decode_timedelta=True,\n",
    "            chunks={},  # chunking can help reduce memory usage\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"time\", \"number\"],\n",
    "        )\n",
    "        ds = ds.rename(time=\"initialization_time\")\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        ds = ds.assign_coords(\n",
    "            forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "        ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        # Just multiples of 24 hours, please.\n",
    "        ds = ds.sel(forecast_hour=range(0, forecast_length + 1, 24))\n",
    "        if shortName == \"z\":\n",
    "            # Let metpy take care of units (must be Quantity first)\n",
    "            # m^2/s^2 -> m\n",
    "            ds = ds.metpy.quantify()\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        ds_pangu = ds\n",
    "        ds_pangu.to_zarr(ofile)\n",
    "    ofile = (\n",
    "        CACHEDIR / f\"fengwu.{init_start:%Y%m%d%H}-{init_end:%Y%m%d%H}.{forecast_length:03d}h.zarr\"\n",
    "    )\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_fengwu = xr.open_zarr(ofile)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        fengwu_dir = Path(\"/glade/derecho/scratch/ahijevyc/ai-models/output/fengwu\")\n",
    "        nested_files = get_nested_files(fengwu_dir, init_times, forecast_length, nmem=31)\n",
    "        ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa, units)\n",
    "        ds_fengwu.to_zarr(ofile)\n",
    "ds_pangu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gefs_members = [\"gec00\"] + [f\"gep{i:02d}\" for i in range(1, 31)]\n",
    "gefsdir = Path(\"/glade/derecho/scratch/ahijevyc/tmp/gefs\")\n",
    "\n",
    "existing_files = set(p.resolve() for p in gefsdir.glob(\"*/??/atmos/pgrb2ap5/*pgrb2a.0p50.f???\"))\n",
    "\n",
    "# Base URL for the public NOAA GEFS S3 bucket\n",
    "base_url = \"https://noaa-gefs-pds.s3.amazonaws.com\"\n",
    "# ===================================================================\n",
    "# PHASE 1: DEFINE FILE REQUIREMENTS\n",
    "# ===================================================================\n",
    "print(\"--- Phase 1: Defining all required file paths ---\")\n",
    "required_files = []\n",
    "\n",
    "for init_time in init_times:\n",
    "    for member in gefs_members:\n",
    "        for fhr in range(0, forecast_length + 1, 24):\n",
    "            valid_time = init_time + pd.Timedelta(hours=fhr)\n",
    "\n",
    "            fhr_str = f\"{fhr:03d}\"\n",
    "            s3_filename = f\"{member}.t{init_time:%H}z.pgrb2a.0p50.f{fhr_str}\"\n",
    "            file_path_on_s3 = f\"gefs.{init_time:%Y%m%d}/{init_time:%H}/atmos/pgrb2ap5/{s3_filename}\"\n",
    "            url = f\"{base_url}/{file_path_on_s3}\"\n",
    "            local_path = local_path_from_url(gefsdir, url)\n",
    "            required_files.append({\"url\": url, \"local_path\": local_path})\n",
    "\n",
    "            # Now that we use ERA5 we don't need GEFS for the extra forecast valid times\n",
    "            # TODO: remove references to valid_time in this loop. Then you can remove\n",
    "            # method=\"nearest\" in the plot of \n",
    "            # 0-hour fcst at valid_time so we have \"truth\" to compare forecasts to.\n",
    "            s3_filename = f\"{member}.t{valid_time:%H}z.pgrb2a.0p50.f000\"\n",
    "            file_path_on_s3 = (\n",
    "                f\"gefs.{valid_time:%Y%m%d}/{valid_time:%H}/atmos/pgrb2ap5/{s3_filename}\"\n",
    "            )\n",
    "            url = f\"{base_url}/{file_path_on_s3}\"\n",
    "            local_path = local_path_from_url(gefsdir, url)\n",
    "            required_files.append({\"url\": url, \"local_path\": local_path})\n",
    "\n",
    "# required files will have duplicates where an init_time is also a valid_time\n",
    "# But that is okay, because if the local_path exists already the url is not downloaded\n",
    "print(f\"âœ… Defined {len(required_files)} files required for analysis (duplicates).\\n\")\n",
    "\n",
    "unique_list = []\n",
    "seen = set()\n",
    "for d in required_files:\n",
    "    # convert dict to tuple of key-value pairs. assumes order of dict keys is consistent\n",
    "    t = tuple(d.items())\n",
    "    if t not in seen:\n",
    "        seen.add(t)\n",
    "        unique_list.append(d)\n",
    "print(f\"  {len(unique_list)} unique files required for analysis.\\n\")\n",
    "required_files = unique_list\n",
    "\n",
    "print(f\"âœ… Data saved in: {gefsdir}\")\n",
    "print(\"-\" * 50)\n",
    "datasets = []\n",
    "\n",
    "ofile = CACHEDIR / f\"gefs{init_start:%Y%m%d%H}-{init_end:%Y%m%d%H}.{forecast_length:03d}h.zarr\"\n",
    "# Try saved ds_gefs. I tried saving zarr with compute=False but when I loaded, values were zero.\n",
    "if os.path.exists(ofile):\n",
    "    print(f\"open existing {ofile}\")\n",
    "    ds_gefs = xr.open_zarr(ofile)\n",
    "else:\n",
    "    print(f\"making {ofile}\")\n",
    "    for required_file in tqdm(required_files):\n",
    "        url = required_file[\"url\"]\n",
    "        local_file_path = required_file[\"local_path\"]\n",
    "        if local_file_path in existing_files:\n",
    "            # print(f\"   -> ðŸŸ¢ File already exists. Skipping.\")\n",
    "            pass\n",
    "        else:\n",
    "            print(local_file_path)\n",
    "            # Ensure the destination directory exists before downloading\n",
    "            Path(local_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   -> â¬‡ï¸  Attempting to download: {url}\")\n",
    "            download_file(url, local_file_path)\n",
    "        ds_gefs = open_grib_dataset(local_file_path)\n",
    "        datasets.append(ds_gefs)\n",
    "\n",
    "    ds_gefs = xr.combine_nested(datasets, concat_dim=[\"time\"])\n",
    "\n",
    "    itime_coords = ds_gefs[\"valid_time\"] - ds_gefs[\"step\"]\n",
    "    ds_gefs = ds_gefs.assign_coords(initialization_time=itime_coords)\n",
    "    ds_gefs = ds_gefs.groupby([\"initialization_time\", \"step\", \"number\"]).first()\n",
    "\n",
    "    step_as_hours = ds_gefs[\"step\"].data / pd.to_timedelta(\"1h\")\n",
    "    ds_gefs = ds_gefs.assign_coords(forecast_hour=(\"step\", step_as_hours))\n",
    "    ds_gefs = ds_gefs.swap_dims(step=\"forecast_hour\")\n",
    "    ds_gefs[\"valid_time\"] = ds_gefs.initialization_time + ds_gefs.step\n",
    "    ds_gefs.to_zarr(ofile)\n",
    "\n",
    "ds_gefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the geographic selection to each dataset and re-assign it to the original variable\n",
    "geographic_sel = dict(latitude=slice(60, 20), longitude=slice(220, 300))\n",
    "ds_gefs = ds_gefs.sel(geographic_sel).load()\n",
    "ds_pangu = ds_pangu.sel(geographic_sel).load()\n",
    "ds_fengwu = ds_fengwu.sel(geographic_sel).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in forecast_hour=0 in fengwu with forecast_hour=0 in pangu\n",
    "# drop \"valid_time\" coordinate or DTypePromotionError\n",
    "# tried xr.Dataset.combine_first instead of xr.concat but had troubles\n",
    "if 0 not in ds_fengwu.forecast_hour:\n",
    "    ds_fengwu = xr.concat(\n",
    "        [ds_pangu.sel(forecast_hour=0).reset_coords(\"valid_time\"), ds_fengwu], dim=\"forecast_hour\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = Path(\"/glade/campaign/collections/rda/data/ds633.0/e5.oper.an.pl\")\n",
    "freq = pd.to_timedelta(\"24h\")\n",
    "# tack on extra truth times to compare to the forecast valid times in the last forecast\n",
    "extra_times = pd.date_range(\n",
    "    start=init_times[-1] + freq,\n",
    "    end=init_times[-1] + pd.to_timedelta(f\"{forecast_length}h\"),\n",
    "    freq=freq,\n",
    ")\n",
    "era5_files = [\n",
    "    era5_dir\n",
    "    / i.strftime(\"%Y%m\")\n",
    "    / f\"e5.oper.an.pl.128_129_{shortName}.ll025sc.{i:%Y%m%d00}_{i:%Y%m%d23}.nc\"\n",
    "    for i in init_times.union(extra_times)\n",
    "]\n",
    "truth = (\n",
    "    xr.open_mfdataset(era5_files)\n",
    "    .sel(level=isobaricInhPa)\n",
    "    .sel(geographic_sel)  # geographic selection\n",
    "    .rename(Z=\"z\", time=\"valid_time\", level=\"isobaricInhPa\")\n",
    ")\n",
    "# era5 available every hour. just return 0z please\n",
    "truth = truth.sel(valid_time=(truth.valid_time.dt.hour == 0))\n",
    "if shortName == \"z\":\n",
    "    truth[\"z\"] = truth[\"z\"] / g.m\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# These will store the handles and labels for shared legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes, [ds_gefs, ds_pangu, ds_fengwu], [\"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "):\n",
    "    # --- Data processing steps (explained above) ---\n",
    "    ensemble_mean = ds_model.mean(dim=[\"number\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Data Reshaping (Stacking) ---\n",
    "    # To prepare for plotting against 'valid_time', we first need to reshape the\n",
    "    # data. `.stack()` combines the two dimensions into a single \"MultiIndex\"\n",
    "    # dimension named 'point'. The data is now effectively a 1D series where\n",
    "    # each data point is indexed by a (initialization_time, forecast_hour) pair.\n",
    "    stacked_ds = ensemble_mean.stack(point=(\"initialization_time\", \"forecast_hour\"))\n",
    "\n",
    "    # --- Swapping the Index ---\n",
    "    # This is a critical step. We replace the 'point' index with a new MultiIndex\n",
    "    # composed of 'initialization_time' and 'valid_time'. This aligns each\n",
    "    # data point with its specific start time and valid time, preparing it for the final pivot.\n",
    "    tidy_ds = stacked_ds.set_index(point=[\"initialization_time\", \"valid_time\"])\n",
    "\n",
    "    # --- Final Pivot (Unstacking) ---\n",
    "    # `.unstack()` performs the final pivot. It converts the data into the ideal 2D\n",
    "    # \"tidy\" format for plotting:\n",
    "    #   - Rows are indexed by 'valid_time' (our desired x-axis).\n",
    "    #   - Columns are indexed by 'initialization_time' (our desired series/hues).\n",
    "    plot_ready = tidy_ds[shortName].unstack(\"point\")\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plot_ready.plot.line(\n",
    "        x=\"valid_time\",\n",
    "        hue=\"initialization_time\",\n",
    "        ax=ax,\n",
    "        add_legend=False,\n",
    "    )\n",
    "    truth[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "        ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\"\n",
    "    )\n",
    "\n",
    "    # --- Axis formatting ---\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.set_xlabel(\"Valid Time\")\n",
    "    ax.set_ylabel(f\"{shortName} [{units}]\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # --- Capture handles and create labels ONCE from the first plot ---\n",
    "    if not handles:  # An empty list is False, so this runs only on the first iteration\n",
    "        handles = ax.get_lines()\n",
    "        # Create nicely formatted labels from the coordinate values\n",
    "        labels = [\n",
    "            pd.to_datetime(t).strftime(\"%Y-%m-%d %Hz\")\n",
    "            for t in plot_ready.initialization_time.values\n",
    "        ]\n",
    "\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    handles, labels, title=\"Initialization Time\", loc=\"center right\", bbox_to_anchor=(1.0, 0.5)\n",
    ")\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "fig.tight_layout(rect=[0, 0, 0.88, 1])  # Adjusted rect to give legend more space\n",
    "#ax.set_xlim((19850.65, 19862.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "\n",
    "# These will store the handles and labels for shared legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes, [ds_gefs, ds_pangu, ds_fengwu], [\"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "):\n",
    "    # Average over spatial dimensions ONLY\n",
    "    # This keeps the 'number' dimension for the ensemble members.\n",
    "    ds_processed = ds_model.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "    # Loop over each initialization time\n",
    "    for init_time in ds_processed.initialization_time:\n",
    "        # no spaghetti lines for run after last init_time\n",
    "        if init_time > init_times[-1]:\n",
    "            continue\n",
    "        # Select all members for this single forecast run\n",
    "        run_with_members = ds_processed.sel(initialization_time=init_time)\n",
    "\n",
    "        # Plot the first member to establish the color and label for the legend\n",
    "        first_member_data = run_with_members.isel(number=0)\n",
    "        line = ax.plot(\n",
    "            first_member_data.valid_time,\n",
    "            first_member_data[shortName],\n",
    "            alpha=0.5,\n",
    "            label=pd.to_datetime(init_time.values).strftime(\"%Y-%m-%d %hz\"),\n",
    "        )\n",
    "        # Get the color matplotlib automatically assigned to the first line\n",
    "        run_color = line[0].get_color()\n",
    "\n",
    "        # Loop over the REST of the members and plot them with the same color\n",
    "        for member_index in ds_model.number[1:]:\n",
    "            member_data = run_with_members.sel(number=member_index)\n",
    "            ax.plot(\n",
    "                member_data.valid_time,\n",
    "                member_data[shortName],\n",
    "                alpha=0.5,\n",
    "                color=run_color,  # Reuse the color from the first member\n",
    "            )\n",
    "    truth[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "        ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.set_xlabel(\"Valid Time\")\n",
    "    ax.set_ylabel(f\"{shortName} [{units}]\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    # --- Capture handles and create labels ONCE from the first plot ---\n",
    "    if not handles:  # An empty list is False, so this runs only on the first iteration\n",
    "        handles = ax.get_lines()[::31]\n",
    "        # Create nicely formatted labels from the coordinate values\n",
    "        labels = [\n",
    "            pd.to_datetime(t).strftime(\"%Y-%m-%d %H:%M\")\n",
    "            for t in plot_ready.initialization_time.values\n",
    "        ]\n",
    "\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    handles, labels, title=\"Initialization Time\", loc=\"center right\", bbox_to_anchor=(1.0, 0.5)\n",
    ")\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "fig.tight_layout(rect=[0, 0, 0.88, 1])  # Adjusted rect to give legend more space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "HET4wJfH2khw",
    "outputId": "1cf2e7ac-477a-46f8-a0ec-6f79ba3f196e"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes, [ds_gefs, ds_pangu, ds_fengwu], [\"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "):\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "    # average over initialization_time, lat and lon. std over ensemble (number).\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    # error = ensemble mean - truth\n",
    "    # Line up truth and forecast along `valid_time`.\n",
    "    error = ds_model.mean(dim=\"number\") - truth.sel(valid_time=ds_model[\"valid_time\"], method=\"nearest\")\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"rmse\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"spread\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"{shortName} [{units}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pangu[shortName].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pangu[shortName].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pangu[shortName].min(), ds_pangu[shortName].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ds_pangu[\"z\"]\n",
    "# --- Please run this exact block of code with your DataArray 'z' ---\n",
    "\n",
    "# 1. Print the full representation of your object 'z'\n",
    "# This will show its name, dimensions, coordinates, and attributes.\n",
    "print(\"--- Full representation of the DataArray 'z' ---\")\n",
    "print(z)\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "# 2. Calculate all three statistics back-to-back\n",
    "min_val = z.min()\n",
    "max_val = z.max()\n",
    "mean_val = z.mean()\n",
    "\n",
    "# 3. Print the results\n",
    "print(f\"Min value from .min(): {min_val.item()}\")\n",
    "print(f\"Max value from .max(): {max_val.item()}\")\n",
    "print(f\"Mean value from .mean(): {mean_val.item()}\")\n",
    "\n",
    "\n",
    "# 4. Perform the crucial consistency check\n",
    "is_consistent = min_val <= mean_val <= max_val\n",
    "\n",
    "print(\"\\n--- Consistency Check ---\")\n",
    "print(f\"Is the result mathematically consistent? {is_consistent}\")\n",
    "print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This forces a true NumPy calculation on the in-memory data,\n",
    "# bypassing any xarray/cfgrib backend magic.\n",
    "true_mean = z.values.mean()\n",
    "\n",
    "print(f\"The TRUE mathematical mean is: {true_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(z.attrs['GRIB_paramId'])\n",
    "# del(z.attrs['GRIB_cfName'])\n",
    "# del(z.attrs['GRIB_cfVarName'])\n",
    "# del(z.attrs['GRIB_name'])\n",
    "# del(z.attrs['GRIB_shortName'])\n",
    "z.attrs.update({\"long_name\": \"Geopotential Height\", \"standard_name\": \"geopotential height\"})\n",
    "z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fengwu[shortName].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gefs[shortName].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pangu.sel(forecast_hour=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fengwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pangu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NPL 2025a",
   "language": "python",
   "name": "npl-2025a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
