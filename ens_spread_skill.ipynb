{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need >77 GB by the time CRPS cells are run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2X-b5-NzDBVB",
    "outputId": "830cc9d8-d560-4244-a0f1-743a16e6735e"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import hwt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import properscoring as ps\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from earth2studio.data.mpas_ens import MPASEnsemble\n",
    "from hwt import get_dynamics_model, init_times\n",
    "from metpy.constants import g\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import era5_varid, gefs_members, get_graphcast_output\n",
    "\n",
    "# Place where Dave cached datasets for quicker use\n",
    "CACHEDIR = Path(\"/glade/derecho/scratch/ahijevyc/ldmason_hwt\")\n",
    "\n",
    "PLOTDIR = Path(\"plots\")\n",
    "if not os.path.exists(PLOTDIR):\n",
    "    os.makedirs(PLOTDIR, exist_ok=True)\n",
    "\n",
    "# sorted list of unique years in init_times\n",
    "years = sorted(list(set(i.year for i in init_times)))\n",
    "\n",
    "forecast_length = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel = True\n",
    "if parallel:\n",
    "    try:\n",
    "        client.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cluster.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    # Create a local cluster with workers (CPUs)\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables may be changed between temperature and height\n",
    "shortName = \"z\"\n",
    "isobaricInhPa = 500\n",
    "units = \"m\"\n",
    "\n",
    "\n",
    "def local_path_from_url(gefsdir, file_url_str):\n",
    "    url_path = Path(file_url_str)\n",
    "    file_path = gefsdir.joinpath(*url_path.parts[-5:])\n",
    "    os.makedirs(file_path.parent, exist_ok=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def open_grib_dataset(path):\n",
    "    ds = xr.open_dataset(\n",
    "        path,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"errors\": \"ignore\"},\n",
    "        filter_by_keys={\n",
    "            \"typeOfLevel\": \"isobaricInhPa\",\n",
    "            \"level\": isobaricInhPa,\n",
    "            \"shortName\": \"gh\" if shortName == \"z\" else shortName,\n",
    "        },\n",
    "        decode_timedelta=True,\n",
    "        chunks={},  # chunking can help reduce memory usage\n",
    "    )\n",
    "    if shortName == \"z\":\n",
    "        ds = ds.rename(gh=\"z\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def ai_ifiles(pangu_dir, model, init_time, mem, forecast_length):\n",
    "    # return list of files at different lead times.\n",
    "    if init_time > pd.to_datetime(\"20250101\"):\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"ens{mem}\" / f\"{model}_forecast_data\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_ens{mem}_pred_{i:03d}.nc\"\n",
    "            for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "    else:\n",
    "        control_or_perturbation = \"p\" if mem > 0 else \"c\"\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"{control_or_perturbation}{mem:02d}\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_gefs_pred_{i:03d}.nc\" for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "\n",
    "    return ifiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_files(ai_dir, init_times, forecast_length, nmem=51):\n",
    "    # derive model from ai_dir\n",
    "    model = ai_dir.name.rstrip(\"_realtime\")\n",
    "    # Create a nested list where list[init_time][member] = [files_for_all_forecast_hours]\n",
    "    nested_files = []\n",
    "    for init_time in init_times:\n",
    "        # List for all members for this init_time\n",
    "        time_specific_files = []\n",
    "        for mem in range(nmem):\n",
    "            # List of all forecast hour files for this specific member\n",
    "            lead_times = ai_ifiles(ai_dir, model, init_time, mem, forecast_length)\n",
    "            if all([os.path.exists(f) for f in lead_times]):\n",
    "                time_specific_files.append(lead_times)\n",
    "            else:\n",
    "                print(f\"not all lead times present for {model} {mem} {init_time}\")\n",
    "\n",
    "        if len(time_specific_files) != nmem:\n",
    "            print(f\"only {len(time_specific_files)}/{nmem} {model} {init_time} files\")\n",
    "            continue\n",
    "        nested_files.append(time_specific_files)\n",
    "\n",
    "    return nested_files\n",
    "\n",
    "\n",
    "def merge_nested_files(nested_files, shortName, isobaricInhPa, units):\n",
    "    # The channel label we want to select\n",
    "    channel_label = f\"{shortName}{isobaricInhPa}\"\n",
    "\n",
    "    nmem = len(nested_files[0])\n",
    "    ds = (\n",
    "        xr.open_mfdataset(\n",
    "            nested_files,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"init_time\", \"number\", \"prediction_timedelta\"],\n",
    "            decode_timedelta=True,\n",
    "            chunks=\"auto\",  # Use 'auto' for better performance with dask\n",
    "        )\n",
    "        # Rename dimensions and coordinates at the start\n",
    "        .rename(\n",
    "            {\n",
    "                \"init_time\": \"initialization_time\",\n",
    "                \"prediction_timedelta\": \"step\",\n",
    "                \"lat\": \"latitude\",\n",
    "                \"lon\": \"longitude\",\n",
    "                \"__xarray_dataarray_variable__\": shortName,  # Rename the main variable\n",
    "            }\n",
    "        )\n",
    "        # Assign the integer coordinate for the 'number' dimension\n",
    "        .assign_coords(number=range(nmem))\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        .pipe(\n",
    "            lambda ds: ds.assign_coords(\n",
    "                forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "            ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        )\n",
    "        # Calculate the valid_time coordinate\n",
    "        .assign_coords(valid_time=lambda ds: ds.initialization_time + ds.step)\n",
    "        # Select the desired channel by its label (more readable)\n",
    "        .sel(channel=channel_label)\n",
    "        # Add the pressure level as a non-dimension coordinate\n",
    "        .assign_coords(isobaricInhPa=isobaricInhPa)\n",
    "    )\n",
    "\n",
    "    if shortName == \"z\":\n",
    "        if \"units\" in ds[\"z\"].attrs:\n",
    "            # Let metpy take care of the units\n",
    "            ds = ds.metpy.quantify()\n",
    "            print(\"divide by g (as Quantity with units)\")\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        else:\n",
    "            # no units, like in fengwu\n",
    "            print(\"divide by g (no units)\")\n",
    "            ds[\"z\"] /= g.m\n",
    "            ds[\"z\"].attrs.update({\"units\": units})\n",
    "    if \"units\" in ds[shortName].attrs:\n",
    "        assert (\n",
    "            ds[shortName].attrs[\"units\"] == units\n",
    "        ), f\"expected units {units}. got {ds[shortName].attrs['units']}\"\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 2025 in years:\n",
    "    # Output from real-time runs in Ryan's directory\n",
    "    pangu_dir = Path(\"/glade/derecho/scratch/sobash/pangu_realtime\")\n",
    "    nested_files = get_nested_files(pangu_dir, init_times, forecast_length)\n",
    "    ds_pangu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "\n",
    "    fengwu_dir = Path(\"/glade/derecho/scratch/sobash/fengwu_realtime\")\n",
    "    nested_files = get_nested_files(fengwu_dir, init_times, forecast_length)\n",
    "    ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "else:\n",
    "    ofile = CACHEDIR / f\"pangu.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_pangu = xr.open_zarr(ofile, decode_timedelta=False)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        # Output from older dates (HWT2023/4)\n",
    "        # Nested list\n",
    "        #                         init0          , ... ,           initn\n",
    "        # model_runs = [[gec0, gep1, ... , gep30], ... , [gec0, gep1, ... , gep30]]\n",
    "        model_runs = []\n",
    "        for init_time in init_times:\n",
    "            idir = Path(\n",
    "                \"/glade/derecho/scratch/ahijevyc/ai-models/output/panguweather\"\n",
    "            ) / init_time.strftime(\"%Y%m%d%H\")\n",
    "            ifiles = sorted(list(idir.glob(\"g??[0-9][0-9].grib\")))\n",
    "            if len(ifiles) == 31:\n",
    "                model_runs.append(ifiles)\n",
    "            else:\n",
    "                logging.warning(f\"{init_time} has {len(ifiles)}/31 pangu files in {idir}\")\n",
    "        ds = xr.open_mfdataset(\n",
    "            model_runs,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"errors\": \"ignore\"},\n",
    "            filter_by_keys={\n",
    "                \"typeOfLevel\": \"isobaricInhPa\",\n",
    "                \"level\": isobaricInhPa,\n",
    "                \"shortName\": shortName,  # Don't worry about z being called gh. It's called z.\n",
    "            },\n",
    "            decode_timedelta=True,\n",
    "            chunks={},  # chunking can help reduce memory usage\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"time\", \"number\"],\n",
    "        )\n",
    "        ds = ds.rename(time=\"initialization_time\")\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        ds = ds.assign_coords(\n",
    "            forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "        ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        # Just multiples of 24 hours, please.\n",
    "        ds = ds.sel(forecast_hour=range(0, forecast_length + 1, 24))\n",
    "        if shortName == \"z\":\n",
    "            # Let metpy take care of units (must be Quantity first)\n",
    "            # m^2/s^2 -> m\n",
    "            ds = ds.metpy.quantify()\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        ds_pangu = ds\n",
    "        ds_pangu.to_zarr(ofile)\n",
    "    ofile = CACHEDIR / f\"fengwu.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_fengwu = xr.open_zarr(ofile, decode_timedelta=False)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        fengwu_dir = Path(\"/glade/derecho/scratch/ahijevyc/ai-models/output/fengwu\")\n",
    "        nested_files = get_nested_files(fengwu_dir, init_times, forecast_length, nmem=31)\n",
    "        ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa, units)\n",
    "        ds_fengwu.to_zarr(ofile)\n",
    "\n",
    "\n",
    "ds_pangu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_graphcast = get_graphcast_output(shortName, isobaricInhPa)\n",
    "ds_graphcast = ds_graphcast.sel(forecast_hour=slice(None, forecast_length))\n",
    "\n",
    "ds_graphcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ofile = \"mpas15-3km.zarr\"\n",
    "# native horizontal mpas mesh\n",
    "if not os.path.exists(ofile):\n",
    "    datadir = Path(\"/glade/campaign/mmm/parc/schwartz\")\n",
    "    # 2023\n",
    "    mpas_datasrc = MPASEnsemble(\n",
    "        grid_path=datadir / \"MPAS/15-3km_mesh/grid_mesh/x5.6488066.grid_CONUS.nc\",\n",
    "        data_path=f\"{datadir}/HWT2023/mpas\",\n",
    "    )\n",
    "    for initialization_time in tqdm(init_times23):\n",
    "        objs = []\n",
    "        for forecast_hour in range(0, int(5.5 * 24) + 1, 24):\n",
    "            ds = mpas_datasrc(initialization_time, variables=[\"t850\", \"z500\"], fhr=forecast_hour)\n",
    "            objs.append(\n",
    "                ds.expand_dims(\n",
    "                    initialization_time=[initialization_time], forecast_hour=[forecast_hour]\n",
    "                ).squeeze(\"Time\")\n",
    "            )\n",
    "        ds = xr.combine_by_coords(objs, combine_attrs=\"override\")\n",
    "        if os.path.exists(ofile):\n",
    "            # consolidated=False because it's not official part of version 3 and local storage means it won't help much\n",
    "            ds.to_zarr(ofile, mode=\"a\", append_dim=\"initialization_time\", consolidated=False)\n",
    "        else:\n",
    "            ds.to_zarr(ofile, mode=\"w\", consolidated=False)\n",
    "    # 2024\n",
    "    mpas_datasrc = MPASEnsemble(\n",
    "        grid_path=datadir / \"MPAS/15-3km_mesh/grid_mesh/x5.6488066.grid_CONUS.nc\",\n",
    "        data_path=f\"{datadir}/HWT2024/mpas\",\n",
    "    )\n",
    "    for initialization_time in tqdm(init_times24):\n",
    "        objs = []\n",
    "        for forecast_hour in range(0, int(5.5 * 24) + 1, 24):\n",
    "            ds = mpas_datasrc(initialization_time, variables=[\"t850\", \"z500\"], fhr=forecast_hour)\n",
    "            objs.append(\n",
    "                ds.expand_dims(\n",
    "                    initialization_time=[initialization_time], forecast_hour=[forecast_hour]\n",
    "                ).squeeze(\"Time\")\n",
    "            )\n",
    "        ds = xr.combine_by_coords(objs, combine_attrs=\"override\")\n",
    "        ds.to_zarr(ofile, mode=\"a\", append_dim=\"initialization_time\", consolidated=False)\n",
    "\n",
    "ds_mpas = (\n",
    "    xr.open_zarr(ofile, consolidated=False)[\"__xarray_dataarray_variable__\"]\n",
    "    .to_dataset(dim=\"variable\")\n",
    "    .rename(\n",
    "        lat=\"latitude\",\n",
    "        lon=\"longitude\",\n",
    "        member=\"number\",\n",
    "        t850=\"t\",\n",
    "        z500=\"z\",\n",
    "    )\n",
    "    .assign_coords(\n",
    "        valid_time=lambda x: x[\"initialization_time\"] + x[\"forecast_hour\"] * pd.Timedelta(\"1h\")\n",
    "    )\n",
    "    .isel(latitude=slice(None, None, -1))\n",
    ")\n",
    "ds_mpas[\"z\"] = ds_mpas[\"z\"] / g.m\n",
    "ds_mpas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolated to lat/lon \"interp_mpas_3km_*.nc\" files\n",
    "zarrfile = \"mpas_interp.zarr\"\n",
    "if os.path.exists(zarrfile):\n",
    "    ds_mpas_interp = xr.open_zarr(zarrfile)\n",
    "else:\n",
    "    # get_dynamics_model also saves f\"{model}.zarr\"\n",
    "    # Reads interpolated files f\"interp_{model}_3km_{init_time:%Y%m%d%H}_mem{mem}_f{fhr:03d}.nc\"\n",
    "    # Not native hybrid levels.\n",
    "    ds_mpas_interp: xr.Dataset = get_dynamics_model(\n",
    "        model=\"mpas\",\n",
    "        forecast_hours=range(0, 132 + 1, 24),  # MPAS only goes 5.5d\n",
    "        ens_size=5,  # only 5/10 members interpolated to lat/lon/pressure_level and saved\n",
    "        vars_dict={\"temperature_850hPa\": \"t\", \"height_500hPa\": \"z\"},\n",
    "        init_times=init_times,\n",
    "        output_grid=ds_pangu,\n",
    "        CACHEDIR=CACHEDIR,\n",
    "    )\n",
    "ds_mpas_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarrfile = \"fv3.zarr\"\n",
    "if os.path.exists(zarrfile):\n",
    "    ds_fv3 = xr.open_zarr(zarrfile)\n",
    "else:\n",
    "    ds_fv3: xr.Dataset = get_dynamics_model(\n",
    "        \"fv3\",\n",
    "        forecast_hours=range(24, 204 + 1, 24),\n",
    "        ens_size=10,\n",
    "        vars_dict={\"TMP850\": \"t\", \"HGT500\": \"z\"},\n",
    "        init_times=init_times,\n",
    "        output_grid=ds_pangu,\n",
    "        CACHEDIR=CACHEDIR,\n",
    "    )\n",
    "ds_fv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = dict(\n",
    "    initialization_time=\"20240501\",\n",
    "    forecast_hour=120,\n",
    "    number=2,\n",
    "    longitude=slice(230, 290),\n",
    "    latitude=slice(52, 21),\n",
    ")\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(18, 4))\n",
    "vmin = 210\n",
    "ds_mpas.sel(sel).t.plot(ax=axes[0], vmin=vmin)\n",
    "ds_mpas_interp.sel(sel)[shortName].plot(ax=axes[1], vmin=vmin)\n",
    "qm = (ds_mpas - ds_mpas_interp).sel(sel)[shortName].plot(ax=axes[2], vmin=-1, vmax=1)\n",
    "qm.figure.suptitle(\"native mpas mesh vs lat-lon interpolated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gefsdir = Path(\"/glade/derecho/scratch/ahijevyc/tmp/gefs\")\n",
    "\n",
    "ofile = CACHEDIR / f\"gefs.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "\n",
    "# Try saved ds_gefs. I tried saving zarr with compute=False but when I loaded, values were zero.\n",
    "if os.path.exists(ofile):\n",
    "    print(f\"open existing {ofile}\")\n",
    "    ds_gefs = xr.open_zarr(ofile, decode_timedelta=False)\n",
    "else:\n",
    "    existing_files = set(p.resolve() for p in gefsdir.glob(\"*/??/atmos/pgrb2ap5/*pgrb2a.0p50.f???\"))\n",
    "\n",
    "    # Base URL for the public NOAA GEFS S3 bucket\n",
    "    base_url = \"https://noaa-gefs-pds.s3.amazonaws.com\"\n",
    "\n",
    "    # ===================================================================\n",
    "    # PHASE 1: DEFINE FILE REQUIREMENTS\n",
    "    # ===================================================================\n",
    "    print(\"--- Phase 1: Defining all required file paths ---\")\n",
    "    required_files = []\n",
    "\n",
    "    for init_time in init_times:\n",
    "        for member in gefs_members:\n",
    "            for fhr in range(0, forecast_length + 1, 24):\n",
    "                fhr_str = f\"{fhr:03d}\"\n",
    "                s3_filename = f\"{member}.t{init_time:%H}z.pgrb2a.0p50.f{fhr_str}\"\n",
    "                file_path_on_s3 = (\n",
    "                    f\"gefs.{init_time:%Y%m%d}/{init_time:%H}/atmos/pgrb2ap5/{s3_filename}\"\n",
    "                )\n",
    "                url = f\"{base_url}/{file_path_on_s3}\"\n",
    "                local_path = local_path_from_url(gefsdir, url)\n",
    "                required_files.append({\"url\": url, \"local_path\": local_path})\n",
    "\n",
    "    print(f\"âœ… Defined {len(required_files)} files required for analysis.\\n\")\n",
    "    print(f\"âœ… Data saved in: {gefsdir}\")\n",
    "    print(\"-\" * 50)\n",
    "    datasets = []\n",
    "\n",
    "    print(f\"making {ofile}\")\n",
    "    for required_file in tqdm(required_files):\n",
    "        url = required_file[\"url\"]\n",
    "        local_file_path = required_file[\"local_path\"]\n",
    "        if local_file_path in existing_files:\n",
    "            # print(f\"   -> ðŸŸ¢ File already exists. Skipping.\")\n",
    "            pass\n",
    "        else:\n",
    "            print(local_file_path)\n",
    "            # Ensure the destination directory exists before downloading\n",
    "            Path(local_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   -> â¬‡ï¸  Attempting to download: {url}\")\n",
    "            download_file(url, local_file_path)\n",
    "        ds_gefs = open_grib_dataset(local_file_path)\n",
    "        datasets.append(ds_gefs)\n",
    "\n",
    "    ds_gefs = xr.combine_nested(datasets, concat_dim=[\"time\"])\n",
    "\n",
    "    itime_coords = ds_gefs[\"valid_time\"] - ds_gefs[\"step\"]\n",
    "    ds_gefs = ds_gefs.assign_coords(initialization_time=itime_coords)\n",
    "    ds_gefs = ds_gefs.groupby([\"initialization_time\", \"step\", \"number\"]).first()\n",
    "\n",
    "    step_as_hours = ds_gefs[\"step\"].data / pd.to_timedelta(\"1h\")\n",
    "    ds_gefs = ds_gefs.assign_coords(forecast_hour=(\"step\", step_as_hours))\n",
    "    ds_gefs = ds_gefs.swap_dims(step=\"forecast_hour\")\n",
    "    ds_gefs = ds_gefs.assign_coords(valid_time=ds_gefs.initialization_time + ds_gefs.step)\n",
    "    ds_gefs.to_zarr(ofile)\n",
    "\n",
    "ds_gefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = Path(\"/glade/campaign/collections/rda/data/d633000/e5.oper.an.pl\")\n",
    "\n",
    "# tack on extra truth times to compare to the forecast valid times\n",
    "truth_times = init_times\n",
    "for init_time in init_times:\n",
    "    valid_times = pd.date_range(init_time, init_time + pd.to_timedelta(f\"{forecast_length}h\"))\n",
    "    truth_times = truth_times.union(valid_times)\n",
    "\n",
    "era5_files = [\n",
    "    era5_dir\n",
    "    / i.strftime(\"%Y%m\")\n",
    "    / f\"e5.oper.an.pl.128_{era5_varid[shortName]}_{shortName}.ll025sc.{i:%Y%m%d00}_{i:%Y%m%d23}.nc\"\n",
    "    for i in truth_times\n",
    "]\n",
    "truth = (\n",
    "    xr.open_mfdataset(era5_files)\n",
    "    .sel(level=isobaricInhPa)\n",
    "    .rename(time=\"valid_time\", level=\"isobaricInhPa\")\n",
    "    .rename({shortName.upper(): shortName})  # T->t, Z->z. uppercase shortName to lower\n",
    "    .drop_vars(\"utc_date\")\n",
    ")\n",
    "# era5 available every hour. just return 0z please\n",
    "truth = truth.sel(valid_time=(truth.valid_time.dt.hour == 0))\n",
    "\n",
    "# convert geopotential to geopotential height\n",
    "if shortName == \"z\":\n",
    "    truth[\"z\"] = truth[\"z\"] / g.m\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_graphcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the geographic selection to each dataset, mask HWT region\n",
    "geographic_sel = dict(latitude=slice(60, 20), longitude=slice(220, 300))\n",
    "print(\"mpas\")\n",
    "ds_mpas = ds_mpas.sel(geographic_sel).where(hwt.mask0p25).load()\n",
    "print(\"mpas_interp\")\n",
    "ds_mpas_interp = ds_mpas_interp.sel(geographic_sel).where(hwt.mask0p25).load()\n",
    "print(\"fv3\")\n",
    "ds_fv3 = ds_fv3.where(hwt.mask0p25).load()\n",
    "print(\"gefs\")\n",
    "ds_gefs = ds_gefs.sel(geographic_sel).where(hwt.mask0p5).load()\n",
    "if parallel:\n",
    "    try:\n",
    "        client.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cluster.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    # Create a local cluster with workers (CPUs)\n",
    "    cluster = LocalCluster(threads_per_worker=2)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(client.dashboard_link)\n",
    "print(client)\n",
    "print(\"pangu\")\n",
    "ds_pangu = (\n",
    "    ds_pangu.sel(geographic_sel).where(hwt.mask0p25).load()\n",
    ")  # couldn't load with mem split among 16 cpus. try fewer\n",
    "print(\"fengwu\")\n",
    "ds_fengwu = ds_fengwu.sel(geographic_sel).where(hwt.mask0p25).load()\n",
    "print(\"graphcast\")\n",
    "ds_graphcast = ds_graphcast.where(hwt.mask0p25).load()\n",
    "print(\"truth\")\n",
    "truth = truth.sel(geographic_sel).where(hwt.mask0p25).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assumes the following variables are already loaded in your script:\n",
    "# ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengw, ds_graphcast, : xr.Dataset objects with model data\n",
    "# truth: xr.Dataset with the ground truth data\n",
    "# shortName: A string representing the variable name (e.g., 't' for temperature)\n",
    "# units: A string for the y-axis label (e.g., 'K' for Kelvin)\n",
    "# isobaricInhPa: An integer representing the pressure level (e.g., 500)\n",
    "\n",
    "\n",
    "# These will store the handles and labels for the shared legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "# --- Define models and years to iterate over ---\n",
    "models_data = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast]\n",
    "model_titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Graphcast\"]\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# Create a nxm grid of subplots: n rows (for years), m columns (for models)\n",
    "fig, axes = plt.subplots(nrows=len(years), ncols=len(models_data), figsize=(22, 9), sharey=True)\n",
    "\n",
    "\n",
    "# --- Main Loop: Iterate over years and models ---\n",
    "# Loop over each year to create a row of plots\n",
    "for row_idx, year in enumerate(years):\n",
    "    # Loop over each model to create a plot in the corresponding column\n",
    "    for col_idx, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # --- Data processing steps (as before) ---\n",
    "        ensemble_mean = ds_model.mean(dim=[\"number\", \"latitude\", \"longitude\"])\n",
    "        stacked_ds = ensemble_mean.stack(point=(\"initialization_time\", \"forecast_hour\"))\n",
    "        tidy_ds = stacked_ds.set_index(point=[\"initialization_time\", \"valid_time\"])\n",
    "        plot_ready_full = tidy_ds[shortName].unstack(\"point\")\n",
    "\n",
    "        # --- Filter data for the specific year ---\n",
    "        # Use xr's .sel() method for selection instead of pandas' .index\n",
    "        plot_ready = plot_ready_full.sel(valid_time=plot_ready_full.valid_time.dt.year == year)\n",
    "        truth_filtered = truth.sel(valid_time=truth.valid_time.dt.year == year)\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Plot the model forecast lines\n",
    "        # Use .size to check if an xr DataArray has data\n",
    "        if not plot_ready.size:\n",
    "            print(\"skip\", year, title, \"no data\")\n",
    "            continue\n",
    "        plot_ready.plot.line(\n",
    "            x=\"valid_time\",\n",
    "            hue=\"initialization_time\",\n",
    "            ax=ax,\n",
    "            add_legend=False,\n",
    "        )\n",
    "        # Plot the truth data\n",
    "        assert truth_filtered[shortName].size > 0\n",
    "        truth_filtered[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "            ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\"\n",
    "        )\n",
    "\n",
    "        # --- Axis formatting ---\n",
    "        # Set the model title for the top row plots only\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(title, fontsize=14)\n",
    "\n",
    "        # Set the y-axis label to include the year on the leftmost plot of each row\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{year}\\n{shortName} [{units}]\", fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")  # Hide y-label for other plots in the row\n",
    "\n",
    "        ax.set_xlabel(\"Valid Time\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # --- Capture handles and labels ONCE from the first valid plot ---\n",
    "        if not handles and ax.get_lines():\n",
    "            all_lines = ax.get_lines()\n",
    "            truth_line = all_lines[-1]  # The last line plotted is the 'truth' line\n",
    "            forecast_lines = all_lines[:-1]  # The other lines are the forecasts\n",
    "\n",
    "            handles.extend(forecast_lines)\n",
    "            handles.append(truth_line)\n",
    "\n",
    "            # Create labels for forecast lines\n",
    "            forecast_labels = [\n",
    "                pd.to_datetime(t).strftime(\"%Y-%m-%d %Hz\")\n",
    "                for t in plot_ready.initialization_time.values\n",
    "            ]\n",
    "            labels.extend(forecast_labels)\n",
    "            labels.append(\"Truth\")  # Add the label for the truth line\n",
    "\n",
    "# --- Final Figure Adjustments ---\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Initialization Time\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.0, 0.5),\n",
    "    ncols=len(years),\n",
    ")\n",
    "\n",
    "# Add a main title for the entire figure\n",
    "fig.suptitle(f\"{isobaricInhPa}-hPa {shortName} Forecasts by Year\", fontsize=16)\n",
    "\n",
    "# Adjust layout to make room for the legend and suptitle\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(PLOTDIR / f\"ensmean_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the following variables are pre-defined from previous cells:\n",
    "# ds_gefs, ds_pangu, ds_fengwu, ds_mpas, truth, shortName, units, isobaricInhPa\n",
    "\n",
    "\n",
    "# --- Define models and years to iterate over ---\n",
    "models_data = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast]\n",
    "model_titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Graphcast\"]\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(years), ncols=len(models_data), figsize=(22, 10), sharex=False, sharey=True\n",
    ")\n",
    "\n",
    "# --- Legend Handling Setup ---\n",
    "# These will store the handles and labels for the final shared legend\n",
    "fig_handles = []\n",
    "fig_labels = []\n",
    "truth_handle_info = None  # To ensure the 'Truth' entry is added only once\n",
    "\n",
    "# --- Main Loop: Iterate over years (rows) and models (columns) ---\n",
    "for row_idx, year in enumerate(years):\n",
    "    for col_idx, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        ds_processed = ds_model.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "        # Filter the spatially-averaged data by the initialization year\n",
    "        ds_model_year = ds_processed.sel(\n",
    "            initialization_time=ds_processed.initialization_time.dt.year == year\n",
    "        )\n",
    "        # Filter the truth data by the valid time year\n",
    "        truth_year = truth.sel(valid_time=truth.valid_time.dt.year == year)\n",
    "\n",
    "        # Loop over each initialization time in the filtered data\n",
    "        for init_time in ds_model_year.initialization_time:\n",
    "            run_with_members = ds_model_year.sel(initialization_time=init_time)\n",
    "            first_member_data = run_with_members.isel(number=0)\n",
    "\n",
    "            line = ax.plot(\n",
    "                first_member_data.valid_time,\n",
    "                first_member_data[shortName],\n",
    "                alpha=0.5,\n",
    "                label=pd.to_datetime(init_time.values).strftime(\"%Y-%m-%d %Hz\"),\n",
    "            )\n",
    "            run_color = line[0].get_color()\n",
    "\n",
    "            for member_index in ds_model_year.number[1:]:\n",
    "                member_data = run_with_members.sel(number=member_index)\n",
    "                ax.plot(\n",
    "                    member_data.valid_time,\n",
    "                    member_data[shortName],\n",
    "                    alpha=0.5,\n",
    "                    color=run_color,\n",
    "                )\n",
    "\n",
    "        # Plot the \"truth\" data ONCE per subplot\n",
    "        truth_year[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "            ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\", label=\"Truth\"\n",
    "        )\n",
    "\n",
    "        # --- Axis Formatting ---\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(f\"{title} Forecast\")\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{year}\\n{shortName} [{units}]\")\n",
    "\n",
    "        ax.set_xlabel(\"Valid Time\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Capture handles and labels from the first column of EACH row\n",
    "        if col_idx == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "            # Find and separate the 'Truth' line information\n",
    "            if \"Truth\" in labels:\n",
    "                truth_idx = labels.index(\"Truth\")\n",
    "                # Store the truth handle only the first time we see it\n",
    "                if not truth_handle_info:\n",
    "                    truth_handle_info = (handles.pop(truth_idx), labels.pop(truth_idx))\n",
    "                else:  # Otherwise, just remove it\n",
    "                    handles.pop(truth_idx)\n",
    "                    labels.pop(truth_idx)\n",
    "\n",
    "            fig_handles.extend(handles)\n",
    "            fig_labels.extend(labels)\n",
    "\n",
    "# --- Final Legend Assembly ---\n",
    "# Add the single 'Truth' entry back at the end of the list\n",
    "if truth_handle_info:\n",
    "    fig_handles.append(truth_handle_info[0])\n",
    "    fig_labels.append(truth_handle_info[1])\n",
    "\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    fig_handles,\n",
    "    fig_labels,\n",
    "    title=\"Initialization Time\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    "    ncols=2,\n",
    ")\n",
    "\n",
    "# Adjust main title and layout to make room for the legend\n",
    "fig.suptitle(f\"Spaghetti Plot Forecasts ({isobaricInhPa}-hPa {shortName})\", fontsize=16)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(PLOTDIR / f\"spaghetti_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fengwu[shortName].mean(dim=[\"latitude\", \"longitude\", \"forecast_hour\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "HET4wJfH2khw",
    "outputId": "1cf2e7ac-477a-46f8-a0ec-6f79ba3f196e"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=6, figsize=(22, 4), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes,\n",
    "    [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast],\n",
    "    [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Graphcast\"],\n",
    "):\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # select 0.5-deg grid of GEFS or pangu fhr=0 has higher RMSE (because I didn't linearly interpolate pl fields from 0.5 to 0.25-deg grid in pangu input?)\n",
    "    ds_model = ds_model.sel(\n",
    "        latitude=ds_gefs.latitude, longitude=ds_gefs.longitude, method=\"nearest\"\n",
    "    )\n",
    "\n",
    "    # average over initialization_time, spatial dims. std over ensemble (number).\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    # error = ensemble mean - truth\n",
    "    # Line up truth and forecast along `valid_time`.\n",
    "    # if you get error about truth not having all valid_times, you probably forgot to filter based on forecast_length\n",
    "    error = ds_model.mean(dim=\"number\") - truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\", \"initialization_time\"])\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"rmse\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"spread\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"{shortName} [{units}]\")\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=6, figsize=(20, 4), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes,\n",
    "    [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast],\n",
    "    [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Graphcast\"],\n",
    "):\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    # This ensures that for each forecast point, we have a corresponding observation.\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation (Original) ---\n",
    "    # Calculate ensemble spread: std dev across members, then averaged\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "\n",
    "    # Calculate RMSE: error of ensemble mean vs. truth\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # --- CRPS Calculation (Using properscoring) ---\n",
    "    # Define the order of non-ensemble dimensions to ensure consistency.\n",
    "    core_dims = [d for d in ds_model[shortName].dims if d != \"number\"]\n",
    "\n",
    "    # Explicitly align and transpose the DataArrays to ensure their underlying\n",
    "    # numpy arrays have compatible shapes for properscoring.\n",
    "    aligned_truth_da, forecast_da = xr.align(\n",
    "        aligned_truth[shortName], ds_model[shortName], join=\"inner\", exclude=[\"number\"]\n",
    "    )\n",
    "\n",
    "    # Transpose so that the 'number' dimension is last for the forecast.\n",
    "    forecast_da = forecast_da.transpose(*core_dims, \"number\")\n",
    "    aligned_truth_da = aligned_truth_da.transpose(*core_dims)\n",
    "\n",
    "    # Extract the numpy arrays. Their shapes are now guaranteed to be compatible.\n",
    "    truth_vals = aligned_truth_da.values\n",
    "    forecast_vals = forecast_da.values\n",
    "\n",
    "    # Calculate CRPS. The 'number' dimension is now correctly the last axis.\n",
    "    crps_vals = ps.crps_ensemble(truth_vals, forecast_vals)\n",
    "\n",
    "    # Convert the resulting numpy array back to an xr.DataArray.\n",
    "    # The output of crps_ensemble has the same shape as the observations.\n",
    "    crps = xr.DataArray(crps_vals, dims=aligned_truth_da.dims, coords=aligned_truth_da.coords)\n",
    "\n",
    "    # Average the CRPS over spatial and time dimensions for a single score per lead time\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Plot the original and new metrics\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"RMSE\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"Spread\")\n",
    "    crps_mean.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"CRPS\", linestyle=\"--\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"Score [{units}]\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate members\n",
    "member_cutoff = 10  # 1, 5, 10, or None\n",
    "\n",
    "# calculate deviation from the ensemble mean (anomaly)\n",
    "ds_fengwu_anom = ds_fengwu - ds_fengwu.isel(number=slice(0, member_cutoff)).mean(dim=\"number\")\n",
    "# inflate the spread about the mean\n",
    "ds_fengwu_inflated_spread = (\n",
    "    ds_fengwu.isel(number=slice(0, member_cutoff)).mean(dim=\"number\") + 3 * ds_fengwu_anom\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(13, 13), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes.flat,\n",
    "    [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_fengwu_inflated_spread, ds_graphcast],\n",
    "    [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Fengwu spreadx3\", \"Graphcast Operational\"],\n",
    "):\n",
    "    # Select only the first 5 ensemble members for evaluation\n",
    "    ds_model = ds_model.isel(number=slice(0, member_cutoff))\n",
    "\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    # This ensures that for each forecast point, we have a corresponding observation.\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation (Original) ---\n",
    "    # Calculate ensemble spread: std dev across members, then averaged\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "\n",
    "    # Calculate RMSE: error of ensemble mean vs. truth\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # --- CRPS Calculation (Using properscoring) ---\n",
    "    # Define the order of non-ensemble dimensions to ensure consistency.\n",
    "    core_dims = [d for d in ds_model[shortName].dims if d != \"number\"]\n",
    "\n",
    "    # Explicitly align and transpose the DataArrays to ensure their underlying\n",
    "    # numpy arrays have compatible shapes for properscoring.\n",
    "    aligned_truth_da, forecast_da = xr.align(\n",
    "        aligned_truth[shortName], ds_model[shortName], join=\"inner\", exclude=[\"number\"]\n",
    "    )\n",
    "\n",
    "    # Transpose so that the 'number' dimension is last for the forecast.\n",
    "    forecast_da = forecast_da.transpose(*core_dims, \"number\")\n",
    "    aligned_truth_da = aligned_truth_da.transpose(*core_dims)\n",
    "\n",
    "    # Extract the numpy arrays. Their shapes are now guaranteed to be compatible.\n",
    "    truth_vals = aligned_truth_da.values\n",
    "    forecast_vals = forecast_da.values\n",
    "\n",
    "    # Calculate CRPS. The 'number' dimension is now correctly the last axis.\n",
    "    crps_vals = ps.crps_ensemble(truth_vals, forecast_vals)\n",
    "\n",
    "    # Convert the resulting numpy array back to an xr.DataArray.\n",
    "    # The output of crps_ensemble has the same shape as the observations.\n",
    "    crps = xr.DataArray(crps_vals, dims=aligned_truth_da.dims, coords=aligned_truth_da.coords)\n",
    "\n",
    "    # Average the CRPS over spatial and time dimensions for a single score per lead time\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Plot the original and new metrics\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"RMSE\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"Spread\")\n",
    "    crps_mean.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"CRPS\", linestyle=\"--\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(\n",
    "        f\"{title} Forecast {member_cutoff}-member cutoff\\n{isobaricInhPa}-hPa {shortName}\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"Score [{units}]\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    PLOTDIR / f\"rmse_spread_crps_fengwu_spreadx3_{isobaricInhPa}{shortName}_{years}.png\", dpi=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and their titles for the legend\n",
    "datasets = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_fengwu_inflated_spread, ds_graphcast]\n",
    "titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Fengwu spreadx3\", \"Graphcast\"]\n",
    "\n",
    "# --- Create Subplots: One for each metric ---\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 7))\n",
    "ax_rmse, ax_spread, ax_crps = axes\n",
    "\n",
    "# Variable to store the color of the 'Fengwu' plot\n",
    "fengwu_color = None\n",
    "\n",
    "# --- Loop through models, calculate metrics, and plot on the correct panel ---\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "    # Select only the first 5 ensemble members for evaluation\n",
    "    ds_model = ds_model.isel(number=slice(0, member_cutoff))\n",
    "\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation ---\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # --- CRPS Calculation ---\n",
    "    core_dims = [d for d in ds_model[shortName].dims if d != \"number\"]\n",
    "    aligned_truth_da, forecast_da = xr.align(\n",
    "        aligned_truth[shortName], ds_model[shortName], join=\"inner\", exclude=[\"number\"]\n",
    "    )\n",
    "    forecast_da = forecast_da.transpose(*core_dims, \"number\")\n",
    "    aligned_truth_da = aligned_truth_da.transpose(*core_dims)\n",
    "    crps_vals = ps.crps_ensemble(aligned_truth_da.values, forecast_da.values)\n",
    "    crps = xr.DataArray(crps_vals, dims=aligned_truth_da.dims, coords=aligned_truth_da.coords)\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Plotting Logic ---\n",
    "    plot_kwargs = {\"marker\": \"o\", \"label\": title, \"alpha\": 0.7}\n",
    "\n",
    "    # Check if the current model is the one to be dashed\n",
    "    if title == \"Fengwu spreadx3\":\n",
    "        plot_kwargs[\"linestyle\"] = \"--\"\n",
    "        # Use the stored color from the 'Fengwu' plot\n",
    "        if fengwu_color:\n",
    "            plot_kwargs[\"color\"] = fengwu_color\n",
    "\n",
    "    # Plot each metric and store the color if it's the 'Fengwu' model\n",
    "    line_rmse = rmse.plot.line(ax=ax_rmse, x=\"day\", **plot_kwargs)\n",
    "    line_spread = ensemble_spread.plot.line(ax=ax_spread, x=\"day\", **plot_kwargs)\n",
    "    line_crps = crps_mean.plot.line(ax=ax_crps, x=\"day\", **plot_kwargs)\n",
    "\n",
    "    # If we just plotted 'Fengwu', get its color for the next iteration\n",
    "    if title == \"Fengwu\":\n",
    "        # The plot function returns a list of lines; get the color from the first one\n",
    "        fengwu_color = line_rmse[0].get_color()\n",
    "\n",
    "\n",
    "# --- Formatting for all subplots ---\n",
    "for ax in axes:\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend(title=f\"{member_cutoff}-member cutoff\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Set titles and y-axis label for each panel\n",
    "ax_rmse.set_title(f\"RMSE\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "ax_spread.set_title(f\"Spread\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "ax_crps.set_title(f\"CRPS\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "\n",
    "ax_rmse.set_ylabel(f\"Score [{units}]\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    PLOTDIR\n",
    "    / f\"rmse_spread_crps_all_models_on_same_panel_{member_cutoff}-member_cutoff_{isobaricInhPa}{shortName}_{years}.png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climo_var = {\"z\": \"hgt\", \"t\": \"air\"}\n",
    "climo = xr.open_dataset(\n",
    "    f\"/glade/work/ahijevyc/share/ACC/{climo_var[shortName]}.day.ltm.1981-2010.nc\",\n",
    "    decode_times=xr.coders.CFDatetimeCoder(use_cftime=True),\n",
    ").rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "# Create the regridding tool\n",
    "regridder = xe.Regridder(\n",
    "    climo,\n",
    "    xr.Dataset(\n",
    "        coords={\n",
    "            \"latitude\": ds_pangu.coords[\"latitude\"],\n",
    "            \"longitude\": ds_pangu.coords[\"longitude\"],\n",
    "        }\n",
    "    ),\n",
    "    method=\"bilinear\",\n",
    "    periodic=True,\n",
    ")\n",
    "\n",
    "# Apply the regridding to your dataset\n",
    "ds_climo = regridder(climo)\n",
    "\n",
    "# make day of year the coordinate\n",
    "ds_climo = ds_climo.assign_coords(dayofyear=ds_climo.time.dt.dayofyear).swap_dims(time=\"dayofyear\")\n",
    "\n",
    "ds_climo = ds_climo.rename({climo_var[shortName]: shortName}).sel(level=isobaricInhPa)\n",
    "\n",
    "anom_truth = truth.unify_chunks().groupby(\"valid_time.dayofyear\") - ds_climo\n",
    "anom_truth\n",
    "\n",
    "# Create a single figure and axes object\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# Loop through each model to calculate and plot its ACC\n",
    "for ds_model, title in zip(\n",
    "    [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast],\n",
    "    [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Graphcast\"],\n",
    "):\n",
    "    ds_model = ds_model.sel(number=slice(None, member_cutoff))\n",
    "    # Calculate the anomaly\n",
    "    anom = ds_model.groupby(\"valid_time.dayofyear\") - ds_climo\n",
    "\n",
    "    # Calculate the Anomaly Correlation Coefficient (ACC)\n",
    "    acc = xr.corr(\n",
    "        anom[shortName].mean(dim=\"number\"),\n",
    "        anom_truth[shortName].sel(valid_time=anom.valid_time),\n",
    "        dim=[\"latitude\", \"longitude\", \"initialization_time\"],\n",
    "    )\n",
    "\n",
    "    # Convert forecast_hour to days for the x-axis\n",
    "    acc = acc.assign_coords(day=acc.forecast_hour / 24)\n",
    "\n",
    "    # Plot the ACC line on the single axes, using the model title as the label\n",
    "    acc.plot.line(ax=ax, x=\"day\", marker=\"o\", label=title)\n",
    "\n",
    "# --- Configure the final plot ---\n",
    "\n",
    "# Set a single title for the chart\n",
    "ax.set_title(f\"Forecast Comparison: {isobaricInhPa}-hPa {shortName}\")\n",
    "\n",
    "# Set labels for the axes\n",
    "ax.set_ylabel(f\"ACC\")\n",
    "ax.set_xlabel(\"Forecast Day\")\n",
    "\n",
    "ax.set_ylim(bottom=0.65, top=1.02)\n",
    "\n",
    "# Add a grid and a legend\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax.legend(title=f\"member cutoff: {member_cutoff}\")\n",
    "\n",
    "# Adjust layout to prevent labels from overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "ofile = f\"plots/acc.{shortName}{isobaricInhPa}.{member_cutoff}memcutoff.png\"\n",
    "fig.savefig(ofile)\n",
    "print(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Logic ---\n",
    "datasets = [ds_mpas, ds_mpas_interp, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast]\n",
    "titles = [\"MPAS\", \"MPAS_interp\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Graphcast Operational\"]\n",
    "\n",
    "# --- 1. Pre-calculate all RMSE arrays to find a global color scale ---\n",
    "all_rmse_maps = []\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "    # Align the truth dataset to the forecast's valid_time\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"], method=\"nearest\")\n",
    "\n",
    "    # Calculate error and RMSE, but keep spatial dimensions\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    # Average over initialization time to get a stable RMSE map for each forecast hour\n",
    "    mse = se.mean(dim=[\"initialization_time\"])\n",
    "    rmse = np.sqrt(mse)\n",
    "    all_rmse_maps.append(rmse)\n",
    "\n",
    "# Select the forecast hour to plot (e.g., the last one)\n",
    "fhr_to_plot = ds_mpas.forecast_hour.values[-1]\n",
    "rmse_maps_at_fhr = [rmse.sel(forecast_hour=fhr_to_plot) for rmse in all_rmse_maps]\n",
    "\n",
    "# Determine a robust color range using percentiles to avoid outliers skewing the scale\n",
    "# First, combine all data into a single array\n",
    "all_rmse_values = np.concatenate([da.values.ravel() for da in rmse_maps_at_fhr])\n",
    "all_rmse_values = all_rmse_values[~np.isnan(all_rmse_values)]\n",
    "# Then calculate the 2nd and 98th percentiles\n",
    "vmin = np.percentile(all_rmse_values, 1)\n",
    "vmax = np.percentile(all_rmse_values, 99.5)\n",
    "\n",
    "# --- 2. Create the plots ---\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=3,\n",
    "    figsize=(15, 8),\n",
    "    subplot_kw={\n",
    "        \"projection\": ccrs.LambertConformal(central_longitude=-98.0, central_latitude=39.5)\n",
    "    },\n",
    ")\n",
    "\n",
    "for i, (ax, ds_model, title) in enumerate(zip(axes.flat, datasets, titles)):\n",
    "    rmse_map = rmse_maps_at_fhr[i]\n",
    "\n",
    "    # Plot the 2D RMSE map\n",
    "    im = rmse_map.plot.pcolormesh(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=\"viridis\",\n",
    "        add_colorbar=False,  # We will add a single colorbar later\n",
    "    )\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "    ax.add_feature(cfeature.STATES, edgecolor=\"gray\", lw=0.5)\n",
    "\n",
    "    # Set title for each subplot\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    extent = [-124, -72, 25, 50]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(datasets), len(axes.flat)):\n",
    "    axes.flat[i].axis(\"off\")\n",
    "\n",
    "# Add a single, shared colorbar for the entire figure\n",
    "fig.colorbar(\n",
    "    im,\n",
    "    ax=axes,\n",
    "    orientation=\"vertical\",\n",
    "    label=f\"RMSE of {isobaricInhPa}{shortName} [{units}]\",\n",
    "    shrink=0.8,\n",
    "    pad=0.02,\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(f\"RMSE of {isobaricInhPa}{shortName} at Forecast Hour {fhr_to_plot}\", fontsize=16)\n",
    "plt.savefig(PLOTDIR / f\"spatial_rmse_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Pre-calculate all Spread arrays to find a global color scale ---\n",
    "all_spread_maps = []\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "    # Calculate ensemble spread, keeping spatial dimensions\n",
    "    # Average over initialization_time to get a stable spread map for each forecast hour\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName].std(dim=\"number\", ddof=1).mean(dim=[\"initialization_time\"])\n",
    "    )\n",
    "    all_spread_maps.append(ensemble_spread)\n",
    "\n",
    "# Select the forecast hour to plot (e.g., the last one)\n",
    "fhr_to_plot = ds_mpas.forecast_hour.values[-1]\n",
    "spread_maps_at_fhr = [spread.sel(forecast_hour=fhr_to_plot) for spread in all_spread_maps]\n",
    "\n",
    "# Determine a robust color range using percentiles to avoid outliers skewing the scale\n",
    "# First, combine all data into a single array\n",
    "all_spread_values = np.concatenate([da.values.ravel() for da in spread_maps_at_fhr])\n",
    "all_spread_values = all_spread_values[~np.isnan(all_spread_values)]\n",
    "# Then calculate the 1st and 99.5th percentiles\n",
    "vmin = np.percentile(all_spread_values, 1)\n",
    "vmax = np.percentile(all_spread_values, 99.5)\n",
    "\n",
    "\n",
    "# --- 2. Create the plots ---\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=3,\n",
    "    figsize=(15, 8),\n",
    "    subplot_kw={\n",
    "        \"projection\": ccrs.LambertConformal(central_longitude=-98.0, central_latitude=39.5)\n",
    "    },\n",
    ")\n",
    "\n",
    "for i, (ax, ds_model, title) in enumerate(zip(axes.flat, datasets, titles)):\n",
    "    spread_map = spread_maps_at_fhr[i]\n",
    "\n",
    "    # Plot the 2D Spread map\n",
    "    im = spread_map.plot.pcolormesh(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=\"viridis\",\n",
    "        add_colorbar=False,  # We will add a single colorbar later\n",
    "    )\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "    ax.add_feature(cfeature.STATES, edgecolor=\"gray\", lw=0.5)\n",
    "\n",
    "    # Set title for each subplot\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    extent = [-124, -72, 25, 50]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(datasets), len(axes.flat)):\n",
    "    axes.flat[i].axis(\"off\")\n",
    "\n",
    "# Add a single, shared colorbar for the entire figure\n",
    "# Note: colorbar is attached to the whole figure, not just the populated axes\n",
    "fig.colorbar(\n",
    "    im,\n",
    "    ax=axes.ravel().tolist(),\n",
    "    orientation=\"vertical\",\n",
    "    label=f\"Spread of {isobaricInhPa}{shortName} [{units}]\",\n",
    "    shrink=0.8,\n",
    "    pad=0.02,\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(f\"Spread of {isobaricInhPa}{shortName} at Forecast Hour {fhr_to_plot}\", fontsize=16)\n",
    "plt.savefig(PLOTDIR / f\"spatial_spread_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:daa]",
   "language": "python",
   "name": "conda-env-daa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
