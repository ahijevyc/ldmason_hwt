{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need >77 GB by the time CRPS cells are run\n",
    "\n",
    "Made chunks larger so they are not 2Mb but 20Mb by unchunking `number` (ensemble member) dimension. (100-200Mb ideal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2X-b5-NzDBVB",
    "outputId": "830cc9d8-d560-4244-a0f1-743a16e6735e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import hwt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import properscoring as ps\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from earth2studio.data import WB2Climatology\n",
    "from earth2studio.data.mpas_ens import MPASEnsemble\n",
    "from earth2studio.data.rx import SurfaceGeoPotential\n",
    "from hwt import get_dynamics_model, init_times, init_times23, init_times24\n",
    "from loguru import logger\n",
    "from metpy.constants import g\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import era5_varid, gefs_members, get_graphcast_output\n",
    "\n",
    "# Set the earth2studio logger to INFO\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "\n",
    "# Place where Dave cached datasets for quicker use\n",
    "CACHEDIR = Path(\"/glade/derecho/scratch/ahijevyc/ldmason_hwt\")\n",
    "\n",
    "graphcast_subdir = \"graphcast\"\n",
    "\n",
    "PLOTDIR = Path(\"plots\") / graphcast_subdir\n",
    "if not os.path.exists(PLOTDIR):\n",
    "    os.makedirs(PLOTDIR, exist_ok=True)\n",
    "\n",
    "# sorted list of unique years in init_times\n",
    "years = sorted(list(set(i.year for i in init_times)))\n",
    "\n",
    "forecast_length = 240\n",
    "# Truncate members\n",
    "member_cutoff = 10  # 1, 5, 10, or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel = True\n",
    "if parallel:\n",
    "    try:\n",
    "        client.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cluster.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    # Create a local cluster with workers (CPUs)\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables may be changed between temperature and height\n",
    "shortName = \"z\"\n",
    "isobaricInhPa = 500\n",
    "units = \"m\"\n",
    "\n",
    "\n",
    "def local_path_from_url(gefsdir, file_url_str):\n",
    "    url_path = Path(file_url_str)\n",
    "    file_path = gefsdir.joinpath(*url_path.parts[-5:])\n",
    "    os.makedirs(file_path.parent, exist_ok=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def open_grib_dataset(path):\n",
    "    ds = xr.open_dataset(\n",
    "        path,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"errors\": \"ignore\"},\n",
    "        filter_by_keys={\n",
    "            \"typeOfLevel\": \"isobaricInhPa\",\n",
    "            \"level\": isobaricInhPa,\n",
    "            \"shortName\": \"gh\" if shortName == \"z\" else shortName,\n",
    "        },\n",
    "        decode_timedelta=True,\n",
    "        chunks={},  # chunking can help reduce memory usage\n",
    "    )\n",
    "    if shortName == \"z\":\n",
    "        ds = ds.rename(gh=\"z\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def ai_ifiles(pangu_dir, model, init_time, mem, forecast_length):\n",
    "    # return list of files at different lead times.\n",
    "    if init_time > pd.to_datetime(\"20250101\"):\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"ens{mem}\" / f\"{model}_forecast_data\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_ens{mem}_pred_{i:03d}.nc\"\n",
    "            for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "    else:\n",
    "        control_or_perturbation = \"p\" if mem > 0 else \"c\"\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"{control_or_perturbation}{mem:02d}\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_gefs_pred_{i:03d}.nc\" for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "\n",
    "    return ifiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_files(ai_dir, init_times, forecast_length, nmem=51):\n",
    "    # derive model from ai_dir\n",
    "    model = ai_dir.name.rstrip(\"_realtime\")\n",
    "    # Create a nested list where list[init_time][member] = [files_for_all_forecast_hours]\n",
    "    nested_files = []\n",
    "    for init_time in init_times:\n",
    "        # List for all members for this init_time\n",
    "        time_specific_files = []\n",
    "        for mem in range(nmem):\n",
    "            # List of all forecast hour files for this specific member\n",
    "            lead_times = ai_ifiles(ai_dir, model, init_time, mem, forecast_length)\n",
    "            if all([os.path.exists(f) for f in lead_times]):\n",
    "                time_specific_files.append(lead_times)\n",
    "            else:\n",
    "                print(f\"not all lead times present for {model} {mem} {init_time}\")\n",
    "\n",
    "        if len(time_specific_files) != nmem:\n",
    "            print(f\"only {len(time_specific_files)}/{nmem} {model} {init_time} files\")\n",
    "            continue\n",
    "        nested_files.append(time_specific_files)\n",
    "\n",
    "    return nested_files\n",
    "\n",
    "\n",
    "def merge_nested_files(nested_files, shortName, isobaricInhPa, units):\n",
    "    # The channel label we want to select\n",
    "    channel_label = f\"{shortName}{isobaricInhPa}\"\n",
    "\n",
    "    nmem = len(nested_files[0])\n",
    "    ds = (\n",
    "        xr.open_mfdataset(\n",
    "            nested_files,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"init_time\", \"number\", \"prediction_timedelta\"],\n",
    "            decode_timedelta=True,\n",
    "            chunks=\"auto\",  # Use 'auto' for better performance with dask\n",
    "        )\n",
    "        # Rename dimensions and coordinates at the start\n",
    "        .rename(\n",
    "            {\n",
    "                \"init_time\": \"initialization_time\",\n",
    "                \"prediction_timedelta\": \"step\",\n",
    "                \"lat\": \"latitude\",\n",
    "                \"lon\": \"longitude\",\n",
    "                \"__xarray_dataarray_variable__\": shortName,  # Rename the main variable\n",
    "            }\n",
    "        )\n",
    "        # Assign the integer coordinate for the 'number' dimension\n",
    "        .assign_coords(number=range(nmem))\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        .pipe(\n",
    "            lambda ds: ds.assign_coords(\n",
    "                forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "            ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        )\n",
    "        # Calculate the valid_time coordinate\n",
    "        .assign_coords(valid_time=lambda ds: ds.initialization_time + ds.step)\n",
    "        # Select the desired channel by its label (more readable)\n",
    "        .sel(channel=channel_label)\n",
    "        # Add the pressure level as a non-dimension coordinate\n",
    "        .assign_coords(isobaricInhPa=isobaricInhPa)\n",
    "    )\n",
    "\n",
    "    if shortName == \"z\":\n",
    "        if \"units\" in ds[\"z\"].attrs:\n",
    "            # Let metpy take care of the units\n",
    "            ds = ds.metpy.quantify()\n",
    "            print(\"divide by g (as Quantity with units)\")\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        else:\n",
    "            # no units, like in fengwu\n",
    "            print(\"divide by g (no units)\")\n",
    "            ds[\"z\"] /= g.m\n",
    "            ds[\"z\"].attrs.update({\"units\": units})\n",
    "    if \"units\" in ds[shortName].attrs:\n",
    "        assert (\n",
    "            ds[shortName].attrs[\"units\"] == units\n",
    "        ), f\"expected units {units}. got {ds[shortName].attrs['units']}\"\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 2025 in years:\n",
    "    # Output from real-time runs in Ryan's directory\n",
    "    pangu_dir = Path(\"/glade/derecho/scratch/sobash/pangu_realtime\")\n",
    "    nested_files = get_nested_files(pangu_dir, init_times, forecast_length)\n",
    "    ds_pangu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "\n",
    "    fengwu_dir = Path(\"/glade/derecho/scratch/sobash/fengwu_realtime\")\n",
    "    nested_files = get_nested_files(fengwu_dir, init_times, forecast_length)\n",
    "    ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "else:\n",
    "    ofile = CACHEDIR / f\"pangu.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_pangu = xr.open_zarr(ofile, decode_timedelta=False)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        # Output from older dates (HWT2023/4)\n",
    "        # Nested list\n",
    "        #                         init0          , ... ,           initn\n",
    "        # model_runs = [[gec0, gep1, ... , gep30], ... , [gec0, gep1, ... , gep30]]\n",
    "        model_runs = []\n",
    "        for init_time in init_times:\n",
    "            idir = Path(\n",
    "                \"/glade/derecho/scratch/ahijevyc/ai-models/output/panguweather\"\n",
    "            ) / init_time.strftime(\"%Y%m%d%H\")\n",
    "            ifiles = sorted(list(idir.glob(\"g??[0-9][0-9].grib\")))\n",
    "            if len(ifiles) == 31:\n",
    "                model_runs.append(ifiles)\n",
    "            else:\n",
    "                logging.warning(f\"{init_time} has {len(ifiles)}/31 pangu files in {idir}\")\n",
    "        ds = xr.open_mfdataset(\n",
    "            model_runs,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"errors\": \"ignore\"},\n",
    "            filter_by_keys={\n",
    "                \"typeOfLevel\": \"isobaricInhPa\",\n",
    "                \"level\": isobaricInhPa,\n",
    "                \"shortName\": shortName,  # Don't worry about z being called gh. It's called z.\n",
    "            },\n",
    "            decode_timedelta=True,\n",
    "            chunks={},  # chunking can help reduce memory usage\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"time\", \"number\"],\n",
    "        )\n",
    "        ds = ds.rename(time=\"initialization_time\")\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        ds = ds.assign_coords(\n",
    "            forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "        ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        # Just multiples of 24 hours, please.\n",
    "        ds = ds.sel(forecast_hour=range(0, forecast_length + 1, 24))\n",
    "        if shortName == \"z\":\n",
    "            # Let metpy take care of units (must be Quantity first)\n",
    "            # m^2/s^2 -> m\n",
    "            ds = ds.metpy.quantify()\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        ds_pangu = ds\n",
    "        ds_pangu.to_zarr(ofile)\n",
    "    ofile = CACHEDIR / f\"fengwu.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        # fengwu runs out of mem if chunks['number'] = 1. Set to -1.\n",
    "        ds_fengwu = xr.open_zarr(ofile, decode_timedelta=False, chunks={\"number\": -1})\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        fengwu_dir = Path(\"/glade/derecho/scratch/ahijevyc/ai-models/output/fengwu\")\n",
    "        nested_files = get_nested_files(fengwu_dir, init_times, forecast_length, nmem=31)\n",
    "        ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa, units)\n",
    "        ds_fengwu.to_zarr(ofile)\n",
    "\n",
    "    # weathernext zarr has multiple variables/isobaricInhPa, as opposed to pangu and fengwu zarr files.\n",
    "    ofile = CACHEDIR / f\"weathernext.{years}.{forecast_length:03d}h.zarr\"\n",
    "    idir = Path(\"/glade/derecho/scratch/sobash/weathernext\")\n",
    "    ifiles = [\n",
    "        [\n",
    "            idir / f\"{d:%Y%m%d%H}/weathernext_{d:%Y%m%d%H}_{fhr:03d}.nc\"\n",
    "            for fhr in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "        for d in init_times\n",
    "    ]\n",
    "\n",
    "    def openwxnext(ifiles, var, isobaricInhPa):\n",
    "        ds = (\n",
    "            xr.open_mfdataset(\n",
    "                ifiles,\n",
    "                combine=\"nested\",\n",
    "                concat_dim=[\"time\", \"prediction_timedelta\"],\n",
    "                chunks={\"number\": -1},\n",
    "            )[var]\n",
    "            .sel(level=[isobaricInhPa])  # pass list to preserve isobaricInhPa coord for zarr\n",
    "            .rename(\n",
    "                level=\"isobaricInhPa\",\n",
    "                sample=\"number\",\n",
    "                lat=\"latitude\",\n",
    "                lon=\"longitude\",\n",
    "                time=\"initialization_time\",\n",
    "            )\n",
    "        )\n",
    "        ds[\"forecast_hour\"] = ds[\"prediction_timedelta\"] / pd.to_timedelta(1, unit=\"h\")\n",
    "        ds[\"valid_time\"] = ds[\"initialization_time\"] + ds[\"prediction_timedelta\"]\n",
    "        ds = ds.swap_dims(prediction_timedelta=\"forecast_hour\")\n",
    "        return ds\n",
    "\n",
    "    if not os.path.exists(ofile):\n",
    "        print(f\"making {ofile}\")\n",
    "        ds_z = openwxnext(ifiles, \"geopotential\", 500).rename(\"z\")\n",
    "        ds_t = openwxnext(ifiles, \"temperature\", 850).rename(\"t\")\n",
    "        # Tried concat, but won't handle two variables.\n",
    "        # Tried 'compat=\"override\"' or 'join=\"exact\"' since coordinates\n",
    "        # (except isobaricInhPa) are identical to save compute time.\n",
    "        # But AlignmentError: cannot align join='exact' where index not equal: 'isobaricInhPa' ('isobaricInhPa',)\n",
    "        ds_wxnext = xr.combine_by_coords([ds_z, ds_t], compat=\"override\").sortby(\n",
    "            \"latitude\", ascending=False\n",
    "        )\n",
    "        ds_wxnext.to_zarr(ofile)\n",
    "    # Thought by saving with chunks this wouldn't be necessary, but it is.\n",
    "    # Unchunked forecast_hour too since chunks were so tiny (2.12MiB). Now they are 21.2MiB)\n",
    "    print(f\"open existing {ofile}\")\n",
    "    ds_wxnext = xr.open_zarr(ofile, chunks={\"number\": -1, \"forecast_hour\": -1})\n",
    "    ds_wxnext = ds_wxnext[[shortName]].sel(isobaricInhPa=isobaricInhPa)\n",
    "    # convert geopotential (m2/s2) to geopotential height (m)\n",
    "    if shortName == \"z\":\n",
    "        ds_wxnext[\"z\"] = ds_wxnext[\"z\"] / g.m\n",
    "\n",
    "ds_wxnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofile = CACHEDIR / f\"{graphcast_subdir}.{years}.zarr\"\n",
    "if not ofile.exists():\n",
    "    ds_graphcast = get_graphcast_output(shortName, isobaricInhPa, subdir=graphcast_subdir)\n",
    "    print(f\"making {ofile}\")\n",
    "    ds_graphcast.to_zarr(ofile)\n",
    "ds_graphcast = xr.open_zarr(ofile, chunks={\"number\": -1})\n",
    "ds_graphcast = ds_graphcast.sel(forecast_hour=slice(None, forecast_length))\n",
    "ds_graphcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ofile = CACHEDIR / \"mpas15-3km.zarr\"\n",
    "# native horizontal mpas mesh\n",
    "if not ofile.exists():\n",
    "    datadir = Path(\"/glade/campaign/mmm/parc/schwartz\")\n",
    "    # 2023\n",
    "    mpas_datasrc = MPASEnsemble(\n",
    "        grid_path=datadir / \"MPAS/15-3km_mesh/grid_mesh/x5.6488066.grid_CONUS.nc\",\n",
    "        data_path=f\"{datadir}/HWT2023/mpas\",\n",
    "    )\n",
    "    for initialization_time in tqdm(init_times23):\n",
    "        objs = []\n",
    "        for forecast_hour in range(0, int(5.5 * 24) + 1, 24):\n",
    "            ds = mpas_datasrc(initialization_time, variables=[\"t850\", \"z500\"], fhr=forecast_hour)\n",
    "            objs.append(\n",
    "                ds.expand_dims(\n",
    "                    initialization_time=[initialization_time], forecast_hour=[forecast_hour]\n",
    "                ).squeeze(\"Time\")\n",
    "            )\n",
    "        ds = xr.combine_by_coords(objs, combine_attrs=\"override\")\n",
    "        if os.path.exists(ofile):\n",
    "            # consolidated=False because it's not official part of version 3 and local storage means it won't help much\n",
    "            ds.to_zarr(ofile, mode=\"a\", append_dim=\"initialization_time\", consolidated=False)\n",
    "        else:\n",
    "            ds.to_zarr(ofile, mode=\"w\", consolidated=False)\n",
    "    # 2024\n",
    "    mpas_datasrc = MPASEnsemble(\n",
    "        grid_path=datadir / \"MPAS/15-3km_mesh/grid_mesh/x5.6488066.grid_CONUS.nc\",\n",
    "        data_path=f\"{datadir}/HWT2024/mpas\",\n",
    "    )\n",
    "    for initialization_time in tqdm(init_times24):\n",
    "        objs = []\n",
    "        for forecast_hour in range(0, int(5.5 * 24) + 1, 24):\n",
    "            ds = mpas_datasrc(initialization_time, variables=[\"t850\", \"z500\"], fhr=forecast_hour)\n",
    "            objs.append(\n",
    "                ds.expand_dims(\n",
    "                    initialization_time=[initialization_time], forecast_hour=[forecast_hour]\n",
    "                ).squeeze(\"Time\")\n",
    "            )\n",
    "        ds = xr.combine_by_coords(objs, combine_attrs=\"override\")\n",
    "        ds.to_zarr(ofile, mode=\"a\", append_dim=\"initialization_time\", consolidated=False)\n",
    "\n",
    "ds_mpas = (\n",
    "    xr.open_zarr(ofile, consolidated=False)[\"__xarray_dataarray_variable__\"]\n",
    "    .to_dataset(dim=\"variable\")\n",
    "    .rename(\n",
    "        lat=\"latitude\",\n",
    "        lon=\"longitude\",\n",
    "        member=\"number\",\n",
    "        t850=\"t\",\n",
    "        z500=\"z\",\n",
    "    )\n",
    "    .assign_coords(\n",
    "        valid_time=lambda x: x[\"initialization_time\"] + x[\"forecast_hour\"] * pd.Timedelta(\"1h\")\n",
    "    )\n",
    "    .isel(latitude=slice(None, None, -1))\n",
    ")\n",
    "ds_mpas[\"z\"] = ds_mpas[\"z\"] / g.m\n",
    "ds_mpas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarrfile = CACHEDIR / \"fv3.zarr\"\n",
    "if not zarrfile.exists():\n",
    "    ds_fv3: xr.Dataset = get_dynamics_model(\n",
    "        \"fv3\",\n",
    "        forecast_hours=range(24, 204 + 1, 24),\n",
    "        ens_size=10,\n",
    "        vars_dict={\"TMP850\": \"t\", \"HGT500\": \"z\"},\n",
    "        init_times=init_times,\n",
    "        output_grid=ds_pangu,\n",
    "        CACHEDIR=CACHEDIR,\n",
    "    )\n",
    "ds_fv3 = xr.open_zarr(\n",
    "    zarrfile, chunks={\"number\": -1, \"forecast_hour\": -1, \"initialization_time\": 10}\n",
    ")\n",
    "ds_fv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # interpolated to lat/lon \"interp_mpas_3km_*.nc\" files\n",
    "    zarrfile = CACHEDIR / \"mpas_interp.zarr\"\n",
    "    if not zarrfile.exists():\n",
    "        # get_dynamics_model also saves f\"{model}.zarr\"\n",
    "        # Reads interpolated files f\"interp_{model}_3km_{init_time:%Y%m%d%H}_mem{mem}_f{fhr:03d}.nc\"\n",
    "        # Not native hybrid levels.\n",
    "        ds_mpas_interp: xr.Dataset = get_dynamics_model(\n",
    "            model=\"mpas\",\n",
    "            forecast_hours=range(0, 132 + 1, 24),  # MPAS only goes 5.5d\n",
    "            ens_size=5,  # only 5/10 members interpolated to lat/lon/pressure_level and saved\n",
    "            vars_dict={\"temperature_850hPa\": \"t\", \"height_500hPa\": \"z\"},\n",
    "            init_times=init_times,\n",
    "            output_grid=ds_pangu,\n",
    "            CACHEDIR=CACHEDIR,\n",
    "        )\n",
    "    ds_mpas_interp = xr.open_zarr(zarrfile)\n",
    "    ds_mpas_interp\n",
    "    \n",
    "    sel = dict(\n",
    "        initialization_time=\"20240501\",\n",
    "        forecast_hour=120,\n",
    "        number=2,\n",
    "        longitude=slice(230, 290),\n",
    "        latitude=slice(52, 21),\n",
    "    )\n",
    "    fig, axes = plt.subplots(ncols=3, figsize=(18, 4))\n",
    "    vmin = 210\n",
    "    ds_mpas.sel(sel).t.plot(ax=axes[0], vmin=vmin)\n",
    "    ds_mpas_interp.sel(sel)[shortName].plot(ax=axes[1], vmin=vmin)\n",
    "    qm = (ds_mpas - ds_mpas_interp).sel(sel)[shortName].plot(ax=axes[2], vmin=-1, vmax=1)\n",
    "    qm.figure.suptitle(\"native mpas mesh vs lat-lon interpolated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gefsdir = Path(\"/glade/derecho/scratch/ahijevyc/tmp/gefs\")\n",
    "\n",
    "ofile = CACHEDIR / f\"gefs.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "\n",
    "# Try saved ds_gefs. I tried saving zarr with compute=False but when I loaded, values were zero.\n",
    "if os.path.exists(ofile):\n",
    "    print(f\"open existing {ofile}\")\n",
    "    ds_gefs = xr.open_zarr(ofile, decode_timedelta=False, chunks={\"number\": -1})\n",
    "else:\n",
    "    existing_files = set(p.resolve() for p in gefsdir.glob(\"*/??/atmos/pgrb2ap5/*pgrb2a.0p50.f???\"))\n",
    "\n",
    "    # Base URL for the public NOAA GEFS S3 bucket\n",
    "    base_url = \"https://noaa-gefs-pds.s3.amazonaws.com\"\n",
    "\n",
    "    # ===================================================================\n",
    "    # PHASE 1: DEFINE FILE REQUIREMENTS\n",
    "    # ===================================================================\n",
    "    print(\"--- Phase 1: Defining all required file paths ---\")\n",
    "    required_files = []\n",
    "\n",
    "    for init_time in init_times:\n",
    "        for member in gefs_members:\n",
    "            for fhr in range(0, forecast_length + 1, 24):\n",
    "                fhr_str = f\"{fhr:03d}\"\n",
    "                s3_filename = f\"{member}.t{init_time:%H}z.pgrb2a.0p50.f{fhr_str}\"\n",
    "                file_path_on_s3 = (\n",
    "                    f\"gefs.{init_time:%Y%m%d}/{init_time:%H}/atmos/pgrb2ap5/{s3_filename}\"\n",
    "                )\n",
    "                url = f\"{base_url}/{file_path_on_s3}\"\n",
    "                local_path = local_path_from_url(gefsdir, url)\n",
    "                required_files.append({\"url\": url, \"local_path\": local_path})\n",
    "\n",
    "    print(f\"âœ… Defined {len(required_files)} files required for analysis.\\n\")\n",
    "    print(f\"âœ… Data saved in: {gefsdir}\")\n",
    "    print(\"-\" * 50)\n",
    "    datasets = []\n",
    "\n",
    "    print(f\"making {ofile}\")\n",
    "    for required_file in tqdm(required_files):\n",
    "        url = required_file[\"url\"]\n",
    "        local_file_path = required_file[\"local_path\"]\n",
    "        if local_file_path in existing_files:\n",
    "            # print(f\"   -> ðŸŸ¢ File already exists. Skipping.\")\n",
    "            pass\n",
    "        else:\n",
    "            print(local_file_path)\n",
    "            # Ensure the destination directory exists before downloading\n",
    "            Path(local_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   -> â¬‡ï¸  Attempting to download: {url}\")\n",
    "            download_file(url, local_file_path)\n",
    "        ds_gefs = open_grib_dataset(local_file_path)\n",
    "        datasets.append(ds_gefs)\n",
    "\n",
    "    ds_gefs = xr.combine_nested(datasets, concat_dim=[\"time\"])\n",
    "\n",
    "    itime_coords = ds_gefs[\"valid_time\"] - ds_gefs[\"step\"]\n",
    "    ds_gefs = ds_gefs.assign_coords(initialization_time=itime_coords)\n",
    "    ds_gefs = ds_gefs.groupby([\"initialization_time\", \"step\", \"number\"]).first()\n",
    "\n",
    "    step_as_hours = ds_gefs[\"step\"].data / pd.to_timedelta(\"1h\")\n",
    "    ds_gefs = ds_gefs.assign_coords(forecast_hour=(\"step\", step_as_hours))\n",
    "    ds_gefs = ds_gefs.swap_dims(step=\"forecast_hour\")\n",
    "    ds_gefs = ds_gefs.assign_coords(valid_time=ds_gefs.initialization_time + ds_gefs.step)\n",
    "    ds_gefs.to_zarr(ofile)\n",
    "\n",
    "ds_gefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = Path(\"/glade/campaign/collections/rda/data/d633000/e5.oper.an.pl\")\n",
    "\n",
    "# tack on extra truth times to compare to the forecast valid times\n",
    "truth_times = init_times\n",
    "for init_time in init_times:\n",
    "    valid_times = pd.date_range(init_time, init_time + pd.to_timedelta(f\"{forecast_length}h\"))\n",
    "    truth_times = truth_times.union(valid_times)\n",
    "\n",
    "era5_files = [\n",
    "    era5_dir\n",
    "    / i.strftime(\"%Y%m\")\n",
    "    / f\"e5.oper.an.pl.128_{era5_varid[shortName]}_{shortName}.ll025sc.{i:%Y%m%d00}_{i:%Y%m%d23}.nc\"\n",
    "    for i in truth_times\n",
    "]\n",
    "truth = (\n",
    "    xr.open_mfdataset(era5_files)  # tried chunks time=10 but shuffle-sorter took forever\n",
    "    .sel(level=isobaricInhPa)\n",
    "    .rename(time=\"valid_time\", level=\"isobaricInhPa\")\n",
    "    .rename({shortName.upper(): shortName})  # T->t, Z->z. uppercase shortName to lower\n",
    "    .drop_vars(\"utc_date\")\n",
    ")\n",
    "# era5 available every hour. just return 0z please\n",
    "truth = truth.sel(valid_time=(truth.valid_time.dt.hour == 0))\n",
    "\n",
    "# convert geopotential to geopotential height\n",
    "if shortName == \"z\":\n",
    "    truth[\"z\"] = truth[\"z\"] / g.m\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mpas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply geographic_sel to ds_gefs and mask to HWT region.\n",
    "geographic_sel = dict(latitude=slice(60, 20), longitude=slice(220, 300))\n",
    "# Use GEFS coords to select from all other Datasets.\n",
    "\n",
    "print(\"gefs\")\n",
    "ds_gefs = ds_gefs.sel(geographic_sel).where(hwt.mask0p5).load()\n",
    "gefs0p5 = dict(latitude=ds_gefs.latitude, longitude=ds_gefs.longitude)\n",
    "\n",
    "# Select ds_gefs coords from other datasets.\n",
    "print(\"mpas\")\n",
    "ds_mpas = ds_mpas.sel(gefs0p5).where(hwt.mask0p5).load()\n",
    "print(\"fv3\")\n",
    "ds_fv3 = ds_fv3.sel(gefs0p5).where(hwt.mask0p5).load()\n",
    "print(\"pangu\")\n",
    "ds_pangu = ds_pangu.sel(gefs0p5).where(hwt.mask0p5).load()\n",
    "print(\"fengwu\")\n",
    "ds_fengwu = ds_fengwu.sel(gefs0p5).where(hwt.mask0p5).load()\n",
    "print(\"graphcast\")\n",
    "ds_graphcast = ds_graphcast.sel(gefs0p5).where(hwt.mask0p5).load()\n",
    "print(\"wxnext\")  # subset of \"HWT\" domain, 60-20N 220-300E.\n",
    "# Reindex to larger domain so you can use same mask. method=None to pad with NaNs\n",
    "ds_wxnext = ds_wxnext.reindex(gefs0p5).where(hwt.mask0p5)#.load()\n",
    "print(\"truth\")\n",
    "truth = truth.sel(gefs0p5).where(hwt.mask0p5).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assumes the following variables are already loaded in your script:\n",
    "# ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengw, ds_graphcast, : xr.Dataset objects with model data\n",
    "# truth: xr.Dataset with the ground truth data\n",
    "# shortName: A string representing the variable name (e.g., 't' for temperature)\n",
    "# units: A string for the y-axis label (e.g., 'K' for Kelvin)\n",
    "# isobaricInhPa: An integer representing the pressure level (e.g., 500)\n",
    "\n",
    "\n",
    "# These will store the handles and labels for the shared legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "# --- Define models and years to iterate over ---\n",
    "models_data = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext]\n",
    "model_titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"]\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# Create a nxm grid of subplots: n rows (for years), m columns (for models)\n",
    "fig, axes = plt.subplots(nrows=len(years), ncols=len(models_data), figsize=(20, 7.5), sharey='row')\n",
    "\n",
    "\n",
    "# --- Main Loop: Iterate over years and models ---\n",
    "# Loop over each year to create a row of plots\n",
    "for row_idx, year in enumerate(years):\n",
    "    # Loop over each model to create a plot in the corresponding column\n",
    "    for col_idx, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # --- Data processing steps (as before) ---\n",
    "        ensemble_mean = ds_model.mean(dim=[\"number\", \"latitude\", \"longitude\"])\n",
    "        nmembers = ds_model.sizes[\"number\"]\n",
    "        stacked_ds = ensemble_mean.stack(point=(\"initialization_time\", \"forecast_hour\"))\n",
    "        tidy_ds = stacked_ds.set_index(point=[\"initialization_time\", \"valid_time\"])\n",
    "        plot_ready_full = tidy_ds[shortName].unstack(\"point\")\n",
    "\n",
    "        # --- Filter data for the specific year ---\n",
    "        # Use xr's .sel() method for selection instead of pandas' .index\n",
    "        plot_ready = plot_ready_full.sel(valid_time=plot_ready_full.valid_time.dt.year == year)\n",
    "        truth_filtered = truth.sel(valid_time=truth.valid_time.dt.year == year)\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Plot the model forecast lines\n",
    "        # Use .size to check if an xr DataArray has data\n",
    "        if not plot_ready.size:\n",
    "            print(\"skip\", year, title, \"no data\")\n",
    "            continue\n",
    "        plot_ready.plot.line(\n",
    "            x=\"valid_time\",\n",
    "            hue=\"initialization_time\",\n",
    "            ax=ax,\n",
    "            add_legend=False,\n",
    "        )\n",
    "        # Plot the truth data\n",
    "        assert truth_filtered[shortName].size > 0\n",
    "        truth_filtered[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "            ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\", markersize=3\n",
    "        )\n",
    "\n",
    "        # --- Axis formatting ---\n",
    "        # Set the model title for the top row plots only\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(f\"{title} ({nmembers} mem)\", fontsize=14)\n",
    "\n",
    "        # Set the y-axis label to include the year on the leftmost plot of each row\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{year}\\n{shortName} [{units}]\", fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")  # Hide y-label for other plots in the row\n",
    "\n",
    "        ax.set_xlabel(\"Valid Time\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # --- Capture handles and labels ONCE from the first valid plot ---\n",
    "        if not handles and ax.get_lines():\n",
    "            all_lines = ax.get_lines()\n",
    "            truth_line = all_lines[-1]  # The last line plotted is the 'truth' line\n",
    "            forecast_lines = all_lines[:-1]  # The other lines are the forecasts\n",
    "\n",
    "            handles.extend(forecast_lines)\n",
    "            handles.append(truth_line)\n",
    "\n",
    "            # Create labels for forecast lines\n",
    "            forecast_labels = [\n",
    "                pd.to_datetime(t).strftime(\"%Y-%m-%d\")\n",
    "                for t in plot_ready.initialization_time.values\n",
    "            ]\n",
    "            labels.extend(forecast_labels)\n",
    "            labels.append(\"Truth\")  # Add the label for the truth line\n",
    "\n",
    "# --- Final Figure Adjustments ---\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Initialization\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.0, 0.5),\n",
    "    ncols=len(years),\n",
    ")\n",
    "\n",
    "# Add a main title for the entire figure\n",
    "fig.suptitle(f\"{isobaricInhPa}-hPa {shortName} Forecasts by Year\", fontsize=16)\n",
    "\n",
    "# Adjust layout to make room for the legend and suptitle\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(PLOTDIR / f\"ensmean_{isobaricInhPa}{shortName}_{years}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "models_data = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext]\n",
    "model_titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(years), ncols=len(models_data), figsize=(20, 7.5), sharex=False, sharey=\"row\"\n",
    ")\n",
    "\n",
    "fig_handles = []\n",
    "fig_labels = []\n",
    "# Create color cycle to pull from--one color for each init_time\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "# --- Main Loop ---\n",
    "for row_idx, year in enumerate(years):\n",
    "    for col_idx, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # 1. Filter and reduce spatially - keep it as a Dask graph for now\n",
    "        ds_year = ds_model.sel(initialization_time=ds_model.initialization_time.dt.year == year)\n",
    "\n",
    "        # 2. COMPUTE: This is the heavy lift.\n",
    "        # Returns a small in-memory xarray DataArray of (init_time, number, forecast_hour)\n",
    "        plot_data = ds_year[shortName].mean(dim=[\"latitude\", \"longitude\"]).compute()\n",
    "\n",
    "        # 3. Plot each initialization group\n",
    "        for i, init_time in enumerate(plot_data.initialization_time):\n",
    "            group = plot_data.sel(initialization_time=init_time)\n",
    "\n",
    "            group_color = colors[i % len(colors)]\n",
    "\n",
    "            # Using xarray's vectorized line plot for all members in this init_time\n",
    "            # Removed add_labels=False to avoid the AttributeError\n",
    "            lines = group.plot.line(\n",
    "                ax=ax,\n",
    "                x=\"valid_time\",\n",
    "                hue=\"number\",\n",
    "                add_legend=False,\n",
    "                alpha=0.5,\n",
    "                linewidth=0.8,\n",
    "                color=group_color,\n",
    "            )\n",
    "\n",
    "            # 4. Legend Capture (Only from first subplot)\n",
    "            # 'lines' is a list of Line2D objects. We just need one to represent the group.\n",
    "            if col_idx == 0:\n",
    "                label_str = pd.to_datetime(init_time.values).strftime(\"%Y-%m-%d\")\n",
    "                if label_str not in fig_labels:\n",
    "                    # Set a specific color for the whole group to keep legend clean\n",
    "                    group_color = lines[0].get_color()\n",
    "                    fig_handles.append(lines[0])\n",
    "                    fig_labels.append(label_str)\n",
    "\n",
    "        # 5. Plot Truth\n",
    "        truth_year = truth.sel(valid_time=truth.valid_time.dt.year == year)\n",
    "        truth_mean = truth_year[shortName].mean(dim=[\"latitude\", \"longitude\"]).compute()\n",
    "\n",
    "        # Plotting truth and capturing the handle for the legend\n",
    "        t_line = ax.plot(\n",
    "            truth_mean.valid_time,\n",
    "            truth_mean.values,\n",
    "            color=\"black\",\n",
    "            marker=\"o\",\n",
    "            markersize=3,\n",
    "            linewidth=2,\n",
    "            label=\"Truth\",\n",
    "        )\n",
    "\n",
    "        if col_idx == 0 and \"Truth\" not in fig_labels:\n",
    "            fig_handles.append(t_line[0])\n",
    "            fig_labels.append(\"Truth\")\n",
    "\n",
    "        # --- Formatting ---\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(f\"{title} ({ds_model.sizes['number']} mem)\")\n",
    "\n",
    "        # Clean up x-axis labels to avoid overlaps\n",
    "        ax.set_xlabel(\"\")\n",
    "        if row_idx == len(years) - 1:\n",
    "            ax.set_title(\"\")\n",
    "            ax.set_xlabel(\"Valid Time\")\n",
    "\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{year}\\n{shortName} [{units}]\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# --- Final Adjustments ---\n",
    "fig.legend(\n",
    "    fig_handles,\n",
    "    fig_labels,\n",
    "    title=\"Initialization\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    "    ncols=2,\n",
    ")\n",
    "\n",
    "fig.suptitle(f\"{isobaricInhPa}-hPa {shortName} Forecasts by Year\", fontsize=16)\n",
    "plt.tight_layout()  # Make room for the legend\n",
    "\n",
    "plt.savefig(PLOTDIR / f\"spaghetti_{isobaricInhPa}{shortName}_{years}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fengwu[shortName].mean(dim=[\"latitude\", \"longitude\", \"forecast_hour\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "HET4wJfH2khw",
    "outputId": "1cf2e7ac-477a-46f8-a0ec-6f79ba3f196e"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=7, nrows=2, figsize=(18, 6), sharex=True, sharey=\"row\")\n",
    "max_y_top = 0\n",
    "for col_idx, ((ax, ax_bot), ds_model, title) in enumerate(\n",
    "    zip(\n",
    "        axes.T,\n",
    "        [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext],\n",
    "        [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"],\n",
    "    )\n",
    "):\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # select 0.5-deg grid of GEFS or pangu fhr=0 has higher RMSE\n",
    "    # (because I didn't linearly interpolate pl fields from 0.5 to 0.25-deg grid in pangu input?)\n",
    "    ds_model = ds_model.sel(\n",
    "        latitude=ds_gefs.latitude, longitude=ds_gefs.longitude, method=\"nearest\"\n",
    "    )\n",
    "    nmembers = ds_model.sizes[\"number\"]\n",
    "    # average over initialization_time, spatial dims. std over ensemble (number).\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    # error = ensemble mean - truth\n",
    "    # Line up truth and forecast along `valid_time`.\n",
    "    # if you get error about truth not having all valid_times,\n",
    "    # you probably forgot to filter based on forecast_length\n",
    "    error = ds_model.mean(dim=\"number\") - truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\", \"initialization_time\"])\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"rmse\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"spread\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} ({nmembers}) {isobaricInhPa}-hPa {shortName}\", fontsize=14)\n",
    "\n",
    "    current_max = max(rmse.max(), ensemble_spread.max())\n",
    "    max_y_top = max(max_y_top, current_max)\n",
    "\n",
    "    ratio = ensemble_spread / rmse\n",
    "    ratio.plot.line(ax=ax_bot, x=\"day\", color=\"black\", marker=\"s\", label=\"ratio\")\n",
    "    # Reference line for perfect spread\n",
    "    ax_bot.axhline(1, color=\"red\", linestyle=\"-\", alpha=0.3)\n",
    "    ax_bot.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax_bot.set_ylim(0, 1.5)  # Ratios usually hover around 1.0\n",
    "\n",
    "for ax in axes[0, :]:\n",
    "    ax.set_ylim(0, max_y_top * 1.05)  # Added 5% padding for headspace\n",
    "plt.tight_layout()\n",
    "ofile = PLOTDIR / f\"rmse_spread_{isobaricInhPa}{shortName}_{years}.png\"\n",
    "plt.savefig(ofile, dpi=200)\n",
    "print(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "HET4wJfH2khw",
    "outputId": "1cf2e7ac-477a-46f8-a0ec-6f79ba3f196e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the global aesthetic\n",
    "sns.set_theme(style=\"ticks\")\n",
    "fig, axes = plt.subplots(ncols=7, figsize=(18, 3), sharex=True, sharey=True)\n",
    "forecast_hour_to_plot = 132\n",
    "\n",
    "models = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext]\n",
    "titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"]\n",
    "\n",
    "for ax, ds_model, title in zip(axes, models, titles):\n",
    "    # --- 1. Processing & Calculation ---\n",
    "    ds_sub = ds_model.sel(\n",
    "        latitude=ds_gefs.latitude,\n",
    "        longitude=ds_gefs.longitude,\n",
    "        method=\"nearest\",\n",
    "        forecast_hour=forecast_hour_to_plot,\n",
    "    )\n",
    "\n",
    "    spread = ds_sub[shortName].std(dim=\"number\", ddof=1).mean(dim=[\"latitude\", \"longitude\"])\n",
    "    ens_mean = ds_sub[shortName].mean(dim=\"number\")\n",
    "    obs = truth[shortName].sel(valid_time=ds_sub[\"valid_time\"])\n",
    "    rmse = np.sqrt(((ens_mean - obs) ** 2).mean(dim=[\"latitude\", \"longitude\"]))\n",
    "\n",
    "    # --- 2. Tidying & Rolling Mean ---\n",
    "    def tidy_with_rolling(da, label, window=7):\n",
    "        df = da.to_dataframe(name=\"value\").reset_index()\n",
    "        df[\"year\"] = df[\"initialization_time\"].dt.year.astype(str)\n",
    "        df[\"day\"] = df[\"initialization_time\"].dt.dayofyear\n",
    "        df[\"metric\"] = label\n",
    "\n",
    "        df = df.sort_values(\"initialization_time\")\n",
    "\n",
    "        # Calculate rolling mean per year to avoid bleeding 2023 into 2024\n",
    "        df[\"rolling_val\"] = df.groupby(\"year\")[\"value\"].transform(\n",
    "            lambda x: x.rolling(window=window, center=True).mean()\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    df_plot = pd.concat(\n",
    "        [tidy_with_rolling(rmse, \"RMSE\"), tidy_with_rolling(spread, \"Spread\")]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # --- 3. Plotting ---\n",
    "    # Plot raw data as faint lines (alpha=0.3)\n",
    "    sns.lineplot(\n",
    "        data=df_plot,\n",
    "        x=\"day\",\n",
    "        y=\"value\",\n",
    "        hue=\"year\",\n",
    "        style=\"metric\",\n",
    "        hue_order=[\"2023\", \"2024\"],\n",
    "        alpha=0.25,\n",
    "        lw=1,\n",
    "        legend=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Plot rolling mean as thick, solid lines\n",
    "    sns.lineplot(\n",
    "        data=df_plot,\n",
    "        x=\"day\",\n",
    "        y=\"rolling_val\",\n",
    "        hue=\"year\",\n",
    "        style=\"metric\",\n",
    "        hue_order=[\"2023\", \"2024\"],\n",
    "        lw=2.5,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # --- 4. Aesthetics ---\n",
    "    ax.set_title(f\"{title} {forecast_hour_to_plot}h {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.grid(True, axis=\"y\", linestyle=\":\", alpha=0.6)\n",
    "    ax.set_xlabel(\"Day of Year\" if ax in axes[2:4] else \"\")\n",
    "    ax.set_ylabel(f\"{shortName} ({units})\" if ax == axes[0] else \"\")\n",
    "\n",
    "    if ax != axes[-1]:\n",
    "        ax.get_legend().remove()\n",
    "    else:\n",
    "        # Customizing legend labels for clarity\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        # Filter legend to only show the main categories\n",
    "        ax.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTDIR / f\"rmse_spread_timeseries_{isobaricInhPa}{shortName}_{years}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=7, figsize=(18, 3), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes,\n",
    "    [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext],\n",
    "    [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"],\n",
    "):\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "    print(title)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    # This ensures that for each forecast point, we have a corresponding observation.\n",
    "    aligned_truth = truth[shortName].sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread ---\n",
    "    # Calculate ensemble spread: std dev across members, then averaged\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "\n",
    "    # Calculate RMSE: error of ensemble mean vs. truth\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # Calculate CRPS. The 'number' dimension is now correctly the last axis.\n",
    "    # via apply_ufunc (Avoids .values)\n",
    "    # This keeps the calculation 'lazy' and memory-efficient\n",
    "    crps = xr.apply_ufunc(\n",
    "        ps.crps_ensemble,  # The function to apply\n",
    "        aligned_truth,  # Argument 1 (observations)\n",
    "        ds_model[shortName],  # Argument 2 (forecasts)\n",
    "        input_core_dims=[[], [\"number\"]],  # 'number' is the ensemble dim in forecast\n",
    "        join=\"inner\",  # handle coord mismatches\n",
    "        dask=\"parallelized\",  # Allows Dask to handle it in chunks\n",
    "        output_dtypes=[float],\n",
    "        dask_gufunc_kwargs={\"allow_rechunk\": True},\n",
    "    )\n",
    "\n",
    "    # 5. Aggregate CRPS and Trigger Computation\n",
    "    # We call .compute() or .load() only on the small, reduced result\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"]).compute()\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Plot the original and new metrics\n",
    "    logging.info(\"plot\")\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"RMSE\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"Spread\")\n",
    "    crps_mean.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"CRPS\", linestyle=\"--\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"Score [{units}]\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_data = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext]\n",
    "model_titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"]\n",
    "# We set sharex=False and sharey=False because the scales differ wildly between\n",
    "# a 10-member and 64-member ensemble.\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "    ax = axes[i]\n",
    "    print(f\"Processing {title} (Members: {ds_model.number.size})...\")\n",
    "\n",
    "    # 1. Dynamic Member Count\n",
    "    n_members = ds_model.number.size\n",
    "    # The ideal probability for a perfectly calibrated ensemble\n",
    "    ideal_freq = 1 / (n_members + 1)\n",
    "\n",
    "    # ds_gefs has 0.5deg grid; select only those points from ds_model and truth.\n",
    "    ds_model = ds_model.sel(\n",
    "        latitude=ds_gefs.latitude, longitude=ds_gefs.longitude, method=\"nearest\"\n",
    "    )\n",
    "\n",
    "    aligned_truth = truth[shortName].sel(\n",
    "        valid_time=ds_model[\"valid_time\"],\n",
    "        latitude=ds_gefs.latitude,\n",
    "        longitude=ds_gefs.longitude,\n",
    "        method=\"nearest\",\n",
    "    )\n",
    "\n",
    "    # 1. Ensure we are working with raw numpy arrays to avoid xarray/dask overhead\n",
    "    # We flatten the spatial/temporal dims but keep the 'number' (ensemble) dim\n",
    "    obs = aligned_truth.values.flatten()  # shape: (N,)\n",
    "    fct = ds_model[shortName].values.reshape(-1, n_members)  # shape: (N, members)\n",
    "\n",
    "    # 2. Filter out NaNs manually (critical!)\n",
    "    valid_mask = ~np.isnan(obs) & ~np.isnan(fct).any(axis=1)\n",
    "    obs_clean = obs[valid_mask]\n",
    "    fct_clean = fct[valid_mask]\n",
    "\n",
    "    # 3. Calculate ranks: how many ensemble members are LESS than the observation?\n",
    "    # (This is the core of a rank histogram)\n",
    "    ranks = (fct_clean < obs_clean[:, np.newaxis]).sum(axis=1)\n",
    "\n",
    "    # 4. Create the histogram bins (0 to n_members)\n",
    "    counts_np, _ = np.histogram(ranks, bins=np.arange(n_members + 2))\n",
    "\n",
    "    # 5. Convert back to xarray just for your plotting code to work\n",
    "    counts = xr.DataArray(counts_np, coords={\"rank\": np.arange(n_members + 1)}, dims=\"rank\")\n",
    "    # 4. Normalize to Relative Frequency\n",
    "    # This makes the y-axis a probability (0.0 to 1.0) rather than a raw count (e.g., 500,000)\n",
    "    freq = counts / counts.sum()\n",
    "\n",
    "    # Calculate Reliability Score (Root Mean Square Error from Ideal)\n",
    "    # A perfectly flat histogram will have a score of 0.\n",
    "    rel_score = np.sqrt(((freq - ideal_freq) ** 2).mean())\n",
    "    dynamic_lw = 0.2 if n_members > 30 else 0.7\n",
    "    # Use ['rank'] to get the coordinate, and .values to pass a numpy array to matplotlib\n",
    "    ax.bar(\n",
    "        freq[\"rank\"].values,\n",
    "        freq.values,\n",
    "        color=\"skyblue\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=dynamic_lw,\n",
    "        width=1.0,\n",
    "        align=\"center\",\n",
    "    )\n",
    "    # Add the dynamic \"Perfect\" line\n",
    "    ax.axhline(y=ideal_freq, color=\"red\", linestyle=\"--\", label=\"Ideal\")\n",
    "\n",
    "    # Aesthetics\n",
    "    ax.set_title(f\"{title} (N={n_members})\")\n",
    "    # Add Reliability Index text box\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.8,\n",
    "        f\"Reliability Index\\n(lower better): {rel_score.values:.3f}\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=\"small\",\n",
    "        va=\"top\",\n",
    "        ha=\"center\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Rank\")\n",
    "    ax.set_ylabel(\"Relative Frequency\")\n",
    "\n",
    "    # Important: Center the ticks if you have fewer members (like N=10)\n",
    "    ax.set_xticks(range(n_members + 1))\n",
    "    # Dynamic Y-Limit to focus on the deviations\n",
    "    # We set the top to 2.5x the ideal frequency so U-shapes are visible but consistent\n",
    "    ax.set_ylim(0, ideal_freq * 2.5)\n",
    "\n",
    "    # Only show every 5th or 10th tick on x-axis if there are too many members (e.g. 64)\n",
    "    if n_members > 30:\n",
    "        ax.set_xticks(range(0, n_members + 2, 10))\n",
    "\n",
    "    ax.legend(loc=\"upper center\")\n",
    "\n",
    "# Remove empty subplot\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "ofile = PLOTDIR / f\"rank_histogram_{isobaricInhPa}{shortName}_{years}.png\"\n",
    "plt.savefig(ofile, dpi=200)\n",
    "print(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_model, title in zip(\n",
    "    [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext],\n",
    "    [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"],\n",
    "):\n",
    "    print(title, ds_model.nbytes / 1024 / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # calculate deviation from the ensemble mean (anomaly)\n",
    "    ds_fengwu_anom = ds_fengwu - ds_fengwu.isel(number=slice(0, member_cutoff)).mean(dim=\"number\")\n",
    "    # inflate the spread about the mean\n",
    "    ds_fengwu_inflated_spread = (\n",
    "        ds_fengwu.isel(number=slice(0, member_cutoff)).mean(dim=\"number\") + 3 * ds_fengwu_anom\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, nrows=2, figsize=(14, 8), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes.flat,\n",
    "    [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext],\n",
    "    [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"],\n",
    "):\n",
    "    print(title)\n",
    "    # Select only the first 5 ensemble members for evaluation\n",
    "    ds_model = ds_model[shortName].isel(number=slice(0, member_cutoff))\n",
    "\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    # This ensures that for each forecast point, we have a corresponding observation.\n",
    "    aligned_truth = truth[shortName].sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation (Original) ---\n",
    "    # Calculate ensemble spread: std dev across members, then averaged\n",
    "    ensemble_spread = ds_model.std(dim=\"number\", ddof=1).mean(\n",
    "        dim=[\"initialization_time\", \"latitude\", \"longitude\"]\n",
    "    )\n",
    "\n",
    "    # Calculate RMSE: error of ensemble mean vs. truth\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error**2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    crps = xr.apply_ufunc(\n",
    "        ps.crps_ensemble,  # The function to apply\n",
    "        aligned_truth,  # Argument 1 (observations)\n",
    "        ds_model,  # Argument 2 (forecasts)\n",
    "        input_core_dims=[[], [\"number\"]],  # 'number' is the ensemble dim in forecast\n",
    "        join=\"inner\",  # handle coord mismatches\n",
    "        dask=\"parallelized\",  # Allows Dask to handle it in chunks\n",
    "        output_dtypes=[float],\n",
    "        dask_gufunc_kwargs={\"allow_rechunk\": True},\n",
    "    )\n",
    "\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"]).compute()\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Plot the original and new metrics\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"RMSE\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"Spread\")\n",
    "    crps_mean.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"CRPS\", linestyle=\"--\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(\n",
    "        f\"{title} {member_cutoff}-mem cutoff\\n{isobaricInhPa}-hPa {shortName}\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"Score [{units}]\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "ofile = (\n",
    "    PLOTDIR\n",
    "    / f\"rmse_spread_crps_{member_cutoff}-member_cutoff_{isobaricInhPa}{shortName}_{years}.png\"\n",
    ")\n",
    "plt.savefig(ofile, dpi=200)\n",
    "print(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the models and their titles for the legend\n",
    "datasets = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext]\n",
    "titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"]\n",
    "\n",
    "# --- Create Subplots: One for each metric ---\n",
    "fig, axes = plt.subplots(ncols=4, figsize=(16, 5))\n",
    "ax_rmse, ax_spread, ax_ratio, ax_crps = axes\n",
    "\n",
    "# Variable to store the color of the 'Fengwu' plot\n",
    "fengwu_color = None\n",
    "\n",
    "z = SurfaceGeoPotential(cache=False)([0])\n",
    "sfc_hgt_cutoff = 2000\n",
    "\n",
    "# --- Loop through models, calculate metrics, and plot on the correct panel ---\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "    print(title)\n",
    "    # Select only the first 5 ensemble members for evaluation\n",
    "    ds_model = ds_model.isel(number=slice(0, member_cutoff))\n",
    "\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "    # Mask surface >= 2000m\n",
    "    ds_model = ds_model.where(\n",
    "        z.sel(lat=ds_model.latitude, lon=ds_model.longitude).squeeze() < 2000 * g.m\n",
    "    )\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    aligned_truth = truth[shortName].sel(valid_time=ds_model[\"valid_time\"])\n",
    "    # --- RMSE and Spread Calculation ---\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "    ratio = ensemble_spread/rmse\n",
    "\n",
    "    crps = xr.apply_ufunc(\n",
    "        ps.crps_ensemble,  # The function to apply\n",
    "        aligned_truth,  # Argument 1 (observations)\n",
    "        ds_model[shortName],  # Argument 2 (forecasts)\n",
    "        input_core_dims=[[], [\"number\"]],  # 'number' is the ensemble dim in forecast\n",
    "        join=\"inner\",  # handle coord mismatches\n",
    "        dask=\"parallelized\",  # Allows Dask to handle it in chunks\n",
    "        output_dtypes=[float],\n",
    "        dask_gufunc_kwargs={\"allow_rechunk\": True},\n",
    "    )\n",
    "\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"]).compute()\n",
    "\n",
    "    # --- Plotting Logic ---\n",
    "    plot_kwargs = {\"marker\": \"o\", \"label\": title, \"alpha\": 0.7}\n",
    "\n",
    "    # Check if the current model is the one to be dashed\n",
    "    if title == \"Fengwu spreadx3\":\n",
    "        plot_kwargs[\"linestyle\"] = \"--\"\n",
    "        # Use the stored color from the 'Fengwu' plot\n",
    "        if fengwu_color:\n",
    "            plot_kwargs[\"color\"] = fengwu_color\n",
    "\n",
    "    # Plot each metric and store the color if it's the 'Fengwu' model\n",
    "    line_rmse = rmse.plot.line(ax=ax_rmse, x=\"day\", **plot_kwargs)\n",
    "    line_spread = ensemble_spread.plot.line(ax=ax_spread, x=\"day\", **plot_kwargs)\n",
    "    line_ratio = ratio.plot.line(ax=ax_ratio, x=\"day\", **{**plot_kwargs, \"marker\": \"s\"})\n",
    "    line_crps = crps_mean.plot.line(ax=ax_crps, x=\"day\", **plot_kwargs)\n",
    "\n",
    "    # If we just plotted 'Fengwu', get its color for the next iteration\n",
    "    if title == \"Fengwu\":\n",
    "        # The plot function returns a list of lines; get the color from the first one\n",
    "        fengwu_color = line_rmse[0].get_color()\n",
    "\n",
    "\n",
    "\n",
    "# --- Formatting for all subplots ---\n",
    "for ax in axes:\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend(title=f\"{member_cutoff}-mem cutoff\", fontsize=\"small\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Set titles and y-axis label for each panel\n",
    "ax_rmse.set_title(\"RMSE\")\n",
    "ax_spread.set_title(\"Spread\")\n",
    "ax_ratio.set_title(\"Spread/Error Ratio\")\n",
    "# Reference line for perfect spread\n",
    "ax_ratio.axhline(1, color=\"red\", linestyle=\"-\", alpha=0.3)\n",
    "ax_ratio.set_ylim(0.1, 2)  # Ratios usually hover around 1.0\n",
    "ax_crps.set_title(\"CRPS\")\n",
    "\n",
    "ax_rmse.set_ylabel(f\"Score [{units}]\")\n",
    "\n",
    "plt.suptitle(f\"{isobaricInhPa}-hPa {shortName}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    PLOTDIR\n",
    "    / f\"rmse_spread_crps_all_models_on_same_panel_lt{sfc_hgt_cutoff}m_{member_cutoff}-mem_cutoff_{isobaricInhPa}{shortName}_{years}.png\",\n",
    "    dpi=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize climatology source - defaults to 1990-2020 mean\n",
    "climo_src = WB2Climatology(verbose=False)\n",
    "\n",
    "# 2. Calculate Truth Anomaly\n",
    "# fetch() returns data aligned to the provided times\n",
    "wb2_var = f\"{shortName}{isobaricInhPa}\"\n",
    "ds_climo = await climo_src.fetch(pd.to_datetime(truth.valid_time), wb2_var)\n",
    "ds_climo = ds_climo.squeeze(\"variable\")\n",
    "ds_climo = ds_climo.rename({\"time\": \"valid_time\", \"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "if shortName == \"z\":\n",
    "    # it's geopotential; divide by gravity to get height\n",
    "    ds_climo = ds_climo / g.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_truth = truth[shortName] - ds_climo\n",
    "\n",
    "# Create a single figure and axes object\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# Loop through each model to calculate and plot its ACC\n",
    "for ds_model, title in zip(\n",
    "    [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext],\n",
    "    [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"],\n",
    "):\n",
    "    print(title)\n",
    "    ds_model = ds_model[shortName].sel(number=slice(None, member_cutoff)).mean(dim=\"number\")\n",
    "\n",
    "    # Calculate the anomaly\n",
    "    # Crucial to select valid_time from ds_climo, or \n",
    "    # initialization_time and valid_time will not match,\n",
    "    # and automatic broadcasting across both will blow up memory.\n",
    "    anom = ds_model - ds_climo.sel(valid_time=ds_model.valid_time)\n",
    "\n",
    "    # Calculate the Anomaly Correlation Coefficient (ACC)\n",
    "    acc = xr.corr(\n",
    "        anom,\n",
    "        anom_truth.sel(valid_time=anom.valid_time),\n",
    "        dim=[\"latitude\", \"longitude\", \"initialization_time\"],\n",
    "    )\n",
    "    # Convert forecast_hour to days for the x-axis\n",
    "    acc = acc.assign_coords(day=acc.forecast_hour / 24)\n",
    "\n",
    "    # Plot the ACC line on the single axes, using the model title as the label\n",
    "    line_acc = acc.plot.line(ax=ax, x=\"day\", marker=\"o\", label=title, alpha=0.7)\n",
    "\n",
    "ax.set_title(f\"Forecast Comparison: {isobaricInhPa}-hPa {shortName}\")\n",
    "\n",
    "# Set labels for the axes\n",
    "ax.set_ylabel(\"ACC\")\n",
    "ax.set_xlabel(\"Forecast Day\")\n",
    "\n",
    "ax.set_ylim(bottom=0.65, top=1.02)\n",
    "\n",
    "# Add a grid and a legend\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax.legend(title=f\"member cutoff: {member_cutoff}\")\n",
    "\n",
    "# Adjust layout to prevent labels from overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "ofile = PLOTDIR / f\"acc.{shortName}{isobaricInhPa}.{member_cutoff}memcutoff.png\"\n",
    "fig.savefig(ofile)\n",
    "print(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between WeatherBench2 climo (1990-2019) and my climo (1959-1988)\n",
    "my_climo = (\n",
    "    xr.open_dataset(\"/glade/work/ahijevyc/share/ACC/hgt.4Xday.1959-1988.ltm.nc\")[\"hgt\"]\n",
    "    .rename(lat=\"latitude\", lon=\"longitude\", time=\"valid_time\")\n",
    "    .sel(level=isobaricInhPa, **geographic_sel)\n",
    ")\n",
    "\n",
    "\n",
    "def add_day_hour(da):\n",
    "    da[\"day_hour\"] = da.valid_time.dt.dayofyear * 100 + da.valid_time.dt.hour\n",
    "    return da\n",
    "\n",
    "\n",
    "ds_climo = add_day_hour(ds_climo)\n",
    "my_climo = add_day_hour(my_climo)\n",
    "my_climo = my_climo.swap_dims(valid_time=\"day_hour\").drop_vars(\"valid_time\")\n",
    "\n",
    "ds_climo = ds_climo.sel(latitude=my_climo.latitude, longitude=my_climo.longitude)\n",
    "my_climo = my_climo.sel(day_hour=ds_climo.day_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax_args = {\"projection\": ccrs.PlateCarree()}\n",
    "\n",
    "# Create the 3-panel layout (2 rows, 2 columns)\n",
    "ax1 = plt.subplot(2, 2, 1, **ax_args)  # Top Left\n",
    "ax2 = plt.subplot(2, 2, 2, **ax_args)  # Top Right\n",
    "ax3 = plt.subplot(2, 2, (3, 4), **ax_args)  # Bottom (spanning both columns)\n",
    "\n",
    "# 2. Define plotting parameters\n",
    "plot_data = [\n",
    "    (ds_climo.mean(\"valid_time\"), ax1, \"WeatherBench2 (1990-2020)\", \"viridis\"),\n",
    "    (my_climo.mean(\"valid_time\"), ax2, \"My Climo (1980-2010)\", \"viridis\"),\n",
    "    (diff.mean(\"valid_time\"), ax3, \"Difference (WB2 - My)\", \"RdBu_r\"),\n",
    "]\n",
    "\n",
    "# 3. Iterate and plot\n",
    "for data, ax, title, cmap in plot_data:\n",
    "    p = data.plot(ax=ax, transform=ccrs.PlateCarree(), cmap=cmap, add_colorbar=True)\n",
    "    ax.coastlines()\n",
    "    ax.set_title(title, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Logic ---\n",
    "datasets = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext]\n",
    "titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"]\n",
    "\n",
    "# --- 1. Pre-calculate all RMSE arrays to find a global color scale ---\n",
    "all_rmse_maps = []\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "    # Align the truth dataset to the forecast's valid_time\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"], method=\"nearest\")\n",
    "\n",
    "    # Calculate error and RMSE, but keep spatial dimensions\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    # Average over initialization time to get a stable RMSE map for each forecast hour\n",
    "    mse = se.mean(dim=[\"initialization_time\"])\n",
    "    rmse = np.sqrt(mse)\n",
    "    all_rmse_maps.append(rmse)\n",
    "\n",
    "# Select the forecast hour to plot (e.g., the last one)\n",
    "fhr_to_plot = ds_mpas.forecast_hour.values[-1]\n",
    "rmse_maps_at_fhr = [rmse.sel(forecast_hour=fhr_to_plot) for rmse in all_rmse_maps]\n",
    "\n",
    "# Determine a robust color range using percentiles to avoid outliers skewing the scale\n",
    "# First, combine all data into a single array\n",
    "all_rmse_values = np.concatenate([da.values.ravel() for da in rmse_maps_at_fhr])\n",
    "all_rmse_values = all_rmse_values[~np.isnan(all_rmse_values)]\n",
    "# Then calculate the 2nd and 98th percentiles\n",
    "vmin = np.percentile(all_rmse_values, 1)\n",
    "vmax = np.percentile(all_rmse_values, 99.5)\n",
    "\n",
    "# --- 2. Create the plots ---\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=3,\n",
    "    figsize=(15, 8),\n",
    "    subplot_kw={\n",
    "        \"projection\": ccrs.LambertConformal(central_longitude=-98.0, central_latitude=39.5)\n",
    "    },\n",
    ")\n",
    "\n",
    "for i, (ax, ds_model, title) in enumerate(zip(axes.flat, datasets, titles)):\n",
    "    rmse_map = rmse_maps_at_fhr[i]\n",
    "\n",
    "    # Plot the 2D RMSE map\n",
    "    im = rmse_map.plot.pcolormesh(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=\"viridis\",\n",
    "        add_colorbar=False,  # We will add a single colorbar later\n",
    "    )\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "    ax.add_feature(cfeature.STATES, edgecolor=\"gray\", lw=0.5)\n",
    "\n",
    "    # Set title for each subplot\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    extent = [-124, -72, 25, 50]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(datasets), len(axes.flat)):\n",
    "    axes.flat[i].axis(\"off\")\n",
    "\n",
    "# Add a single, shared colorbar for the entire figure\n",
    "fig.colorbar(\n",
    "    im,\n",
    "    ax=axes,\n",
    "    orientation=\"vertical\",\n",
    "    label=f\"RMSE of {isobaricInhPa}{shortName} [{units}]\",\n",
    "    shrink=0.8,\n",
    "    pad=0.02,\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(f\"RMSE of {isobaricInhPa}{shortName} at Forecast Hour {fhr_to_plot}\", fontsize=16)\n",
    "plt.savefig(PLOTDIR / f\"spatial_rmse_{isobaricInhPa}{shortName}_{years}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Pre-calculate all Spread arrays to find a global color scale ---\n",
    "all_spread_maps = []\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "    # Calculate ensemble spread, keeping spatial dimensions\n",
    "    # Average over initialization_time to get a stable spread map for each forecast hour\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName].std(dim=\"number\", ddof=1).mean(dim=[\"initialization_time\"])\n",
    "    )\n",
    "    all_spread_maps.append(ensemble_spread)\n",
    "\n",
    "# Select the forecast hour to plot (e.g., the last one)\n",
    "fhr_to_plot = ds_mpas.forecast_hour.values[-1]\n",
    "spread_maps_at_fhr = [spread.sel(forecast_hour=fhr_to_plot) for spread in all_spread_maps]\n",
    "\n",
    "# Determine a robust color range using percentiles to avoid outliers skewing the scale\n",
    "# First, combine all data into a single array\n",
    "all_spread_values = np.concatenate([da.values.ravel() for da in spread_maps_at_fhr])\n",
    "all_spread_values = all_spread_values[~np.isnan(all_spread_values)]\n",
    "# Then calculate the 1st and 99.5th percentiles\n",
    "vmin = np.percentile(all_spread_values, 1)\n",
    "vmax = np.percentile(all_spread_values, 99.5)\n",
    "\n",
    "\n",
    "# --- 2. Create the plots ---\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=3,\n",
    "    figsize=(15, 8),\n",
    "    subplot_kw={\n",
    "        \"projection\": ccrs.LambertConformal(central_longitude=-98.0, central_latitude=39.5)\n",
    "    },\n",
    ")\n",
    "\n",
    "for i, (ax, ds_model, title) in enumerate(zip(axes.flat, datasets, titles)):\n",
    "    spread_map = spread_maps_at_fhr[i]\n",
    "\n",
    "    # Plot the 2D Spread map\n",
    "    im = spread_map.plot.pcolormesh(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=\"viridis\",\n",
    "        add_colorbar=False,  # We will add a single colorbar later\n",
    "    )\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "    ax.add_feature(cfeature.STATES, edgecolor=\"gray\", lw=0.5)\n",
    "\n",
    "    # Set title for each subplot\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    extent = [-124, -72, 25, 50]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(datasets), len(axes.flat)):\n",
    "    axes.flat[i].axis(\"off\")\n",
    "\n",
    "# Add a single, shared colorbar for the entire figure\n",
    "# Note: colorbar is attached to the whole figure, not just the populated axes\n",
    "fig.colorbar(\n",
    "    im,\n",
    "    ax=axes.ravel().tolist(),\n",
    "    orientation=\"vertical\",\n",
    "    label=f\"Spread of {isobaricInhPa}{shortName} [{units}]\",\n",
    "    shrink=0.8,\n",
    "    pad=0.02,\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(f\"Spread of {isobaricInhPa}{shortName} at Forecast Hour {fhr_to_plot}\", fontsize=16)\n",
    "plt.savefig(PLOTDIR / f\"spatial_spread_{isobaricInhPa}{shortName}_{years}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "target_init_time = ds_mpas.initialization_time.values[0] \n",
    "fhr_to_plot = 120 \n",
    "levels = [5400, 5640, 5880] \n",
    "\n",
    "datasets = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_graphcast, ds_wxnext]\n",
    "titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu\", \"Fengwu\", \"GraphCast\", \"WxNext\"]\n",
    "\n",
    "# --- Create the plots ---\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=3,\n",
    "    figsize=(16, 10), # Adjusted height for tighter look\n",
    "    subplot_kw={\n",
    "        \"projection\": ccrs.LambertConformal(central_longitude=-98.0, central_latitude=39.5)\n",
    "    },\n",
    ")\n",
    "\n",
    "for i, (ax, ds_model, title) in enumerate(zip(axes.flat, datasets, titles)):\n",
    "    # 1. Select Slices\n",
    "    ds_slice = ds_model.sel(initialization_time=target_init_time, forecast_hour=fhr_to_plot, method=\"nearest\")\n",
    "    ens_mean = ds_slice[shortName].mean(dim=\"number\")\n",
    "    target_valid_time = ds_slice.valid_time.values\n",
    "    truth_slice = truth.sel(valid_time=target_valid_time, method=\"nearest\")[shortName]\n",
    "\n",
    "    # 2. Calculate RMSE\n",
    "    error_sq = (ens_mean - truth_slice)**2\n",
    "    spatial_rmse = np.sqrt(error_sq.mean().values)\n",
    "\n",
    "    # 3. Plot Ensemble Members (Gray)\n",
    "    for mem in ds_slice.number.values:\n",
    "        ax.contour(\n",
    "            ds_slice.longitude, ds_slice.latitude, ds_slice.sel(number=mem)[shortName],\n",
    "            levels=levels, colors='gray', linewidths=0.3, alpha=0.8, transform=ccrs.PlateCarree()\n",
    "        )\n",
    "\n",
    "    # 4. Plot Ensemble Mean (Black) & Add Labels\n",
    "    mean_contour = ax.contour(\n",
    "        ds_slice.longitude, ds_slice.latitude, ens_mean,\n",
    "        levels=levels, colors='black', linewidths=2.0, transform=ccrs.PlateCarree()\n",
    "    )\n",
    "    # Inline labels for the mean only\n",
    "    ax.clabel(mean_contour, inline=True, fontsize=9, fmt='%d', colors='black')\n",
    "    \n",
    "    # 5. Plot Truth (Red)\n",
    "    ax.contour(\n",
    "        truth_slice.longitude, truth_slice.latitude, truth_slice,\n",
    "        levels=levels, colors='red', linestyles='--', linewidths=1.5, transform=ccrs.PlateCarree()\n",
    "    )\n",
    "\n",
    "    # --- Map Aesthetics ---\n",
    "    ax.coastlines(resolution='110m', linewidth=0.8)\n",
    "    ax.add_feature(cfeature.STATES, edgecolor=\"gray\", lw=0.3)\n",
    "    ax.set_extent([-122, -74, 25, 48], crs=ccrs.PlateCarree()) # Slightly tighter extent\n",
    "    \n",
    "    ax.set_title(f\"{title} (RMSE: {spatial_rmse:.1f})\", fontsize=11, pad=5)\n",
    "\n",
    "# Cleanup\n",
    "for i in range(len(datasets), len(axes.flat)):\n",
    "    axes.flat[i].axis(\"off\")\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='gray', lw=1, label='Members'),\n",
    "    Line2D([0], [0], color='black', lw=2, label='Ens Mean'),\n",
    "    Line2D([0], [0], color='red', lw=1.5, ls='--', label='Truth')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=3, frameon=False, bbox_to_anchor=(0.5, 0.07))\n",
    "\n",
    "# --- Tighten Layout ---\n",
    "# wspace: width between subplots, hspace: height between subplots\n",
    "plt.subplots_adjust(left=0.05, right=0.95, bottom=0.12, top=0.90, wspace=0.02, hspace=0.15)\n",
    "\n",
    "plt.suptitle(f\"{isobaricInhPa} {shortName} Spaghetti Plot | Init: {target_init_time} | Fhr {fhr_to_plot}\", fontsize=14, y=0.96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "o = zarr.open(\"gefs.850t.[2023, 2024].240h.zarr\")\n",
    "o[\"t\"].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zarr.open(\"gefs.500z.[2023, 2024].240h.zarr\")\n",
    "z[\"z\"].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:daa]",
   "language": "python",
   "name": "conda-env-daa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
