{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2X-b5-NzDBVB",
    "outputId": "830cc9d8-d560-4244-a0f1-743a16e6735e"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Sequence\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import hwt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import properscoring as ps\n",
    "import requests\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from metpy.constants import g\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Place where Dave cached datasets for quicker use\n",
    "CACHEDIR = Path(\"/glade/derecho/scratch/ahijevyc/ldmason_hwt\")\n",
    "\n",
    "init_times23 = pd.date_range(hwt.firstRun(2023), pd.to_datetime(f\"{2023}0531\"), freq=\"24h\")\n",
    "init_times24 = pd.date_range(hwt.firstRun(2024), pd.to_datetime(f\"{2024}0531\"), freq=\"24h\")\n",
    "\n",
    "# Combine 2023 and 2024\n",
    "init_times = init_times23.union(init_times24)\n",
    "# sorted list of unique years in init_times\n",
    "years = sorted(list(set(i.year for i in init_times)))\n",
    "\n",
    "forecast_length = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel = False\n",
    "if parallel:\n",
    "    try:\n",
    "        client.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cluster.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    # Create a local cluster with workers (CPUs)\n",
    "    cluster = LocalCluster(threads_per_worker=1)\n",
    "    client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables may be changed between temperature and height\n",
    "shortName = \"z\"\n",
    "isobaricInhPa = 500\n",
    "units = \"m\"\n",
    "mpas_rename = {\"height_500hPa\": \"z\", \"temperature_850hPa\": \"t\"}\n",
    "\n",
    "# shouldn't need to change unless additional variables are analyzed\n",
    "era5_varid = {\"z\": 129, \"t\": 130}\n",
    "\n",
    "def build_file_url(init_time, mem, forecast_hour):\n",
    "    return (\n",
    "        f\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gens/prod/gefs.{init_time:%Y%m%d}/\"\n",
    "        f\"{init_time:%H}/atmos/pgrb2ap5/\"\n",
    "        f\"{mem}.t{init_time:%H}z.pgrb2a.0p50.f{forecast_hour:03d}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def local_path_from_url(gefsdir, file_url_str):\n",
    "    url_path = Path(file_url_str)\n",
    "    file_path = gefsdir.joinpath(*url_path.parts[-5:])\n",
    "    os.makedirs(file_path.parent, exist_ok=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def open_grib_dataset(path):\n",
    "    ds = xr.open_dataset(\n",
    "        path,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"errors\": \"ignore\"},\n",
    "        filter_by_keys={\n",
    "            \"typeOfLevel\": \"isobaricInhPa\",\n",
    "            \"level\": isobaricInhPa,\n",
    "            \"shortName\": \"gh\" if shortName == \"z\" else shortName,\n",
    "        },\n",
    "        decode_timedelta=True,\n",
    "        chunks={},  # chunking can help reduce memory usage\n",
    "    )\n",
    "    if shortName == \"z\":\n",
    "        ds = ds.rename(gh=\"z\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def download_file(url, local_file_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully downloaded: {local_file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        print(f\"Could not download {url}\")\n",
    "\n",
    "\n",
    "def ai_ifiles(pangu_dir, model, init_time, mem, forecast_length):\n",
    "    # return list of files at different lead times.\n",
    "    if init_time > pd.to_datetime(\"20250101\"):\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"ens{mem}\" / f\"{model}_forecast_data\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_ens{mem}_pred_{i:03d}.nc\"\n",
    "            for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "    else:\n",
    "        control_or_perturbation = \"p\" if mem > 0 else \"c\"\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"{control_or_perturbation}{mem:02d}\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_gefs_pred_{i:03d}.nc\" for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "\n",
    "    return ifiles\n",
    "\n",
    "\n",
    "def add_ensemble_number(ds):\n",
    "    \"\"\"\n",
    "    A preprocessing function to be used with xr.open_mfdataset.\n",
    "    It extracts the ensemble member number from the source filename\n",
    "    and adds it as a 'number' coordinate.\n",
    "    \"\"\"\n",
    "    # Get the basename of the source file (e.g., \"pangu_ens0_pred_162.nc\")\n",
    "    try:\n",
    "        filename = os.path.basename(ds.encoding[\"source\"])\n",
    "    except (KeyError, TypeError):\n",
    "        # Fallback if source encoding is not available\n",
    "        return ds\n",
    "\n",
    "    # Use a regular expression to find the number following 'ens'\n",
    "    match = re.search(r\"ens(\\d+)\", filename)\n",
    "\n",
    "    if match:\n",
    "        # Extract the number, convert to integer\n",
    "        ensemble_number = int(match.group(1))\n",
    "\n",
    "        # Add 'number' as a new dimension and assign the extracted number as its coordinate\n",
    "        return ds.expand_dims(number=[ensemble_number])\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_files(ai_dir, init_times, forecast_length, nmem=51):\n",
    "    # derive model from ai_dir\n",
    "    model = ai_dir.name.rstrip(\"_realtime\")\n",
    "    # Create a nested list where list[init_time][member] = [files_for_all_forecast_hours]\n",
    "    nested_files = []\n",
    "    for init_time in init_times:\n",
    "        # List for all members for this init_time\n",
    "        time_specific_files = []\n",
    "        for mem in range(nmem):\n",
    "            # List of all forecast hour files for this specific member\n",
    "            lead_times = ai_ifiles(ai_dir, model, init_time, mem, forecast_length)\n",
    "            if all([os.path.exists(f) for f in lead_times]):\n",
    "                time_specific_files.append(lead_times)\n",
    "            else:\n",
    "                print(f\"not all lead times present for {model} {mem} {init_time}\")\n",
    "\n",
    "        if len(time_specific_files) != nmem:\n",
    "            print(f\"only {len(time_specific_files)}/{nmem} {model} {init_time} files\")\n",
    "            continue\n",
    "        nested_files.append(time_specific_files)\n",
    "\n",
    "    return nested_files\n",
    "\n",
    "\n",
    "def merge_nested_files(nested_files, shortName, isobaricInhPa, units):\n",
    "    # The channel label we want to select\n",
    "    channel_label = f\"{shortName}{isobaricInhPa}\"\n",
    "\n",
    "    nmem = len(nested_files[0])\n",
    "    ds = (\n",
    "        xr.open_mfdataset(\n",
    "            nested_files,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"init_time\", \"number\", \"prediction_timedelta\"],\n",
    "            chunks=\"auto\",  # Use 'auto' for better performance with dask\n",
    "        )\n",
    "        # Rename dimensions and coordinates at the start\n",
    "        .rename(\n",
    "            {\n",
    "                \"init_time\": \"initialization_time\",\n",
    "                \"prediction_timedelta\": \"step\",\n",
    "                \"lat\": \"latitude\",\n",
    "                \"lon\": \"longitude\",\n",
    "                \"__xarray_dataarray_variable__\": shortName,  # Rename the main variable\n",
    "            }\n",
    "        )\n",
    "        # Assign the integer coordinate for the 'number' dimension\n",
    "        .assign_coords(number=range(nmem))\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        .pipe(\n",
    "            lambda ds: ds.assign_coords(\n",
    "                forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "            ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        )\n",
    "        # Calculate the valid_time coordinate\n",
    "        .assign_coords(valid_time=lambda ds: ds.initialization_time + ds.step)\n",
    "        # Select the desired channel by its label (more readable)\n",
    "        .sel(channel=channel_label)\n",
    "        # Add the pressure level as a non-dimension coordinate\n",
    "        .assign_coords(isobaricInhPa=isobaricInhPa)\n",
    "    )\n",
    "\n",
    "    if shortName == \"z\":\n",
    "        if \"units\" in ds[\"z\"].attrs:\n",
    "            # Let metpy take care of the units\n",
    "            ds = ds.metpy.quantify()\n",
    "            print(\"divide by g (as Quantity with units)\")\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        else:\n",
    "            # no units, like in fengwu\n",
    "            print(\"divide by g (no units)\")\n",
    "            ds[\"z\"] /= g.m\n",
    "            ds[\"z\"].attrs.update({\"units\": units})\n",
    "    if \"units\" in ds[shortName].attrs:\n",
    "        assert (\n",
    "            ds[shortName].attrs[\"units\"] == units\n",
    "        ), f\"expected units {units}. got {ds[shortName].attrs['units']}\"\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 2025 in years:\n",
    "    # Output from real-time runs in Ryan's directory\n",
    "    pangu_dir = Path(\"/glade/derecho/scratch/sobash/pangu_realtime\")\n",
    "    nested_files = get_nested_files(pangu_dir, init_times, forecast_length)\n",
    "    ds_pangu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "\n",
    "    fengwu_dir = Path(\"/glade/derecho/scratch/sobash/fengwu_realtime\")\n",
    "    nested_files = get_nested_files(fengwu_dir, init_times, forecast_length)\n",
    "    ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "else:\n",
    "    ofile = (\n",
    "        CACHEDIR / f\"pangu.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "    )\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_pangu = xr.open_zarr(ofile)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        # Output from older dates (HWT2023/4)\n",
    "        # Nested list\n",
    "        #                         init0          , ... ,           initn\n",
    "        # model_runs = [[gec0, gep1, ... , gep30], ... , [gec0, gep1, ... , gep30]]\n",
    "        model_runs = []\n",
    "        for init_time in init_times:\n",
    "            idir = Path(\n",
    "                \"/glade/derecho/scratch/ahijevyc/ai-models/output/panguweather\"\n",
    "            ) / init_time.strftime(\"%Y%m%d%H\")\n",
    "            ifiles = sorted(list(idir.glob(\"g??[0-9][0-9].grib\")))\n",
    "            if len(ifiles) == 31:\n",
    "                model_runs.append(ifiles)\n",
    "            else:\n",
    "                logging.warning(f\"{init_time} has {len(ifiles)}/31 pangu files in {idir}\")\n",
    "        ds = xr.open_mfdataset(\n",
    "            model_runs,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"errors\": \"ignore\"},\n",
    "            filter_by_keys={\n",
    "                \"typeOfLevel\": \"isobaricInhPa\",\n",
    "                \"level\": isobaricInhPa,\n",
    "                \"shortName\": shortName,  # Don't worry about z being called gh. It's called z.\n",
    "            },\n",
    "            decode_timedelta=True,\n",
    "            chunks={},  # chunking can help reduce memory usage\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"time\", \"number\"],\n",
    "        )\n",
    "        ds = ds.rename(time=\"initialization_time\")\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        ds = ds.assign_coords(\n",
    "            forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "        ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        # Just multiples of 24 hours, please.\n",
    "        ds = ds.sel(forecast_hour=range(0, forecast_length + 1, 24))\n",
    "        if shortName == \"z\":\n",
    "            # Let metpy take care of units (must be Quantity first)\n",
    "            # m^2/s^2 -> m\n",
    "            ds = ds.metpy.quantify()\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        ds_pangu = ds\n",
    "        ds_pangu.to_zarr(ofile)\n",
    "    ofile = (\n",
    "        CACHEDIR / f\"fengwu.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "    )\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_fengwu = xr.open_zarr(ofile)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        fengwu_dir = Path(\"/glade/derecho/scratch/ahijevyc/ai-models/output/fengwu\")\n",
    "        nested_files = get_nested_files(fengwu_dir, init_times, forecast_length, nmem=31)\n",
    "        ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa, units)\n",
    "        ds_fengwu.to_zarr(ofile)\n",
    "        \n",
    "\n",
    "ds_pangu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamics_model(\n",
    "    model: str,\n",
    "    forecast_hours: Iterable[int],\n",
    "    ens_size: int,\n",
    "    vars_dict: Dict[str, str],\n",
    "    init_times: Sequence[pd.Timestamp],\n",
    "    output_grid: xr.Dataset,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Retrieves and processes a dynamics model dataset.\n",
    "\n",
    "    Checks for a cached version of the data, otherwise loads it from NetCDF files,\n",
    "    processes it, regrids it to a target grid, and saves it to a Zarr store.\n",
    "    \"\"\"\n",
    "    # Tried native mesh but no 850t or 500z.\n",
    "    ofile: Path = CACHEDIR / f\"{model}.zarr\"\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"opening existing {ofile}\")\n",
    "        ds: xr.Dataset = xr.open_zarr(ofile)\n",
    "    else:\n",
    "        print(f\"creating {ofile}\")\n",
    "        # Create list of input files\n",
    "        # This is a nested list comprehension, looping through\n",
    "        # init_times\n",
    "        #   forecast_hours\n",
    "        #       members (1 through ens_size)\n",
    "        # Create a triply-nested list of input files:\n",
    "        # Level 1: Initialization Times\n",
    "        # Level 2: Forecast Hours\n",
    "        # Level 3: Ensemble Members\n",
    "        ifiles: List[List[List[Path]]] = [\n",
    "            [\n",
    "                [\n",
    "                    Path(f\"/glade/campaign/mmm/parc/schwartz/HWT{init_time:%Y}/{model}\")\n",
    "                    / init_time.strftime(\"%Y%m%d%H\")\n",
    "                    / \"post\"\n",
    "                    / f\"mem_{mem}\"\n",
    "                    / f\"interp_{model}_3km_{init_time:%Y%m%d%H}_mem{mem}_f{fhr:03d}.nc\"\n",
    "                    for mem in range(1, ens_size + 1)\n",
    "                ]\n",
    "                for fhr in forecast_hours\n",
    "            ]\n",
    "            for init_time in init_times\n",
    "        ]\n",
    "\n",
    "        ds = xr.open_mfdataset(\n",
    "            ifiles,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"initialization_time\", \"forecast_hour\", \"number\"],\n",
    "            preprocess=lambda ds: ds.assign_coords(forecast_hour=int(ds.attrs[\"forecastHour\"])),\n",
    "            drop_variables=[\"total_precip_hrly\"],\n",
    "            coords=\"minimal\",\n",
    "            compat=\"override\",\n",
    "            combine_attrs=\"drop\",\n",
    "            chunks={},\n",
    "        ).squeeze(dim=\"time\")\n",
    "        ds = ds.assign_coords(number=range(1, ens_size + 1), initialization_time=init_times)\n",
    "        ds = ds.assign_coords(\n",
    "            valid_time=ds[\"initialization_time\"] + ds[\"forecast_hour\"] * pd.Timedelta(\"1h\")\n",
    "        )\n",
    "        ds = ds.rename(lat=\"y\", lon=\"x\")\n",
    "        # Define new output grid\n",
    "        output_grid = xr.Dataset(\n",
    "            coords={\n",
    "                \"latitude\": output_grid.coords[\"latitude\"],\n",
    "                \"longitude\": output_grid.coords[\"longitude\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Select the first index along the dimensions you want to remove\n",
    "        # The `drop=True` argument removes the dimension and coordinate from the variable\n",
    "        for var_name in [\"latitude\", \"longitude\"]:\n",
    "            ds[var_name] = ds[var_name].isel(\n",
    "                initialization_time=0, forecast_hour=0, number=0, drop=True\n",
    "            )\n",
    "        ds[\"longitude\"] = ds[\"longitude\"] + 360  # MPAS/FV3 -180,180\n",
    "        ds = ds.rename(vars_dict)\n",
    "\n",
    "        # Create the regridding tool\n",
    "        regridder: xe.Regridder = xe.Regridder(ds, output_grid, method=\"bilinear\")\n",
    "\n",
    "        # Apply the regridding to your dataset\n",
    "        ds = regridder(ds)\n",
    "\n",
    "        ds[list(vars_dict.values())].to_zarr(ofile)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Example function calls (assuming init_times, CACHEDIR, and ds_pangu are defined)\n",
    "ds_mpas: xr.Dataset = get_dynamics_model(\n",
    "    \"mpas\",\n",
    "    forecast_hours=range(0, 132 + 1, 24),\n",
    "    ens_size=5,\n",
    "    vars_dict={\"temperature_850hPa\": \"t\", \"height_500hPa\": \"z\"},\n",
    "    init_times=init_times,\n",
    "    output_grid=ds_pangu,\n",
    ")\n",
    "ds_fv3: xr.Dataset = get_dynamics_model(\n",
    "    \"fv3\",\n",
    "    forecast_hours=range(24, 204 + 1, 24),\n",
    "    ens_size=10,\n",
    "    vars_dict={\"TMP850\": \"t\", \"HGT500\": \"z\"},\n",
    "    init_times=init_times,\n",
    "    output_grid=ds_pangu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gefs_members = [\"gec00\"] + [f\"gep{i:02d}\" for i in range(1, 31)]\n",
    "gefsdir = Path(\"/glade/derecho/scratch/ahijevyc/tmp/gefs\")\n",
    "\n",
    "ofile = CACHEDIR / f\"gefs.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "\n",
    "# Try saved ds_gefs. I tried saving zarr with compute=False but when I loaded, values were zero.\n",
    "if os.path.exists(ofile):\n",
    "    print(f\"open existing {ofile}\")\n",
    "    ds_gefs = xr.open_zarr(ofile)\n",
    "else:\n",
    "    existing_files = set(p.resolve() for p in gefsdir.glob(\"*/??/atmos/pgrb2ap5/*pgrb2a.0p50.f???\"))\n",
    "    \n",
    "    # Base URL for the public NOAA GEFS S3 bucket\n",
    "    base_url = \"https://noaa-gefs-pds.s3.amazonaws.com\"\n",
    "    \n",
    "    # ===================================================================\n",
    "    # PHASE 1: DEFINE FILE REQUIREMENTS\n",
    "    # ===================================================================\n",
    "    print(\"--- Phase 1: Defining all required file paths ---\")\n",
    "    required_files = []\n",
    "\n",
    "    for init_time in init_times:\n",
    "        for member in gefs_members:\n",
    "            for fhr in range(0, forecast_length + 1, 24):\n",
    "                fhr_str = f\"{fhr:03d}\"\n",
    "                s3_filename = f\"{member}.t{init_time:%H}z.pgrb2a.0p50.f{fhr_str}\"\n",
    "                file_path_on_s3 = f\"gefs.{init_time:%Y%m%d}/{init_time:%H}/atmos/pgrb2ap5/{s3_filename}\"\n",
    "                url = f\"{base_url}/{file_path_on_s3}\"\n",
    "                local_path = local_path_from_url(gefsdir, url)\n",
    "                required_files.append({\"url\": url, \"local_path\": local_path})\n",
    "\n",
    "    print(f\"âœ… Defined {len(required_files)} files required for analysis.\\n\")\n",
    "    print(f\"âœ… Data saved in: {gefsdir}\")\n",
    "    print(\"-\" * 50)\n",
    "    datasets = []\n",
    "\n",
    "    print(f\"making {ofile}\")\n",
    "    for required_file in tqdm(required_files):\n",
    "        url = required_file[\"url\"]\n",
    "        local_file_path = required_file[\"local_path\"]\n",
    "        if local_file_path in existing_files:\n",
    "            # print(f\"   -> ðŸŸ¢ File already exists. Skipping.\")\n",
    "            pass\n",
    "        else:\n",
    "            print(local_file_path)\n",
    "            # Ensure the destination directory exists before downloading\n",
    "            Path(local_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   -> â¬‡ï¸  Attempting to download: {url}\")\n",
    "            download_file(url, local_file_path)\n",
    "        ds_gefs = open_grib_dataset(local_file_path)\n",
    "        datasets.append(ds_gefs)\n",
    "\n",
    "    ds_gefs = xr.combine_nested(datasets, concat_dim=[\"time\"])\n",
    "\n",
    "    itime_coords = ds_gefs[\"valid_time\"] - ds_gefs[\"step\"]\n",
    "    ds_gefs = ds_gefs.assign_coords(initialization_time=itime_coords)\n",
    "    ds_gefs = ds_gefs.groupby([\"initialization_time\", \"step\", \"number\"]).first()\n",
    "\n",
    "    step_as_hours = ds_gefs[\"step\"].data / pd.to_timedelta(\"1h\")\n",
    "    ds_gefs = ds_gefs.assign_coords(forecast_hour=(\"step\", step_as_hours))\n",
    "    ds_gefs = ds_gefs.swap_dims(step=\"forecast_hour\")\n",
    "    ds_gefs = ds_gefs.assign_coords(valid_time = ds_gefs.initialization_time + ds_gefs.step)\n",
    "    ds_gefs.to_zarr(ofile)\n",
    "\n",
    "ds_gefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = Path(\"/glade/campaign/collections/rda/data/d633000/e5.oper.an.pl\")\n",
    "\n",
    "# tack on extra truth times to compare to the forecast valid times\n",
    "truth_times = init_times\n",
    "for init_time in init_times:\n",
    "    valid_times = pd.date_range(init_time, init_time + pd.to_timedelta(f\"{forecast_length}h\"))\n",
    "    truth_times = truth_times.union(valid_times)\n",
    "\n",
    "era5_files = [\n",
    "    era5_dir\n",
    "    / i.strftime(\"%Y%m\")\n",
    "    / f\"e5.oper.an.pl.128_{era5_varid[shortName]}_{shortName}.ll025sc.{i:%Y%m%d00}_{i:%Y%m%d23}.nc\"\n",
    "    for i in truth_times\n",
    "]\n",
    "truth = (\n",
    "    xr.open_mfdataset(era5_files)\n",
    "    .sel(level=isobaricInhPa)\n",
    "    .rename(time=\"valid_time\", level=\"isobaricInhPa\")\n",
    "    .rename({shortName.upper(): shortName})  # T->t, Z->z. uppercase shortName to lower\n",
    "    .drop_vars(\"utc_date\")\n",
    ")\n",
    "# era5 available every hour. just return 0z please\n",
    "truth = truth.sel(valid_time=(truth.valid_time.dt.hour == 0))\n",
    "\n",
    "# convert geopotential to geopotential height\n",
    "if shortName == \"z\":\n",
    "    truth[\"z\"] = truth[\"z\"] / g.m\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the geographic selection to each dataset, mask HWT region\n",
    "geographic_sel = dict(latitude=slice(60, 20), longitude=slice(220, 300))\n",
    "ds_mpas = ds_mpas.sel(geographic_sel).where(hwt.mask0p25).load()\n",
    "ds_fv3 = ds_fv3.sel(geographic_sel).where(hwt.mask0p25).load()\n",
    "ds_gefs = ds_gefs.sel(geographic_sel).where(hwt.mask0p5).load()\n",
    "ds_pangu = ds_pangu.sel(geographic_sel).where(hwt.mask0p25).load()\n",
    "ds_fengwu = ds_fengwu.sel(geographic_sel).where(hwt.mask0p25).load()\n",
    "truth = truth.sel(geographic_sel).where(hwt.mask0p25).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in forecast_hour=0 with forecast_hour=0 in source (ds_pangu)\n",
    "# tried xr.Dataset.combine_first instead of xr.concat but had troubles\n",
    "def fill_fhr0(source, target):\n",
    "    if 0 not in target.forecast_hour:\n",
    "        # select fh-0 for just the members in target dataset (number=target.number)\n",
    "        target = xr.concat([source.sel(forecast_hour=0, number=target.number), target], dim=\"forecast_hour\")\n",
    "    return target\n",
    "\n",
    "ds_fengwu = fill_fhr0(source=ds_pangu, target=ds_fengwu)\n",
    "ds_fv3 = fill_fhr0(source=ds_gefs, target=ds_fv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assumes the following variables are already loaded in your script:\n",
    "# ds_gefs, ds_pangu, ds_fengwu, ds_mpas: xarray.Dataset objects with model data\n",
    "# truth: xarray.Dataset with the ground truth data\n",
    "# shortName: A string representing the variable name (e.g., 't' for temperature)\n",
    "# units: A string for the y-axis label (e.g., 'K' for Kelvin)\n",
    "# isobaricInhPa: An integer representing the pressure level (e.g., 500)\n",
    "\n",
    "\n",
    "# These will store the handles and labels for the shared legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "# --- Define models and years to iterate over ---\n",
    "models_data = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu]\n",
    "model_titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# Create a nxm grid of subplots: n rows (for years), m columns (for models)\n",
    "fig, axes = plt.subplots(nrows=len(years), ncols=len(models_data), figsize=(20, 10), sharey=True)\n",
    "\n",
    "\n",
    "# --- Main Loop: Iterate over years and models ---\n",
    "# Loop over each year to create a row of plots\n",
    "for row_idx, year in enumerate(years):\n",
    "    # Loop over each model to create a plot in the corresponding column\n",
    "    for col_idx, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # --- Data processing steps (as before) ---\n",
    "        ensemble_mean = ds_model.mean(dim=[\"number\", \"latitude\", \"longitude\"])\n",
    "        stacked_ds = ensemble_mean.stack(point=(\"initialization_time\", \"forecast_hour\"))\n",
    "        tidy_ds = stacked_ds.set_index(point=[\"initialization_time\", \"valid_time\"])\n",
    "        plot_ready_full = tidy_ds[shortName].unstack(\"point\")\n",
    "\n",
    "        # --- Filter data for the specific year ---\n",
    "        # Use xarray's .sel() method for selection instead of pandas' .index\n",
    "        plot_ready = plot_ready_full.sel(valid_time=plot_ready_full.valid_time.dt.year == year)\n",
    "        truth_filtered = truth.sel(valid_time=truth.valid_time.dt.year == year)\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Plot the model forecast lines\n",
    "        # Use .size to check if an xarray DataArray has data\n",
    "        assert plot_ready.size > 0\n",
    "        plot_ready.plot.line(\n",
    "            x=\"valid_time\",\n",
    "            hue=\"initialization_time\",\n",
    "            ax=ax,\n",
    "            add_legend=False,\n",
    "        )\n",
    "        # Plot the truth data\n",
    "        assert truth_filtered[shortName].size > 0\n",
    "        truth_filtered[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "            ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\"\n",
    "        )\n",
    "\n",
    "        # --- Axis formatting ---\n",
    "        # Set the model title for the top row plots only\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(title, fontsize=14)\n",
    "\n",
    "        # Set the y-axis label to include the year on the leftmost plot of each row\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{year}\\n{shortName} [{units}]\", fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")  # Hide y-label for other plots in the row\n",
    "\n",
    "        ax.set_xlabel(\"Valid Time\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # --- Capture handles and labels ONCE from the first valid plot ---\n",
    "        if not handles and ax.get_lines():\n",
    "            all_lines = ax.get_lines()\n",
    "            truth_line = all_lines[-1]  # The last line plotted is the 'truth' line\n",
    "            forecast_lines = all_lines[:-1]  # The other lines are the forecasts\n",
    "\n",
    "            handles.extend(forecast_lines)\n",
    "            handles.append(truth_line)\n",
    "\n",
    "            # Create labels for forecast lines\n",
    "            forecast_labels = [\n",
    "                pd.to_datetime(t).strftime(\"%Y-%m-%d %Hz\")\n",
    "                for t in plot_ready.initialization_time.values\n",
    "            ]\n",
    "            labels.extend(forecast_labels)\n",
    "            labels.append(\"Truth\")  # Add the label for the truth line\n",
    "\n",
    "# --- Final Figure Adjustments ---\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Initialization Time\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.0, 0.5),\n",
    "    ncols=len(years),\n",
    ")\n",
    "\n",
    "# Add a main title for the entire figure\n",
    "fig.suptitle(f\"{isobaricInhPa}-hPa {shortName} Forecasts by Year\", fontsize=16)\n",
    "\n",
    "# Adjust layout to make room for the legend and suptitle\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f\"plots/ensmean_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the following variables are pre-defined from previous cells:\n",
    "# ds_gefs, ds_pangu, ds_fengwu, ds_mpas, truth, shortName, units, isobaricInhPa\n",
    "\n",
    "\n",
    "# --- Define models and years to iterate over ---\n",
    "models_data = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu]\n",
    "model_titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(years), ncols=len(models_data), figsize=(20, 10), sharex=False, sharey=True\n",
    ")\n",
    "\n",
    "# --- Legend Handling Setup ---\n",
    "# These will store the handles and labels for the final shared legend\n",
    "fig_handles = []\n",
    "fig_labels = []\n",
    "truth_handle_info = None # To ensure the 'Truth' entry is added only once\n",
    "\n",
    "# --- Main Loop: Iterate over years (rows) and models (columns) ---\n",
    "for row_idx, year in enumerate(years):\n",
    "    for col_idx, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        ds_processed = ds_model.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "        # Filter the spatially-averaged data by the initialization year\n",
    "        ds_model_year = ds_processed.sel(initialization_time=ds_processed.initialization_time.dt.year == year)\n",
    "        # Filter the truth data by the valid time year\n",
    "        truth_year = truth.sel(valid_time=truth.valid_time.dt.year == year)\n",
    "\n",
    "        # Loop over each initialization time in the filtered data\n",
    "        for init_time in ds_model_year.initialization_time:\n",
    "            run_with_members = ds_model_year.sel(initialization_time=init_time)\n",
    "            first_member_data = run_with_members.isel(number=0)\n",
    "\n",
    "            line = ax.plot(\n",
    "                first_member_data.valid_time,\n",
    "                first_member_data[shortName],\n",
    "                alpha=0.5,\n",
    "                label=pd.to_datetime(init_time.values).strftime(\"%Y-%m-%d %Hz\"),\n",
    "            )\n",
    "            run_color = line[0].get_color()\n",
    "\n",
    "            for member_index in ds_model_year.number[1:]:\n",
    "                member_data = run_with_members.sel(number=member_index)\n",
    "                ax.plot(\n",
    "                    member_data.valid_time,\n",
    "                    member_data[shortName],\n",
    "                    alpha=0.5,\n",
    "                    color=run_color,\n",
    "                )\n",
    "\n",
    "        # Plot the \"truth\" data ONCE per subplot\n",
    "        truth_year[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "            ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\", label=\"Truth\"\n",
    "        )\n",
    "\n",
    "        # --- Axis Formatting ---\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(f\"{title} Forecast\")\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{year}\\n{shortName} [{units}]\")\n",
    "\n",
    "        ax.set_xlabel(\"Valid Time\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Capture handles and labels from the first column of EACH row\n",
    "        if col_idx == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "            # Find and separate the 'Truth' line information\n",
    "            if \"Truth\" in labels:\n",
    "                truth_idx = labels.index(\"Truth\")\n",
    "                # Store the truth handle only the first time we see it\n",
    "                if not truth_handle_info:\n",
    "                    truth_handle_info = (handles.pop(truth_idx), labels.pop(truth_idx))\n",
    "                else: # Otherwise, just remove it\n",
    "                    handles.pop(truth_idx)\n",
    "                    labels.pop(truth_idx)\n",
    "\n",
    "            fig_handles.extend(handles)\n",
    "            fig_labels.extend(labels)\n",
    "\n",
    "# --- Final Legend Assembly ---\n",
    "# Add the single 'Truth' entry back at the end of the list\n",
    "if truth_handle_info:\n",
    "    fig_handles.append(truth_handle_info[0])\n",
    "    fig_labels.append(truth_handle_info[1])\n",
    "\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    fig_handles,\n",
    "    fig_labels,\n",
    "    title=\"Initialization Time\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    "    ncols=2\n",
    ")\n",
    "\n",
    "# Adjust main title and layout to make room for the legend\n",
    "fig.suptitle(f\"Spaghetti Plot Forecasts ({isobaricInhPa}-hPa {shortName})\", fontsize=16)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f\"plots/spaghetti_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "HET4wJfH2khw",
    "outputId": "1cf2e7ac-477a-46f8-a0ec-6f79ba3f196e"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes, [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu], [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "):\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # select 0.5-deg grid of GEFS or pangu fhr=0 has higher RMSE (because I didn't linearly interpolate pl fields from 0.5 to 0.25-deg grid in pangu input?)\n",
    "    ds_model = ds_model.sel(latitude=ds_gefs.latitude, longitude=ds_gefs.longitude)\n",
    "\n",
    "    \n",
    "    # average over initialization_time, spatial dims. std over ensemble (number).\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    # error = ensemble mean - truth\n",
    "    # Line up truth and forecast along `valid_time`.\n",
    "    error = ds_model.mean(dim=\"number\") - truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\", \"initialization_time\"])\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"rmse\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"spread\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"{shortName} [{units}]\")\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes, [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu], [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "):\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    # This ensures that for each forecast point, we have a corresponding observation.\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation (Original) ---\n",
    "    # Calculate ensemble spread: std dev across members, then averaged\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "\n",
    "    # Calculate RMSE: error of ensemble mean vs. truth\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # --- CRPS Calculation (Using properscoring) ---\n",
    "    # Define the order of non-ensemble dimensions to ensure consistency.\n",
    "    core_dims = [d for d in ds_model[shortName].dims if d != 'number']\n",
    "\n",
    "    # Explicitly align and transpose the DataArrays to ensure their underlying\n",
    "    # numpy arrays have compatible shapes for properscoring.\n",
    "    aligned_truth_da, forecast_da = xr.align(\n",
    "        aligned_truth[shortName],\n",
    "        ds_model[shortName],\n",
    "        join=\"inner\",\n",
    "        exclude=[\"number\"]\n",
    "    )\n",
    "\n",
    "    # Transpose so that the 'number' dimension is last for the forecast.\n",
    "    forecast_da = forecast_da.transpose(*core_dims, 'number')\n",
    "    aligned_truth_da = aligned_truth_da.transpose(*core_dims)\n",
    "\n",
    "    # Extract the numpy arrays. Their shapes are now guaranteed to be compatible.\n",
    "    truth_vals = aligned_truth_da.values\n",
    "    forecast_vals = forecast_da.values\n",
    "\n",
    "    # Calculate CRPS. The 'number' dimension is now correctly the last axis.\n",
    "    crps_vals = ps.crps_ensemble(truth_vals, forecast_vals)\n",
    "\n",
    "    # Convert the resulting numpy array back to an xarray.DataArray.\n",
    "    # The output of crps_ensemble has the same shape as the observations.\n",
    "    crps = xr.DataArray(\n",
    "        crps_vals,\n",
    "        dims=aligned_truth_da.dims,\n",
    "        coords=aligned_truth_da.coords\n",
    "    )\n",
    "\n",
    "    # Average the CRPS over spatial and time dimensions for a single score per lead time\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Plot the original and new metrics\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"RMSE\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"Spread\")\n",
    "    crps_mean.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"CRPS\", linestyle=\"--\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"Score [{units}]\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate members\n",
    "member_cutoff = 5\n",
    "\n",
    "# calculate deviation from the ensemble mean (anomaly)\n",
    "ds_fengwu_anom = ds_fengwu - ds_fengwu.isel(number=slice(0, member_cutoff)).mean(dim=\"number\")\n",
    "# inflate the spread about the mean\n",
    "ds_fengwu_inflated_spread = ds_fengwu.isel(number=slice(0, member_cutoff)).mean(dim=\"number\") + 3*ds_fengwu_anom\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(13, 9), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes.flat, [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_fengwu_inflated_spread], [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Fengwu spreadx3\"]\n",
    "):\n",
    "\n",
    "    # Select only the first 5 ensemble members for evaluation\n",
    "    ds_model = ds_model.isel(number=slice(0, member_cutoff))\n",
    "\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    # This ensures that for each forecast point, we have a corresponding observation.\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation (Original) ---\n",
    "    # Calculate ensemble spread: std dev across members, then averaged\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "\n",
    "    # Calculate RMSE: error of ensemble mean vs. truth\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # --- CRPS Calculation (Using properscoring) ---\n",
    "    # Define the order of non-ensemble dimensions to ensure consistency.\n",
    "    core_dims = [d for d in ds_model[shortName].dims if d != 'number']\n",
    "\n",
    "    # Explicitly align and transpose the DataArrays to ensure their underlying\n",
    "    # numpy arrays have compatible shapes for properscoring.\n",
    "    aligned_truth_da, forecast_da = xr.align(\n",
    "        aligned_truth[shortName],\n",
    "        ds_model[shortName],\n",
    "        join=\"inner\",\n",
    "        exclude=[\"number\"]\n",
    "    )\n",
    "\n",
    "    # Transpose so that the 'number' dimension is last for the forecast.\n",
    "    forecast_da = forecast_da.transpose(*core_dims, 'number')\n",
    "    aligned_truth_da = aligned_truth_da.transpose(*core_dims)\n",
    "\n",
    "    # Extract the numpy arrays. Their shapes are now guaranteed to be compatible.\n",
    "    truth_vals = aligned_truth_da.values\n",
    "    forecast_vals = forecast_da.values\n",
    "\n",
    "    # Calculate CRPS. The 'number' dimension is now correctly the last axis.\n",
    "    crps_vals = ps.crps_ensemble(truth_vals, forecast_vals)\n",
    "\n",
    "    # Convert the resulting numpy array back to an xarray.DataArray.\n",
    "    # The output of crps_ensemble has the same shape as the observations.\n",
    "    crps = xr.DataArray(\n",
    "        crps_vals,\n",
    "        dims=aligned_truth_da.dims,\n",
    "        coords=aligned_truth_da.coords\n",
    "    )\n",
    "\n",
    "    # Average the CRPS over spatial and time dimensions for a single score per lead time\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Plot the original and new metrics\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"RMSE\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"Spread\")\n",
    "    crps_mean.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"CRPS\", linestyle=\"--\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast {member_cutoff}-member cutoff\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"Score [{units}]\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(f\"plots/rmse_spread_crps_fengwu_spreadx3_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and their titles for the legend\n",
    "datasets = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu, ds_fengwu_inflated_spread]\n",
    "titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Fengwu spreadx3\"]\n",
    "\n",
    "# --- Create Subplots: One for each metric ---\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "ax_rmse, ax_spread, ax_crps = axes\n",
    "\n",
    "# Variable to store the color of the 'Fengwu' plot\n",
    "fengwu_color = None\n",
    "\n",
    "# --- Loop through models, calculate metrics, and plot on the correct panel ---\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "\n",
    "    # Select only the first 5 ensemble members for evaluation\n",
    "    ds_model = ds_model.isel(number=slice(0, member_cutoff))\n",
    "\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation ---\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # --- CRPS Calculation ---\n",
    "    core_dims = [d for d in ds_model[shortName].dims if d != \"number\"]\n",
    "    aligned_truth_da, forecast_da = xr.align(\n",
    "        aligned_truth[shortName], ds_model[shortName], join=\"inner\", exclude=[\"number\"]\n",
    "    )\n",
    "    forecast_da = forecast_da.transpose(*core_dims, \"number\")\n",
    "    aligned_truth_da = aligned_truth_da.transpose(*core_dims)\n",
    "    crps_vals = ps.crps_ensemble(aligned_truth_da.values, forecast_da.values)\n",
    "    crps = xr.DataArray(crps_vals, dims=aligned_truth_da.dims, coords=aligned_truth_da.coords)\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Plotting Logic ---\n",
    "    plot_kwargs = {\"marker\": \"o\", \"label\": title, \"alpha\": 0.7}\n",
    "\n",
    "    # Check if the current model is the one to be dashed\n",
    "    if title == \"Fengwu spreadx3\":\n",
    "        plot_kwargs[\"linestyle\"] = \"--\"\n",
    "        # Use the stored color from the 'Fengwu' plot\n",
    "        if fengwu_color:\n",
    "            plot_kwargs[\"color\"] = fengwu_color\n",
    "\n",
    "    # Plot each metric and store the color if it's the 'Fengwu' model\n",
    "    line_rmse = rmse.plot.line(ax=ax_rmse, x=\"day\", **plot_kwargs)\n",
    "    line_spread = ensemble_spread.plot.line(ax=ax_spread, x=\"day\", **plot_kwargs)\n",
    "    line_crps = crps_mean.plot.line(ax=ax_crps, x=\"day\", **plot_kwargs)\n",
    "\n",
    "    # If we just plotted 'Fengwu', get its color for the next iteration\n",
    "    if title == \"Fengwu\":\n",
    "        # The plot function returns a list of lines; get the color from the first one\n",
    "        fengwu_color = line_rmse[0].get_color()\n",
    "\n",
    "\n",
    "# --- Formatting for all subplots ---\n",
    "for ax in axes:\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend(title=f\"{member_cutoff}-member cutoff\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Set titles and y-axis label for each panel\n",
    "ax_rmse.set_title(f\"RMSE\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "ax_spread.set_title(f\"Spread\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "ax_crps.set_title(f\"CRPS\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "\n",
    "ax_rmse.set_ylabel(f\"Score [{units}]\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"plots/rmse_spread_crps_all_models_on_same_panel_{member_cutoff}-member_cutoff_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Logic ---\n",
    "datasets = [ds_mpas, ds_fv3, ds_gefs, ds_pangu, ds_fengwu]\n",
    "titles = [\"MPAS\", \"FV3\", \"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "\n",
    "# --- 1. Pre-calculate all RMSE arrays to find a global color scale ---\n",
    "all_rmse_maps = []\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "    # Align the truth dataset to the forecast's valid_time\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"], method=\"nearest\")\n",
    "\n",
    "    # Calculate error and RMSE, but keep spatial dimensions\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    # Average over initialization time to get a stable RMSE map for each forecast hour\n",
    "    mse = se.mean(dim=[\"initialization_time\"])\n",
    "    rmse = np.sqrt(mse)\n",
    "    all_rmse_maps.append(rmse)\n",
    "\n",
    "# Select the forecast hour to plot (e.g., the last one)\n",
    "fhr_to_plot = ds_mpas.forecast_hour.values[-1]\n",
    "rmse_maps_at_fhr = [rmse.sel(forecast_hour=fhr_to_plot) for rmse in all_rmse_maps]\n",
    "\n",
    "# Determine a robust color range using percentiles to avoid outliers skewing the scale\n",
    "# First, combine all data into a single array\n",
    "all_rmse_values = np.concatenate([da.values.ravel() for da in rmse_maps_at_fhr])\n",
    "all_rmse_values = all_rmse_values[~np.isnan(all_rmse_values)]\n",
    "# Then calculate the 2nd and 98th percentiles\n",
    "vmin = np.percentile(all_rmse_values, 1)\n",
    "vmax = np.percentile(all_rmse_values, 99.5)\n",
    "\n",
    "# --- 2. Create the plots ---\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=2,\n",
    "    figsize=(14, 5),\n",
    "    subplot_kw={\n",
    "        \"projection\": ccrs.LambertConformal(central_longitude=-98.0, central_latitude=39.5)\n",
    "    },\n",
    ")\n",
    "\n",
    "for i, (ax, ds_model, title) in enumerate(zip(axes.flat, datasets, titles)):\n",
    "    rmse_map = rmse_maps_at_fhr[i]\n",
    "\n",
    "    # Plot the 2D RMSE map\n",
    "    im = rmse_map.plot.pcolormesh(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=\"viridis\",\n",
    "        add_colorbar=False,  # We will add a single colorbar later\n",
    "    )\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "    ax.add_feature(cfeature.STATES, edgecolor=\"gray\", lw=0.5)\n",
    "\n",
    "    # Set title for each subplot\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    extent = [-124, -72, 25, 50]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(datasets), len(axes.flat)):\n",
    "    axes.flat[i].axis(\"off\")\n",
    "\n",
    "# Add a single, shared colorbar for the entire figure\n",
    "fig.colorbar(\n",
    "    im,\n",
    "    ax=axes,\n",
    "    orientation=\"vertical\",\n",
    "    label=f\"RMSE of {isobaricInhPa}{shortName} [{units}]\",\n",
    "    shrink=0.8,\n",
    "    pad=0.02,\n",
    ")\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(f\"RMSE of {isobaricInhPa}{shortName} at Forecast Hour {fhr_to_plot}\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Pre-calculate all Spread arrays to find a global color scale ---\n",
    "all_spread_maps = []\n",
    "for ds_model, title in zip(datasets, titles):\n",
    "    # Calculate ensemble spread, keeping spatial dimensions\n",
    "    # Average over initialization_time to get a stable spread map for each forecast hour\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\"])\n",
    "    )\n",
    "    all_spread_maps.append(ensemble_spread)\n",
    "\n",
    "# Select the forecast hour to plot (e.g., the last one)\n",
    "fhr_to_plot = ds_mpas.forecast_hour.values[-1]\n",
    "spread_maps_at_fhr = [spread.sel(forecast_hour=fhr_to_plot) for spread in all_spread_maps]\n",
    "\n",
    "# Determine a robust color range using percentiles to avoid outliers skewing the scale\n",
    "# First, combine all data into a single array\n",
    "all_spread_values = np.concatenate([da.values.ravel() for da in spread_maps_at_fhr])\n",
    "all_spread_values = all_spread_values[~np.isnan(all_spread_values)]\n",
    "# Then calculate the 1st and 99.5th percentiles\n",
    "vmin = np.percentile(all_spread_values, 1)\n",
    "vmax = np.percentile(all_spread_values, 99.5)\n",
    "\n",
    "\n",
    "# --- 2. Create the plots ---\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=3,\n",
    "    nrows=2,\n",
    "    figsize=(14, 5),\n",
    "    subplot_kw={\"projection\": ccrs.LambertConformal(central_longitude=-98.0, central_latitude=39.5)}\n",
    ")\n",
    "\n",
    "for i, (ax, ds_model, title) in enumerate(zip(axes.flat, datasets, titles)):\n",
    "    spread_map = spread_maps_at_fhr[i]\n",
    "    \n",
    "    # Plot the 2D Spread map\n",
    "    im = spread_map.plot.pcolormesh(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=\"viridis\",\n",
    "        add_colorbar=False  # We will add a single colorbar later\n",
    "    )\n",
    "    \n",
    "    # Add map features\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES, edgecolor='gray', lw=0.5)\n",
    "    \n",
    "    # Set title for each subplot\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    extent = [-124, -72, 25, 50]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(datasets), len(axes.flat)):\n",
    "    axes.flat[i].axis('off')\n",
    "\n",
    "# Add a single, shared colorbar for the entire figure\n",
    "# Note: colorbar is attached to the whole figure, not just the populated axes\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), orientation='vertical', label=f\"Spread of {isobaricInhPa}{shortName} [{units}]\", shrink=0.8, pad=0.02)\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(f\"Spread of {isobaricInhPa}{shortName} at Forecast Hour {fhr_to_plot}\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NPL 2025a",
   "language": "python",
   "name": "npl-2025a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
