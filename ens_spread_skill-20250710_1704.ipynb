{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2X-b5-NzDBVB",
    "outputId": "830cc9d8-d560-4244-a0f1-743a16e6735e"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import properscoring as ps\n",
    "import requests\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from hwt import firstRun\n",
    "from metpy.constants import g\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Place where Dave cached datasets for quicker use\n",
    "CACHEDIR = Path(\"/glade/derecho/scratch/ahijevyc/ldmason_hwt\")\n",
    "\n",
    "init_times23 = pd.date_range(firstRun(2023), pd.to_datetime(f\"{2023}0531\"), freq=\"24h\")\n",
    "init_times24 = pd.date_range(firstRun(2024), pd.to_datetime(f\"{2024}0531\"), freq=\"24h\")\n",
    "\n",
    "# Combine 2023 and 2024\n",
    "init_times = init_times23.union(init_times24)\n",
    "# sorted list of unique years in init_times\n",
    "years = sorted(list(set(i.year for i in init_times)))\n",
    "\n",
    "forecast_length = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel = False\n",
    "if parallel:\n",
    "    try:\n",
    "        client.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cluster.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    # Create a local cluster with workers (CPUs)\n",
    "    cluster = LocalCluster(threads_per_worker=1)\n",
    "    client = Client(cluster)\n",
    "    client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortName, isobaricInhPa, units = \"z\", 500, \"m\"\n",
    "era5_varid = {\"z\": 129, \"t\": 130}\n",
    "\n",
    "\n",
    "def build_file_url(init_time, mem, forecast_hour):\n",
    "    return (\n",
    "        f\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gens/prod/gefs.{init_time:%Y%m%d}/\"\n",
    "        f\"{init_time:%H}/atmos/pgrb2ap5/\"\n",
    "        f\"{mem}.t{init_time:%H}z.pgrb2a.0p50.f{forecast_hour:03d}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def local_path_from_url(gefsdir, file_url_str):\n",
    "    url_path = Path(file_url_str)\n",
    "    file_path = gefsdir.joinpath(*url_path.parts[-5:])\n",
    "    os.makedirs(file_path.parent, exist_ok=True)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def open_grib_dataset(path):\n",
    "    ds = xr.open_dataset(\n",
    "        path,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"errors\": \"ignore\"},\n",
    "        filter_by_keys={\n",
    "            \"typeOfLevel\": \"isobaricInhPa\",\n",
    "            \"level\": isobaricInhPa,\n",
    "            \"shortName\": \"gh\" if shortName == \"z\" else shortName,\n",
    "        },\n",
    "        decode_timedelta=True,\n",
    "        chunks={},  # chunking can help reduce memory usage\n",
    "    )\n",
    "    if shortName == \"z\":\n",
    "        ds = ds.rename(gh=\"z\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def download_file(url, local_file_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully downloaded: {local_file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        print(f\"Could not download {url}\")\n",
    "\n",
    "\n",
    "def ai_ifiles(pangu_dir, model, init_time, mem, forecast_length):\n",
    "    # return list of files at different lead times.\n",
    "    if init_time > pd.to_datetime(\"20250101\"):\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"ens{mem}\" / f\"{model}_forecast_data\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_ens{mem}_pred_{i:03d}.nc\"\n",
    "            for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "    else:\n",
    "        control_or_perturbation = \"p\" if mem > 0 else \"c\"\n",
    "        file_path = (\n",
    "            pangu_dir / init_time.strftime(\"%Y%m%d%H\") / f\"{control_or_perturbation}{mem:02d}\"\n",
    "        )\n",
    "        ifiles = [\n",
    "            file_path / f\"{model}_gefs_pred_{i:03d}.nc\" for i in range(24, forecast_length + 1, 24)\n",
    "        ]\n",
    "\n",
    "    return ifiles\n",
    "\n",
    "\n",
    "def add_ensemble_number(ds):\n",
    "    \"\"\"\n",
    "    A preprocessing function to be used with xr.open_mfdataset.\n",
    "    It extracts the ensemble member number from the source filename\n",
    "    and adds it as a 'number' coordinate.\n",
    "    \"\"\"\n",
    "    # Get the basename of the file (e.g., \"pangu_ens0_pred_162.nc\")\n",
    "    try:\n",
    "        filename = os.path.basename(ds.encoding[\"source\"])\n",
    "    except (KeyError, TypeError):\n",
    "        # Fallback if source encoding is not available\n",
    "        return ds\n",
    "\n",
    "    # Use a regular expression to find the number following 'ens'\n",
    "    match = re.search(r\"ens(\\d+)\", filename)\n",
    "\n",
    "    if match:\n",
    "        # Extract the number, convert to integer\n",
    "        ensemble_number = int(match.group(1))\n",
    "\n",
    "        # Add 'number' as a new dimension and assign the extracted number as its coordinate\n",
    "        return ds.expand_dims(number=[ensemble_number])\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_files(ai_dir, init_times, forecast_length, nmem=51):\n",
    "    # derive model from ai_dir\n",
    "    model = ai_dir.name.rstrip(\"_realtime\")\n",
    "    # Create a nested list where list[init_time][member] = [files_for_all_forecast_hours]\n",
    "    nested_files = []\n",
    "    for init_time in init_times:\n",
    "        # List for all members for this init_time\n",
    "        time_specific_files = []\n",
    "        for mem in range(nmem):\n",
    "            # List of all forecast hour files for this specific member\n",
    "            lead_times = ai_ifiles(ai_dir, model, init_time, mem, forecast_length)\n",
    "            if all([os.path.exists(f) for f in lead_times]):\n",
    "                time_specific_files.append(lead_times)\n",
    "            else:\n",
    "                print(f\"not all lead times present for {model} {mem} {init_time}\")\n",
    "\n",
    "        if len(time_specific_files) != nmem:\n",
    "            print(f\"only {len(time_specific_files)}/{nmem} {model} {init_time} files\")\n",
    "            continue\n",
    "        nested_files.append(time_specific_files)\n",
    "\n",
    "    return nested_files\n",
    "\n",
    "\n",
    "def merge_nested_files(nested_files, shortName, isobaricInhPa, units):\n",
    "    # The channel label we want to select\n",
    "    channel_label = f\"{shortName}{isobaricInhPa}\"\n",
    "\n",
    "    nmem = len(nested_files[0])\n",
    "    ds = (\n",
    "        xr.open_mfdataset(\n",
    "            nested_files,\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"init_time\", \"number\", \"prediction_timedelta\"],\n",
    "            chunks=\"auto\",  # Use 'auto' for better performance with dask\n",
    "        )\n",
    "        # Rename dimensions and coordinates at the start\n",
    "        .rename(\n",
    "            {\n",
    "                \"init_time\": \"initialization_time\",\n",
    "                \"prediction_timedelta\": \"step\",\n",
    "                \"lat\": \"latitude\",\n",
    "                \"lon\": \"longitude\",\n",
    "                \"__xarray_dataarray_variable__\": shortName,  # Rename the main variable\n",
    "            }\n",
    "        )\n",
    "        # Assign the integer coordinate for the 'number' dimension\n",
    "        .assign_coords(number=range(nmem))\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        .pipe(\n",
    "            lambda ds: ds.assign_coords(\n",
    "                forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "            ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        )\n",
    "        # Calculate the valid_time coordinate\n",
    "        .assign(valid_time=lambda ds: ds.initialization_time + ds.step)\n",
    "        # Select the desired channel by its label (more readable)\n",
    "        .sel(channel=channel_label)\n",
    "        # Add the pressure level as a non-dimension coordinate\n",
    "        .assign_coords(isobaricInhPa=isobaricInhPa)\n",
    "    )\n",
    "\n",
    "    if shortName == \"z\":\n",
    "        if \"units\" in ds[\"z\"].attrs:\n",
    "            # Let metpy take care of the units\n",
    "            ds = ds.metpy.quantify()\n",
    "            print(\"divide by g (as Quantity with units)\")\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        else:\n",
    "            # no units, like in fengwu\n",
    "            print(\"divide by g (no units)\")\n",
    "            ds[\"z\"] /= g.m\n",
    "            ds[\"z\"].attrs.update({\"units\": units})\n",
    "    if \"units\" in ds[shortName].attrs:\n",
    "        assert (\n",
    "            ds[shortName].attrs[\"units\"] == units\n",
    "        ), f\"expected units {units}. got {ds[shortName].attrs['units']}\"\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 2025 in years:\n",
    "    # Output from real-time runs in Ryan's directory\n",
    "    pangu_dir = Path(\"/glade/derecho/scratch/sobash/pangu_realtime\")\n",
    "    nested_files = get_nested_files(pangu_dir, init_times, forecast_length)\n",
    "    ds_pangu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "\n",
    "    fengwu_dir = Path(\"/glade/derecho/scratch/sobash/fengwu_realtime\")\n",
    "    nested_files = get_nested_files(fengwu_dir, init_times, forecast_length)\n",
    "    ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa)\n",
    "else:\n",
    "    ofile = (\n",
    "        CACHEDIR / f\"pangu.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "    )\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_pangu = xr.open_zarr(ofile)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        # Output from older dates (HWT2023/4)\n",
    "        # Nested list\n",
    "        #                         init0          , ... ,           initn\n",
    "        # model_runs = [[gec0, gep1, ... , gep30], ... , [gec0, gep1, ... , gep30]]\n",
    "        model_runs = []\n",
    "        for init_time in init_times:\n",
    "            idir = Path(\n",
    "                \"/glade/derecho/scratch/ahijevyc/ai-models/output/panguweather\"\n",
    "            ) / init_time.strftime(\"%Y%m%d%H\")\n",
    "            ifiles = sorted(list(idir.glob(\"g??[0-9][0-9].grib\")))\n",
    "            if len(ifiles) == 31:\n",
    "                model_runs.append(ifiles)\n",
    "            else:\n",
    "                logging.warning(f\"{init_time} has {len(ifiles)}/31 pangu files in {idir}\")\n",
    "        ds = xr.open_mfdataset(\n",
    "            model_runs,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"errors\": \"ignore\"},\n",
    "            filter_by_keys={\n",
    "                \"typeOfLevel\": \"isobaricInhPa\",\n",
    "                \"level\": isobaricInhPa,\n",
    "                \"shortName\": shortName,  # Don't worry about z being called gh. It's called z.\n",
    "            },\n",
    "            decode_timedelta=True,\n",
    "            chunks={},  # chunking can help reduce memory usage\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"time\", \"number\"],\n",
    "        )\n",
    "        ds = ds.rename(time=\"initialization_time\")\n",
    "        # Convert 'step' dimension from timedelta to integer forecast hours\n",
    "        ds = ds.assign_coords(\n",
    "            forecast_hour=(\"step\", ds[\"step\"].data / np.timedelta64(1, \"h\"))\n",
    "        ).swap_dims({\"step\": \"forecast_hour\"})\n",
    "        # Just multiples of 24 hours, please.\n",
    "        ds = ds.sel(forecast_hour=range(0, forecast_length + 1, 24))\n",
    "        if shortName == \"z\":\n",
    "            # Let metpy take care of units (must be Quantity first)\n",
    "            # m^2/s^2 -> m\n",
    "            ds = ds.metpy.quantify()\n",
    "            ds[\"z\"] /= g\n",
    "            ds = ds.metpy.dequantify()\n",
    "        ds_pangu = ds\n",
    "        ds_pangu.to_zarr(ofile)\n",
    "    ofile = (\n",
    "        CACHEDIR / f\"fengwu.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "    )\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"open existing {ofile}\")\n",
    "        ds_fengwu = xr.open_zarr(ofile)\n",
    "    else:\n",
    "        print(f\"making {ofile}\")\n",
    "        fengwu_dir = Path(\"/glade/derecho/scratch/ahijevyc/ai-models/output/fengwu\")\n",
    "        nested_files = get_nested_files(fengwu_dir, init_times, forecast_length, nmem=31)\n",
    "        ds_fengwu = merge_nested_files(nested_files, shortName, isobaricInhPa, units)\n",
    "        ds_fengwu.to_zarr(ofile)\n",
    "ds_pangu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gefs_members = [\"gec00\"] + [f\"gep{i:02d}\" for i in range(1, 31)]\n",
    "gefsdir = Path(\"/glade/derecho/scratch/ahijevyc/tmp/gefs\")\n",
    "\n",
    "existing_files = set(p.resolve() for p in gefsdir.glob(\"*/??/atmos/pgrb2ap5/*pgrb2a.0p50.f???\"))\n",
    "\n",
    "# Base URL for the public NOAA GEFS S3 bucket\n",
    "base_url = \"https://noaa-gefs-pds.s3.amazonaws.com\"\n",
    "# ===================================================================\n",
    "# PHASE 1: DEFINE FILE REQUIREMENTS\n",
    "# ===================================================================\n",
    "print(\"--- Phase 1: Defining all required file paths ---\")\n",
    "required_files = []\n",
    "\n",
    "for init_time in init_times:\n",
    "    for member in gefs_members:\n",
    "        for fhr in range(0, forecast_length + 1, 24):\n",
    "            fhr_str = f\"{fhr:03d}\"\n",
    "            s3_filename = f\"{member}.t{init_time:%H}z.pgrb2a.0p50.f{fhr_str}\"\n",
    "            file_path_on_s3 = f\"gefs.{init_time:%Y%m%d}/{init_time:%H}/atmos/pgrb2ap5/{s3_filename}\"\n",
    "            url = f\"{base_url}/{file_path_on_s3}\"\n",
    "            local_path = local_path_from_url(gefsdir, url)\n",
    "            required_files.append({\"url\": url, \"local_path\": local_path})\n",
    "\n",
    "print(f\"âœ… Defined {len(required_files)} files required for analysis.\\n\")\n",
    "print(f\"âœ… Data saved in: {gefsdir}\")\n",
    "print(\"-\" * 50)\n",
    "datasets = []\n",
    "\n",
    "ofile = CACHEDIR / f\"gefs.{isobaricInhPa}{shortName}.{years}.{forecast_length:03d}h.zarr\"\n",
    "# Try saved ds_gefs. I tried saving zarr with compute=False but when I loaded, values were zero.\n",
    "if os.path.exists(ofile):\n",
    "    print(f\"open existing {ofile}\")\n",
    "    ds_gefs = xr.open_zarr(ofile)\n",
    "else:\n",
    "    print(f\"making {ofile}\")\n",
    "    for required_file in tqdm(required_files):\n",
    "        url = required_file[\"url\"]\n",
    "        local_file_path = required_file[\"local_path\"]\n",
    "        if local_file_path in existing_files:\n",
    "            # print(f\"   -> ðŸŸ¢ File already exists. Skipping.\")\n",
    "            pass\n",
    "        else:\n",
    "            print(local_file_path)\n",
    "            # Ensure the destination directory exists before downloading\n",
    "            Path(local_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"   -> â¬‡ï¸  Attempting to download: {url}\")\n",
    "            download_file(url, local_file_path)\n",
    "        ds_gefs = open_grib_dataset(local_file_path)\n",
    "        datasets.append(ds_gefs)\n",
    "\n",
    "    ds_gefs = xr.combine_nested(datasets, concat_dim=[\"time\"])\n",
    "\n",
    "    itime_coords = ds_gefs[\"valid_time\"] - ds_gefs[\"step\"]\n",
    "    ds_gefs = ds_gefs.assign_coords(initialization_time=itime_coords)\n",
    "    ds_gefs = ds_gefs.groupby([\"initialization_time\", \"step\", \"number\"]).first()\n",
    "\n",
    "    step_as_hours = ds_gefs[\"step\"].data / pd.to_timedelta(\"1h\")\n",
    "    ds_gefs = ds_gefs.assign_coords(forecast_hour=(\"step\", step_as_hours))\n",
    "    ds_gefs = ds_gefs.swap_dims(step=\"forecast_hour\")\n",
    "    ds_gefs[\"valid_time\"] = ds_gefs.initialization_time + ds_gefs.step\n",
    "    ds_gefs.to_zarr(ofile)\n",
    "\n",
    "ds_gefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the geographic selection to each dataset and re-assign it to the original variable\n",
    "geographic_sel = dict(latitude=slice(60, 20), longitude=slice(220, 300))\n",
    "ds_gefs = ds_gefs.sel(geographic_sel).load()\n",
    "ds_pangu = ds_pangu.sel(geographic_sel).load()\n",
    "ds_fengwu = ds_fengwu.sel(geographic_sel).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in forecast_hour=0 in fengwu with forecast_hour=0 in pangu\n",
    "# drop \"valid_time\" coordinate or DTypePromotionError\n",
    "# tried xr.Dataset.combine_first instead of xr.concat but had troubles\n",
    "if 0 not in ds_fengwu.forecast_hour:\n",
    "    ds_fengwu = xr.concat(\n",
    "        [ds_pangu.sel(forecast_hour=0), ds_fengwu], dim=\"forecast_hour\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = Path(\"/glade/campaign/collections/rda/data/d633000/e5.oper.an.pl\")\n",
    "\n",
    "# tack on extra truth times to compare to the forecast valid times\n",
    "truth_times = init_times\n",
    "for init_time in init_times:\n",
    "    valid_times = pd.date_range(init_time, init_time + pd.to_timedelta(f\"{forecast_length}h\"))\n",
    "    truth_times = truth_times.union(valid_times)\n",
    "\n",
    "era5_files = [\n",
    "    era5_dir\n",
    "    / i.strftime(\"%Y%m\")\n",
    "    / f\"e5.oper.an.pl.128_{era5_varid[shortName]}_{shortName}.ll025sc.{i:%Y%m%d00}_{i:%Y%m%d23}.nc\"\n",
    "    for i in truth_times\n",
    "]\n",
    "truth = (\n",
    "    xr.open_mfdataset(era5_files)\n",
    "    .sel(level=isobaricInhPa)\n",
    "    .sel(geographic_sel)  # geographic selection\n",
    "    .rename(time=\"valid_time\", level=\"isobaricInhPa\")\n",
    "    .rename({shortName.upper(): shortName})  # T->t, Z->z. uppercase shortName to lower\n",
    ")\n",
    "# era5 available every hour. just return 0z please\n",
    "truth = truth.sel(valid_time=(truth.valid_time.dt.hour == 0))\n",
    "\n",
    "# convert geopotential to geopotential height\n",
    "if shortName == \"z\":\n",
    "    truth[\"z\"] = truth[\"z\"] / g.m\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assumes the following variables are already loaded in your script:\n",
    "# ds_gefs, ds_pangu, ds_fengwu: xarray.Dataset objects with model data\n",
    "# truth: xarray.Dataset with the ground truth data\n",
    "# shortName: A string representing the variable name (e.g., 't' for temperature)\n",
    "# units: A string for the y-axis label (e.g., 'K' for Kelvin)\n",
    "# isobaricInhPa: An integer representing the pressure level (e.g., 500)\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# Create a nx3 grid of subplots: n rows (for years), 3 columns (for models)\n",
    "fig, axes = plt.subplots(nrows=len(years), ncols=3, figsize=(18, 10), sharey=True)\n",
    "\n",
    "# These will store the handles and labels for the shared legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "# --- Define models and years to iterate over ---\n",
    "models_data = [ds_gefs, ds_pangu, ds_fengwu]\n",
    "model_titles = [\"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "\n",
    "# --- Main Loop: Iterate over years and models ---\n",
    "# Loop over each year to create a row of plots\n",
    "for row_idx, year in enumerate(years):\n",
    "    # Loop over each model to create a plot in the corresponding column\n",
    "    for col_idx, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # --- Data processing steps (as before) ---\n",
    "        ensemble_mean = ds_model.mean(dim=[\"number\", \"latitude\", \"longitude\"])\n",
    "        stacked_ds = ensemble_mean.stack(point=(\"initialization_time\", \"forecast_hour\"))\n",
    "        tidy_ds = stacked_ds.set_index(point=[\"initialization_time\", \"valid_time\"])\n",
    "        plot_ready_full = tidy_ds[shortName].unstack(\"point\")\n",
    "\n",
    "        # --- Filter data for the specific year ---\n",
    "        # Use xarray's .sel() method for selection instead of pandas' .index\n",
    "        plot_ready = plot_ready_full.sel(valid_time=plot_ready_full.valid_time.dt.year == year)\n",
    "        truth_filtered = truth.sel(valid_time=truth.valid_time.dt.year == year)\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Plot the model forecast lines\n",
    "        # Use .size to check if an xarray DataArray has data\n",
    "        assert plot_ready.size > 0\n",
    "        plot_ready.plot.line(\n",
    "            x=\"valid_time\",\n",
    "            hue=\"initialization_time\",\n",
    "            ax=ax,\n",
    "            add_legend=False,\n",
    "        )\n",
    "        # Plot the truth data\n",
    "        assert truth_filtered[shortName].size > 0\n",
    "        truth_filtered[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "            ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\"\n",
    "        )\n",
    "\n",
    "        # --- Axis formatting ---\n",
    "        # Set the model title for the top row plots only\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(title, fontsize=14)\n",
    "\n",
    "        # Set the y-axis label to include the year on the leftmost plot of each row\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{year}\\n{shortName} [{units}]\", fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")  # Hide y-label for other plots in the row\n",
    "\n",
    "        ax.set_xlabel(\"Valid Time\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # --- Capture handles and labels ONCE from the first valid plot ---\n",
    "        if not handles and ax.get_lines():\n",
    "            all_lines = ax.get_lines()\n",
    "            truth_line = all_lines[-1]  # The last line plotted is the 'truth' line\n",
    "            forecast_lines = all_lines[:-1]  # The other lines are the forecasts\n",
    "\n",
    "            handles.extend(forecast_lines)\n",
    "            handles.append(truth_line)\n",
    "\n",
    "            # Create labels for forecast lines\n",
    "            forecast_labels = [\n",
    "                pd.to_datetime(t).strftime(\"%Y-%m-%d %Hz\")\n",
    "                for t in plot_ready.initialization_time.values\n",
    "            ]\n",
    "            labels.extend(forecast_labels)\n",
    "            labels.append(\"Truth\")  # Add the label for the truth line\n",
    "\n",
    "# --- Final Figure Adjustments ---\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Initialization Time\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.0, 0.5),\n",
    "    ncols=len(years),\n",
    ")\n",
    "\n",
    "# Add a main title for the entire figure\n",
    "fig.suptitle(f\"{isobaricInhPa}-hPa {shortName} Forecasts by Year\", fontsize=16, y=1.02)\n",
    "\n",
    "# Adjust layout to make room for the legend and suptitle\n",
    "fig.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "plt.savefig(f\"plots/ensmean_{isobaricInhPa}{shortName}_{years}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the following variables are pre-defined from previous cells:\n",
    "# ds_gefs, ds_pangu, ds_fengwu, truth, shortName, units, isobaricInhPa\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(years), ncols=3, figsize=(18, 10), sharex=False, sharey=True\n",
    ")\n",
    "\n",
    "# --- Define models and years to iterate over ---\n",
    "models_data = [ds_gefs, ds_pangu, ds_fengwu]\n",
    "model_titles = [\"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "\n",
    "# --- Legend Handling Setup ---\n",
    "# These will store the handles and labels for the final shared legend\n",
    "fig_handles = []\n",
    "fig_labels = []\n",
    "truth_handle_info = None # To ensure the 'Truth' entry is added only once\n",
    "\n",
    "# --- Main Loop: Iterate over years (rows) and models (columns) ---\n",
    "for row_idx, year in enumerate(years):\n",
    "    for col_idx, (ds_model, title) in enumerate(zip(models_data, model_titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Average over spatial dimensions for each model first\n",
    "        ds_processed = ds_model.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "        # Filter the spatially-averaged data by the initialization year\n",
    "        ds_model_year = ds_processed.sel(initialization_time=ds_processed.initialization_time.dt.year == year)\n",
    "        # Filter the truth data by the valid time year\n",
    "        truth_year = truth.sel(valid_time=truth.valid_time.dt.year == year)\n",
    "\n",
    "        # Loop over each initialization time in the filtered data\n",
    "        for init_time in ds_model_year.initialization_time:\n",
    "            run_with_members = ds_model_year.sel(initialization_time=init_time)\n",
    "            first_member_data = run_with_members.isel(number=0)\n",
    "\n",
    "            line = ax.plot(\n",
    "                first_member_data.valid_time,\n",
    "                first_member_data[shortName],\n",
    "                alpha=0.5,\n",
    "                label=pd.to_datetime(init_time.values).strftime(\"%Y-%m-%d %Hz\"),\n",
    "            )\n",
    "            run_color = line[0].get_color()\n",
    "\n",
    "            for member_index in ds_model_year.number[1:]:\n",
    "                member_data = run_with_members.sel(number=member_index)\n",
    "                ax.plot(\n",
    "                    member_data.valid_time,\n",
    "                    member_data[shortName],\n",
    "                    alpha=0.5,\n",
    "                    color=run_color,\n",
    "                )\n",
    "\n",
    "        # Plot the \"truth\" data ONCE per subplot\n",
    "        truth_year[shortName].mean(dim=[\"latitude\", \"longitude\"]).plot.line(\n",
    "            ax=ax, x=\"valid_time\", marker=\"o\", color=\"k\", label=\"Truth\"\n",
    "        )\n",
    "\n",
    "        # --- Axis Formatting ---\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(f\"{title} Forecast\")\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f\"{year}\\n{shortName} [{units}]\")\n",
    "\n",
    "        ax.set_xlabel(\"Valid Time\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Capture handles and labels from the first column of EACH row\n",
    "        if col_idx == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "            # Find and separate the 'Truth' line information\n",
    "            if \"Truth\" in labels:\n",
    "                truth_idx = labels.index(\"Truth\")\n",
    "                # Store the truth handle only the first time we see it\n",
    "                if not truth_handle_info:\n",
    "                    truth_handle_info = (handles.pop(truth_idx), labels.pop(truth_idx))\n",
    "                else: # Otherwise, just remove it\n",
    "                    handles.pop(truth_idx)\n",
    "                    labels.pop(truth_idx)\n",
    "\n",
    "            fig_handles.extend(handles)\n",
    "            fig_labels.extend(labels)\n",
    "\n",
    "# --- Final Legend Assembly ---\n",
    "# Add the single 'Truth' entry back at the end of the list\n",
    "if truth_handle_info:\n",
    "    fig_handles.append(truth_handle_info[0])\n",
    "    fig_labels.append(truth_handle_info[1])\n",
    "\n",
    "# Add a shared legend to the right of the figure\n",
    "fig.legend(\n",
    "    fig_handles,\n",
    "    fig_labels,\n",
    "    title=\"Initialization Time\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(0.92, 0.5),\n",
    "    ncols=2\n",
    ")\n",
    "\n",
    "# Adjust main title and layout to make room for the legend\n",
    "fig.suptitle(f\"Spaghetti Plot Forecasts ({isobaricInhPa}-hPa {shortName})\", fontsize=16)\n",
    "fig.tight_layout(rect=[0, 0, 0.92, 0.96])\n",
    "\n",
    "plt.savefig(f\"plots/spaghetti_{isobaricInhPa}{shortName}_{years}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "HET4wJfH2khw",
    "outputId": "1cf2e7ac-477a-46f8-a0ec-6f79ba3f196e"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes, [ds_gefs, ds_pangu, ds_fengwu], [\"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "):\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "    # average over initialization_time, lat and lon. std over ensemble (number).\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "    # error = ensemble mean - truth\n",
    "    # Line up truth and forecast along `valid_time`.\n",
    "    error = ds_model.mean(dim=\"number\") - truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"rmse\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"spread\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast {isobaricInhPa}-hPa {shortName}\")\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"{shortName} [{units}]\")\n",
    "\n",
    "plt.savefig(f\"plots/rmse_spread_{isobaricInhPa}{shortName}_{years}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(18, 6), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes, [ds_gefs, ds_pangu, ds_fengwu], [\"GEFS\", \"Pangu-Weather\", \"Fengwu\"]\n",
    "):\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    # This ensures that for each forecast point, we have a corresponding observation.\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation (Original) ---\n",
    "    # Calculate ensemble spread: std dev across members, then averaged\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "\n",
    "    # Calculate RMSE: error of ensemble mean vs. truth\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # --- CRPS Calculation (Using properscoring) ---\n",
    "    # Define the order of non-ensemble dimensions to ensure consistency.\n",
    "    core_dims = [d for d in ds_model[shortName].dims if d != 'number']\n",
    "\n",
    "    # Explicitly align and transpose the DataArrays to ensure their underlying\n",
    "    # numpy arrays have compatible shapes for properscoring.\n",
    "    aligned_truth_da, forecast_da = xr.align(\n",
    "        aligned_truth[shortName],\n",
    "        ds_model[shortName],\n",
    "        join=\"inner\",\n",
    "        exclude=[\"number\"]\n",
    "    )\n",
    "\n",
    "    # Transpose so that the 'number' dimension is last for the forecast.\n",
    "    forecast_da = forecast_da.transpose(*core_dims, 'number')\n",
    "    aligned_truth_da = aligned_truth_da.transpose(*core_dims)\n",
    "\n",
    "    # Extract the numpy arrays. Their shapes are now guaranteed to be compatible.\n",
    "    truth_vals = aligned_truth_da.values\n",
    "    forecast_vals = forecast_da.values\n",
    "\n",
    "    # Calculate CRPS. The 'number' dimension is now correctly the last axis.\n",
    "    crps_vals = ps.crps_ensemble(truth_vals, forecast_vals)\n",
    "\n",
    "    # Convert the resulting numpy array back to an xarray.DataArray.\n",
    "    # The output of crps_ensemble has the same shape as the observations.\n",
    "    crps = xr.DataArray(\n",
    "        crps_vals,\n",
    "        dims=aligned_truth_da.dims,\n",
    "        coords=aligned_truth_da.coords\n",
    "    )\n",
    "\n",
    "    # Average the CRPS over spatial and time dimensions for a single score per lead time\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Plot the original and new metrics\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"RMSE\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"Spread\")\n",
    "    crps_mean.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"CRPS\", linestyle=\"--\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"Score [{units}]\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate deviation from the ensemble mean (anomaly)\n",
    "ds_fengwu_anom = ds_fengwu - ds_fengwu.mean(dim=\"number\")\n",
    "ds_fengwu_inflate_spread = ds_fengwu + 2*ds_fengwu_anom\n",
    "\n",
    "fig, axes = plt.subplots(ncols=4, figsize=(18, 6), sharex=True, sharey=True)\n",
    "\n",
    "for ax, ds_model, title in zip(\n",
    "    axes, [ds_gefs, ds_pangu, ds_fengwu, ds_fengwu_inflate_spread], [\"GEFS\", \"Pangu-Weather\", \"Fengwu\", \"Fengwu spreadx3\"]\n",
    "):\n",
    "    # Assign 'day' coordinate for plotting\n",
    "    ds_model = ds_model.assign_coords(day=ds_model.forecast_hour / 24)\n",
    "\n",
    "    # Align the truth dataset to the forecast's valid_time for accurate comparison\n",
    "    # This ensures that for each forecast point, we have a corresponding observation.\n",
    "    aligned_truth = truth.sel(valid_time=ds_model[\"valid_time\"])\n",
    "\n",
    "    # --- RMSE and Spread Calculation (Original) ---\n",
    "    # Calculate ensemble spread: std dev across members, then averaged\n",
    "    ensemble_spread = (\n",
    "        ds_model[shortName]\n",
    "        .std(dim=\"number\", ddof=1)\n",
    "        .mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "    )\n",
    "\n",
    "    # Calculate RMSE: error of ensemble mean vs. truth\n",
    "    error = ds_model.mean(dim=\"number\") - aligned_truth\n",
    "    se = error[shortName] ** 2\n",
    "    mse = se.mean(dim=[\"latitude\", \"longitude\"])\n",
    "    rmse = np.sqrt(mse).mean(dim=\"initialization_time\")\n",
    "\n",
    "    # --- CRPS Calculation (Using properscoring) ---\n",
    "    # Define the order of non-ensemble dimensions to ensure consistency.\n",
    "    core_dims = [d for d in ds_model[shortName].dims if d != 'number']\n",
    "\n",
    "    # Explicitly align and transpose the DataArrays to ensure their underlying\n",
    "    # numpy arrays have compatible shapes for properscoring.\n",
    "    aligned_truth_da, forecast_da = xr.align(\n",
    "        aligned_truth[shortName],\n",
    "        ds_model[shortName],\n",
    "        join=\"inner\",\n",
    "        exclude=[\"number\"]\n",
    "    )\n",
    "\n",
    "    # Transpose so that the 'number' dimension is last for the forecast.\n",
    "    forecast_da = forecast_da.transpose(*core_dims, 'number')\n",
    "    aligned_truth_da = aligned_truth_da.transpose(*core_dims)\n",
    "\n",
    "    # Extract the numpy arrays. Their shapes are now guaranteed to be compatible.\n",
    "    truth_vals = aligned_truth_da.values\n",
    "    forecast_vals = forecast_da.values\n",
    "\n",
    "    # Calculate CRPS. The 'number' dimension is now correctly the last axis.\n",
    "    crps_vals = ps.crps_ensemble(truth_vals, forecast_vals)\n",
    "\n",
    "    # Convert the resulting numpy array back to an xarray.DataArray.\n",
    "    # The output of crps_ensemble has the same shape as the observations.\n",
    "    crps = xr.DataArray(\n",
    "        crps_vals,\n",
    "        dims=aligned_truth_da.dims,\n",
    "        coords=aligned_truth_da.coords\n",
    "    )\n",
    "\n",
    "    # Average the CRPS over spatial and time dimensions for a single score per lead time\n",
    "    crps_mean = crps.mean(dim=[\"initialization_time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Plot the original and new metrics\n",
    "    rmse.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"RMSE\")\n",
    "    ensemble_spread.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"Spread\")\n",
    "    crps_mean.plot.line(ax=ax, x=\"day\", marker=\"o\", label=\"CRPS\", linestyle=\"--\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_title(f\"{title} Forecast\\n{isobaricInhPa}-hPa {shortName}\", fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f\"Score [{units}]\")\n",
    "    ax.set_xlabel(\"Forecast Lead Time (days)\")\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NPL 2025a",
   "language": "python",
   "name": "npl-2025a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
